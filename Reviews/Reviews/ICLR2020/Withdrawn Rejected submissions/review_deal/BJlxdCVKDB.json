{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper aims at making a deep RL policy interpretable and verifiable by distilling the policy represented by a deep neural network into an ensemble of decision trees. This should be done without hurting the performance of the policy. The authors achieve this by extending the existing Viper algorithm. The resulting approach can imitate the deep policy better compared with Viper while preserving verifiability. Experiments show that the proposed method improves in terms of cumulative reward and error rate over Viper in four benchmark tasks.\n\nThe amount of improvement over the original Viper is not convincing given the presented results. Moreover, reviewers uniformly agree that the contribution of this work is incremental. I therefore recommend to reject this paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "The paper proposes a method (MOET) to distillate a reinforcement learning policy  represented by a deep neural network into an ensemble of decision trees. The main objective of this procedure is to obtain an \"interpretable\" and verifiable policy while maintaining the performance of the policy. The authors build over the previously published algorithm Viper (Bastani et al, 2018), which distillates deep policies into a single decision tree using the DAGGER procedures, i.e. alternation of imitation learning of an expert policy and of additional data-sampling from the newly learned policy. In the VIPER algorithm decision trees are chosen because their structured nature allows to formally prove properties of the policy they represent when the environments dynamics are known and expressible in closed form. \n\nThe main contribution of the paper is the adaptation of VIPER to ensembles of trees, specifically to mixtures of experts, and an adaption of the mixture of experts algorithm to non-differentiable experts. The mixture of experts is extended to decision trees with an em-like two step procedures that alternatively trains the experts and their input dependent weighting function. The main reason for extending VIPER to multiple trees is to increase the representation power of the procedure and obtain more faithful approximations of the original policy. The choice of using a input-dependent linear mixture of trees model instead of a more classical aggregation procedure like random-forests or xg-boosts is instead due to the assumed interpretability of input-dependent linear combinations.\n\nThe results in the paper seems to demonstrate comparable or improved performance of MOET over viper, but also show the systematic inability of both methods in imitating the actual neural network policy in all the environments but CARTPOLE. The authors conclude showing that the formal methods proposed to verify the policy in (Bastani et al, 2018) can be extended to MOET as they only rely on the choice of using decision trees as a target model.\n\nOn overall the paper creates a consistent narrative grounded on the notions that decision trees and linear models are interpretable, neural networks are not, individual decision trees can only create limited decision boundaries. The authors frame their technical contribution (em-like training of decision trees mixtures of experts) as necessary for creating more sophisticate decision boundaries through ensembling, while keeping the interpretability of the model (the ensembling method is linear and the components are decision trees).\n\nWhile the narrative is consistent and consequent technical work seems valid the paper has some flaws that put it in my opinion slightly below the acceptance threshold:\n\n1) The notions that linear models and decision trees have the \"interpretable\" propertiy is simply assumed as a known fact in the scientific field, while it is actually a largely discussed point of contention. See as an example \"Lipton 2017, The Mythos of Model Interpretability\". The authors should explain specifically what they mean by interpretability and how the properties of decision trees and linear mixtures of experts are consistent with their definition. \n\n2) On the overall paper (similarly to the narrative used for Viper) the authors speak of interpretability as if it was referred to the original deep neural network model while they only interpret the surrogate model. The lack of a formal definition of interpretable masks the problem of interpreting a model through the interpretation of a surrogate model, which would rely on some proof that links whatever is demonstrated on the surrogate to hold for the original model. \n\n3) The presentation of the results focus on the improvement of MOET with respect to VIPER. As a results it is difficult to compare the performance of both procedures to the original neural network model. For Pong and Acrobot the surrogate models do not achieve comparable performance, nor seems to be good imitations of the original policy, the reasons and the implication of this fact are not discussed.\n\n4) There should be a better connection with the knowledge distillation/model compression literature and in general the important idea that the same computation can be represented by different algorithms. (Born Again Trees, Model Compression, Distilling the Knowledge in a Neural Network, Policy Distillation, Born Again Neural Networks).  There is also some related novel literature mapping neural network policies and word models to minimal hidden-markov models that could be of interest to the authors (Learning Finite State Representations of Recurrent Policy Networks, Learning Causal State Representations of Partially Observable Environments). \n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "The paper proposes an extension to the Viper[1] method for interpreting and verifying deep RL policies by learning a mixture of decision trees to mimic the originally learned policy. The proposed approach can imitate the deep policy better compared with Viper while preserving verifiability. Empirically the proposed method demonstrates improvement in terms of cumulative reward and misprediction rate over Viper in four benchmark tasks.\n\nI tend to reject the paper in its current form because (1) the idea of using mixture of trees to do policy extraction is somewhat incremental; and (2) experiments do not consistently show significant performance gain over the existing approach.\n\n=============================================================================================\n\nNovelty and significance\n\nThe paper addresses an important problem in RL, trying to extract an interpretable and verifiable policy from a deep RL model. The novel aspect of the paper is employing a mixture of expert trees model instead of a single decision tree in Viper. The proposed method is somewhat incremental in the sense that it is equivalent to replacing the first layer of the hard decision tree with a soft decision layer, which enables learning a non-axis-align decision boundaries in the first layer. This is more like a small modification of the original method than a significant contribution.\n\nMoreover, in the experiments, the proposed method does not consistently show significant improvements over Viper. Of course one can expect some improvements over Viper because the proposed method is using more flexible model by employing a soft and non-axis-align decision layer. Besides, for Viper the comparison is not fair because it is comparing with the best performing mixture of expert trees model among many candidate structures. I would like to see how Viper compares with the average performance of different structures. \n\n=============================================================================================\n\nOther comments\n\nThe organization of this paper is good and the paper is easy to read. However the authors can improve the tables and figures by providing more descriptions to be more self-contained. For example, I don't know what the column labels in table 1 (D/R/M/C) are referring to by solely looking at the table. And their actual meanings are really hard to find in the text. And for figure 2, the authors should point out the meaning of colors inside the illustrations.\n\n\n\n\n\n\n[1] Bastani, Osbert, Yewen Pu, and Armando Solar-Lezama. \"Verifiable reinforcement learning via policy extraction.\" Advances in Neural Information Processing Systems. 2018.\n\nUpdate after rebuttal: score unchanged\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper suggests learning decision trees to learn policies that can be verified efficiently. The decision trees are learnt by imitation learning (DAGGER) and are guided by a DNN policy (called the oracle) and its Q-function. The paper is an extension of VIPER to mixture of Experts.\n\nThe paper does two contributions:\n- the modification of Viper to learn a mixture of decision tree policies that better mimics the DRL agent, and\n- the fact that the model based on Mixture of Experts is still interpretable.\n\nForm my understanding, it is known that mixture of experts can be more accurate than a single decision tree. I'm unsure about the importance of verifying experimentally that it is also the case in the context of DAGGER. Could you please expand on why using a mixture of experts in the context of DAGGER is challenging? I think that would be an important addition to the paper. That being said, the experimental results are of interest as they bring interpretability and verifiable RL to more challenging environments.\n\nMinor comments:\n- The notation \"EM\" that appears in the fourth paragraph of the introduction is not defined previously.\n- There a few typos: e.g., \"there do not exist\"\n- How many seeds are used for the \"average results (e.g. Appendix E)?"
        }
    ]
}