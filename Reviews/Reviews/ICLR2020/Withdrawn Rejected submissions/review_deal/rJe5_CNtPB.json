{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposed an attention-forcing algorithm that guides the sequence-to-sequence model training to make it more stable. But as pointed out by the reviewers, the proposed method requires alignment which is normally unavailable. The solution to address that is using another teacher-forcing model, which can be expensive. \n\nThe major concern about this paper is the experimental justification is not sufficient:\n* lack of evaluations of the proposed method on different tasks;\n* lack of experiments on understanding how it interact with existing techniques such as scheduled sampling etc;\n* lack of comparisons to related existing supervised attention mechanisms. \n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a novel training scheme for seq2seq models where attention or reference alignment is used in combination with free-running mode for improving training.\n\nThe positives of this paper are that it is well written and very clear. It also is very relevant as seq2seq models can be hard to train and techniques like scheduled sampling and x-forcing algorithms are good heuristics but heuristics none-the-less.\n\nThe downside of this paper is in the experimental results and also complexity. It would’ve been good to see a broader set of experiments to really benchmark attention-forcing from other self-attention models.\n\nAttention forcing also requires a reference or ground-truth alignment, which is often not available. Hence the authors propose to simultaneously train another teacher-forcing model to estimate the reference alignment. However, this would incur twice the computation complexity. \n\nAttention forcing could also be used in conjunction with scheduled sampling. How does that compare with the reported results for attention forcing?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes a method for fixing exposure bias (ie. training vs generated distribution mismatch) in seq2seq modeling with attention, particularly for the application of speech synthesis where reference alignments are available.\n\n\nRelated Work is missing:\n\n- Another paper that studies fixing exposure bias in seq2seq learning:\n\nWiseman & Rush. Sequence-to-Sequence Learning as Beam-Search Optimization\nhttps://arxiv.org/pdf/1606.02960.pdf\n\n- Other papers that try to enforce attention to attend to specific locations:\n\nBao et al. Deriving Machine Attention from Human Rationales. https://arxiv.org/abs/1808.09367\n\nLiu et al. Neural Machine Translation with Supervised Attention. https://www.aclweb.org/anthology/C16-1291/\n\nYu et al. Supervising Neural Attention Models for Video Captioning by Human Gaze Data. https://arxiv.org/abs/1707.06029\n\nWithout the comparison against other related papers that also aim to supervise attention mechanisms (there are other beyond the ones I cited above)s, it is unclear how much is novel about this paper.\n\n- Furthermore, it is conceptually clear to me that attention-forcing fully matches the training vs generated distributions. The authors should describe in greater detail why this happens this, or whether these distributions are not required to fully match in attention-forcing (and in this case, why this would be desirable).\n\n- The experiments are not very convincing (only 30 human evaluators for Speech synthesis with no other quantitative evaluation, NMT results that are not particularly promising).\n\n- Use of non-anonymous github link is questionable for blinded submissions."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "This paper proposes an alternative mechanism of training the attention values of a sequence to sequence learning model as applied to tasks like speech synthesis and translation.  During training they compute two forms of attention: (1) the standard soft-attention from a decoder fed with teacher forced output, and (2) the inference-time attention from a decoder fed with predicted outputs.  Their training objective consists of two terms: The first is the token-wise cross entropy loss but by conditioning on the predicted output  but with teacher-forced attention.  The second is a KL distance between the above two types of attention distributions.   Experiments with mechanical  turks indicate that their attention forcing mechanism is strongly preferred over the existing teacher forced output and attention model.  On translation their method provides little or no improvement.\n\nI am inclined towards rejecting the paper because the experiment and related work section still requires a lot of work before 1. The claimed utility of the idea is established, and 2. The novelty over the many existing attention architectures is established.   I elaborate on each of these next.\n\nRelated work: Recently, many papers have directly or indirectly handled the problem of exposure bias that this paper attempts to address.  The paper does not discuss most of these.  Here are some that are missed from the paper:\n\n1.   Sequence level training with recurrent neural networks\nMA Ranzato, S Chopra, M Auli, W Zaremba, 2015.\nThis paper shows that the scheduled sampling method (discussed in the paper) is much worse than a reinforce-based training mechanism of handling exposure bias.  \n\n2. An actor-critic algorithm for sequence prediction\nD Bahdanau, P Brakel, K Xu, A Goyal, R Lowe\n\n3.  Posterior Attention Models for Sequence to Sequence Learning\nS Shankar, S Sarawagi - 2019\n\n4. Latent Alignment and Variational Attention\nYuntian Deng, Yoon Kim, Justin Chiu, Demi Guo, Alexander M. Rush 2018\n\n5. Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings\nShaohui Kuang, Junhui Li, António Branco, Weihua Luo, Deyi Xiong\n\nExperiments:  Their experiments are rather sketchy and limited.\nThe TTS experiments are only on one dataset.  Their method is compared only with the standard seq2seq learning approach.  Even the scheduled sampling or professor forcing methods are not compared with.  In addition, state of the art TTS methods have gained significantly from hierarchical attention.  As such as far as the TTS task is concerned the significance of the improved quality over a baseline seq2seq method is limited.\n\nFor translation they consider only the English-Vietnamese task whereas there are tens of other translation tasks that are used in recent literature.\n\nOverall, the idea proposed seems quite incremental, experiments are limited, and related work discussion incomplete.\n\n*********\nI read the author response but I do not think the paper is ready for publication yet without the thorough comparison with related work.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}