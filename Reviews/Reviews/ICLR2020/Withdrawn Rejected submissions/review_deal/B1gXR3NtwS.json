{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors develop stochastic variational approaches to learn Bayesian \"structure distributions\" for neural networks. While the reviewers appreciated the updates to the paper made by the authors, there will still a number of remaining concerns. There were particularly concerns about the clarity of the paper (remarking on informality of language and lack of changes in the revision with respect to comments in the original review), and the fairness of comparisons. Regarding comparisons, one reviewer comments: \"I do not agree that the comparison with DARTS is fair because the authors remove the options for retraining in both DARTS and DBSN. The reason DARTS trains using one half of the data and validate on the other is that it includes a retraining phase where all data is used. Therefore fair comparison should use the same procedure as DARTS (including a retraining phrase). At the very least, to compare methods without retraining, results of DARTS with more data (e.g., 80%) for training should be reported.\" The authors are encouraged to continue with this work, carefully accounting for reviewer comments in future revisions.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper combines ideas from neural architecture search (NAS) and Bayesian neural networks. Instead of maintaining uncertainty in network weights, the authors propose to retain uncertainty in the network structure. In particular, building on cell-based differentiable NAS, the authors infer a distribution over the gating weights of different cells incident onto a tensor while relying on point estimates for the weights inside each cell. \n\nOverall, I liked the paper and vote for accepting it. The notion of maintaining uncertainty about the network structure is a sensible one, and the paper explores an as yet under-explored area at the intersection of state-of-the-art network architecture search algorithms and Bayesian neural networks. Moreover, this is accompanied by compelling empirics — results demonstrate gains in both predictive performance and calibration across diverse tasks and careful comparisons to sensible baselines are presented to evaluate various aspects of the proposed approach (Table 1). \n\nDetailed Comments:\n+ One issue the experiments fail to adequately disentangle is the effect of weight uncertainty vs structure uncertainty. Are the observed gains in accuracy and calibration simply a product of better structure learning? In particular, I would love to see a baseline where point estimates of \\alpha are learned but posterior distribution over weights is inferred. I realize NEK-FAC was an attempt at providing such a comparison, but since it uses a different structure, it remains unclear whether it’s poor performance stems from the fundamental difficulty of learning posteriors over high dimensional weights or simply a sub-optimal network structure. \n\n+ In a similar spirit, one can imagine a fully Bayesian DBSN where one infers posterior distributions overbite \\alpha and w. Presumably, this would close the OOD entropy gap between random \\alpha and DBSN. \n\n+ How many Monte Carlo samples were used to evaluate Equation 8. In variational BNNs one often finds that using more MC samples doesn’t necessarily improve predictive accuracy over using the most likely sample (the mean if using a Gaussian variational family). It would be interesting to see predictive performance as a function of the number of MC samples for DBSN. \n\n+ Clarity: While I am mostly upbeat about this paper, the writing could be significantly improved. While the overall ideas come across, there are several instances where the text appears muddled and needs a few more polishing passes. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes to do approximate Bayesian inference in neural networks by treating the neural network structure as a random variable (RV), while inferring the parameters with point estimates. \nWhile performing Bayesian inference for the neural network structure is sensible, I am not convinced by the approach taken in this work. \n\nThe biggest problem is that the model uses a point estimate of the same weights for different, random network structures. \nMajor problems:\n- In the motivation the authors write “DBSN places distributions on the network structure, introducing more global randomness, thus is probable to yield more diverse predictions, and ensembling them brings more calibrated uncertainty”. What is “more global randomness”? This is used multiple times. Does it refer to the hierarchy in the graphical model? Please be precise here and point that out in your model by using an equation or graphical model. Or is it just an intuition? \n- Generally, I would agree that integrating out multiple network structures provides better calibrated uncertainty. However, given that the authors use point estimates for the weights, it is not clear if that is still true, especially since the number of different architectures used in practice is small. \n- What’s more, the approach uses *the same* point estimates for different structures. This leads to a graphical model, where the weights are not conditioned on the architecture/structure. This modeling choice could be a big limitation, because the weights now have to fit multiple different architectures; it may thus defeat the calibration completely. One can easily imagine that only a single random architecture works well with the learned point estimates, thus resulting in an (almost) deterministic model. I assume that this modeling choice was made for practical reasons, but could you expand on its implications / interpretation / limitations? Does the posterior of such a constrained model not quickly converge to an “almost” dirac, effectively just one network structure? \n- Sec. 3.1. presents the above problem resulting from a modeling decision as a “training challenge”. To counter this problem, the authors propose to reduce the variance of the structure distribution. By doing so, the approach becomes even less Bayesian and the predictive uncertainty becomes even less reliable. \n- “We only learn the connections between the B internal nodes, as shown in Appendix D”. All deterministic weights are learned, but the structure only for some parts of the model? If this is the case, then approach becomes again less probabilistic.\n\nRegarding the experiments, the stddevs are calculated from 3(!) independent runs and thus completely misleading (imagine the stddev of the stddev estimate). \n\nIn summary, the model choice of point estimates for the weights, which are not conditioned on the architecture, leads to various problems. The authors have to introduce tricks such as reducing the variance of the random network structures or learning only a part of the whole structure to make the approach converge. The resulting probabilistic model and its predictive uncertainty is questionable. For this reason, this paper should be rejected. \n\nMinor problems\n- Sec. 3.2. “Improvements of the structure learning space”. What is a “structure learning space”? \n- Section 3 introduces the ELBO in Eq. (4) before the complete model is specified. Please specify the whole model first. How do w and alpha depend on each other in your model? \n- The prior for the weights is omitted; at the same time it is mentioned in the experiments (Sec.5.1.) that weight decay is applied. Why not just be explicit about it and say that a Gaussian prior is used?\n- Background Sec. 2.2. is not clear.  what is a cell? some deterministic transformation in general? bunch of neural network layers? What are the operation (last term in Eq. (2)) doing? This is not detailed and abstract to me. Are the alphas probabilities? Is Eq. (2) consequently a mixture model of different architectures? Or is this here just a weighted sum, where the weights take arbitrary values? A small visualization (additionally) might help here, but can probably be rectified by better explanation.\n- Bayesian reasoning on the structure. Inference?\n- Writing that you propose a new “framework” is a bit grandiose for what is actually proposed. There has been previous work in which the architecture is inferred as well and these approaches would certainly be part of the same framework. Please just say model/algorithm/approach, whatever is applicable. \n- new paragraph starting at “To empirically validate” in the intro.\n- Before (4): “Then we rewrite the approximation error”. Eq. (4) is the ELBO, this is not an approximation error. \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposed deep Bayesian structure networks (DBSN) to model weights, \\alpha, of the redundant operations in cell-based differentiable NAS. The authors claim that DBSN can achieve better performance (accuracy) than the state of the art. \n\nOne of my concerns is the Bayesian formulation introduced in Eq. (4) seems problematic. It is not clear what priors are placed on alpha. In the case of Bayes by BP (BBB), which is cited as Blundell et al. 2015 in the paper, a Gaussian prior (with zero mean) is used. Therefore there is a KL term between the variational distribution q(w) and the prior distribution p(w) to regularize q(w). In DBSN, q(\\alpha) is parameterized by \\theta and \\epsilon, and so is p(\\alpha), meaning that the KL term is effectively zero. This is very different from what is done in BBB.\n\nThe second major concern is on the experiments. (1) The authors use DARTS as a main baseline and show that DBSN significantly outperforms DARTS. However, looking at the DARTS paper, the test error on CIFAR-10 is around 3% for both the first-order and second-order versions. The test error in Table 1 is around 9%, which is a lot lower. I notice that the DARTS paper has a parameter number of 3.3M, while in the current paper it set to 1M. Given that DARTS is the main baseline method and the same dataset (CIFAR-10) is used, it would make much more sense to use exactly the same architecture for comparison. The current results is hardly convincing. (2) Besides, note that in the DARTS paper, DenseNet-BC has test error of 3.46%, much higher than DARTS (~3%). In Table 2 of this paper however, DARTS is significantly worse than DenseNet-BC (8.91% versus 4.51%). These results are highly inconsistent with previous work.\n\nAs mentioned in the paper, Dikov & Bayer 2019 has a very similar idea to perform NAS from a Bayesian perspective. It would be best (and would definitely make the paper stronger) to include some comparison. Even if Dikov & Bayer 2019 is not very scalable, it is at least possible to compare them in smaller network size. Otherwise it is hard to evaluate the contribution of DBSN given this highly similar work.\n\nThe authors mentioned in the introduction that DBSN ‘yields more diverse prediction’ and therefore brings more calibrated uncertainty comparing to ensembling different architectures. This is not verified in the experiment section. Table 3 only reports the ECE for one instance of trained networks. For example, it would be interesting to sample different architecture from the alpha learned in DARTS and DBSN, train several networks, ensemble them, and use the variance of the ensemble to compute ECE. This would verify the claim mentioned above.\n\nDo you retrain the network from scratch after the architecture search (which is done in DARTS) for DARTS and DBSN?\n\nI am not convinced by the claim that BNN usually achieve compromising performance. Essentially, BNN, if trained well, is a generalization of deterministic NN. If very flat priors and highly confident variational distributions are used, BNN essentially reduces to deterministic NN.\n\nMissing references on Bayesian deep learning and BNN:\n\nBayesian Dark Knowledge\nTowards Bayesian Deep Learning: A Survey\nNatural-Parameter Networks: A Class of Probabilistic Neural Networks"
        }
    ]
}