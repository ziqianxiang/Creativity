{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this work, the authors propose a scheme for compressing convolutional neural networks which is based on identifying convolutional kernels in previous layers which are most similar to the weights in a particular layer and then representing the weights in that layer as a combination of the previous kernel index + the residual (which is quantized). Additionally, the authors propose an inter-layer loss which regularizes the model so that the kernels in adjacent layers are similar which reduces the amount of information which needs to be stored when representing the network.\n\nOverall, the paper is clearly written and straightforward to understand. However, I have a number of concerns with the work in its present form, based on which I am leaning towards rejecting this paper:\n\n1. The work focuses on reducing the memory requirements of the network but does not discuss the increase in the time required to “reconstruct” the network for inference. It is hard to determine how effective such a method would be in practice without measuring this as well. \n2. The method is not compared against existing works which makes it harder to determine the relative benefits of the proposed methods.\n3. It was not exactly clear to me what the the similarity function L used to compare two convolutional kernels. Equation 3 suggests that it might be the L1 norm on the weights is that the case? Please clarify in the revised version.\n4. A clarification question about the selection of \\lambda (Section 4.1): Was the optimal value of \\lambda determined on a held-out set, or on the actual test set (which would be flawed to do).\n5. In Table 1: If I understood correctly, the model is not retrained after the procedure. How do the authors explain the improvement after applying the FSS and LSS methods on the test set?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a compression scheme for the convolutional layers of a neural network. The method is based on the observation that subsequent layers are correlated. The idea is that it is sufficient to encode the residuals of the convolutional filters compared to the previous layer, which is cheaper than encoding the filters by themselves.\n\nAs far as I know, this is the first paper to study the correlation between subsequent layers and try to exploit it for compression. It is an interesting idea that might be worth exploring.\n\nThe writing of the paper is good. It is easy to read and it is well organized. I communicates the ideas well. But I think the paper could be fit in the recommended 8 pages.\n\nI have two concerns regarding methodology. Firstly, the study of the correlation between layers (SVWH) is not rigorous. Eq. (1) is a vague statement saying that the distance between two subsequent kernels is likely to be less than the distance to any other. It is unclear what the distance metric can be, what the probabilities are taken over, whether these statements hold simultaneously for all i,j,u,v and for which datasets. A hypothesis like this should be more explicit and it should be evaluated against the null hypothesis. This hypothesis is the motivation for the whole paper so it should be thoroughly examined.\n\nMy second concern is the evaluation of the method. The baseline is 8 bit quantization and Huffman coding, which does not accurately represent the current state of the art in compression methods. I would recommend comparison against Deep compression or Bayesian compression as baselines. It is also unclear what method was used for quantization. Is it simple rounding or trained quantization as in Deep compression?\n\nOverall assessment:\nThe paper presents an interesting  idea, but it has shortcomings in technical writing and evaluation. It requires a revision before it is ready to be presented at a major conference."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a method to compress deep neural networks.\nThe authors first observe that kernels in adjacent layers have strong similarities. Based on this observation, the authors propose to quantize the difference between similar kernels instead of the kernels themselves. An inter-layer loss that pushes kernels in adjacent layers to be similar by minimizing their difference is also proposed.\n\nThe idea is simple and straightforward. However, the paper is not very well written. Many symbols and terms are used before been defined or introduced. Moreover, there are some typos even in important formulas, making the paper a bit hard to follow. Some questions are listed here:\n1. What is texture/non-texture and what are texture bits and non-texture bits in the abstract?\n2. The two sides in eq(1) are exactly the same, why there is a \">\"? What does this mean?\n2. What is c_i in eq(2)?\n3. In the whole network, is only the first layer not quantized? If so, from the third layer, are the weight kernels in each layer quantized based on the quantized weight kernels in the previous layer? \n\nThe clarification in the experiment section can also be improved. From Table 1, the proposed method either has a larger model size, or performs much worse than the baseline model. Moreover, the comparison with more recent state-of-the-art quantization models are not provided, thus it is hard to determine the efficacy of the proposed method.\n"
        }
    ]
}