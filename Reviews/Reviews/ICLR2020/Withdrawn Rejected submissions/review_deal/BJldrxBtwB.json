{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper analyses the shortcomings of GANs that have been observed in previous research, proposing and testing two potential reasons for the observed shortcomings, and proposing methods to mitigate these. The two proposed causes (sample insufficiency and pixel wise combination) are tested both empirically and theoretically. Sample inefficiency relates to batch size, and the authors show that the FID scores are lower when larger batch sizes are used. Pixel wise combination refers to the idea that if the discriminator picks up pixel wise combinations of training items as positive samples, then the generator will generate pixel wise combinations as well, despite these being not similar to the samples.\n\nThe paper is well written and contains both theoretical justifications for the hypotheses as well as empirical verification. The arguments presented are convincing.\n\nThe solutions presented are not convincing to me though. The authors propose pixel-wise combination regularisation and sample correction.\n\nPixel-wise combination regularisation generates pixel wise combinations and adds them to the negative training set. A concern here is that it is possible that some pixel wise combinations are in fact valid positive samples. In particular, one could envision that two very similar input images would have a combination that looks coherent. Adding this to the negative set may cause problems.\n\nI think I may have been missing something in terms of sample correction. The paper proposes removing realistic negative samples without providing any indication of how this could be done in general. I think it makes sense that increasing the distance between the positive samples and negative samples would make the discriminator's job easier. But this is easier said than done, and I feel that this is perhaps not meaningful enough as a contribution.\n\nOn the whole, I think the work is good, and I look forward to reading the author's explanations and responses to these concerns.\n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper tries to understand and tackle what they call the anomalous generalization of GANs. They first show that GANs suffer from instability because of sample insufficiency, they show experimentally that when decreasing the batch-size on both a toy dataset and CelebA that the training become unstable and the performance drops, leading to the generation of outliers, they also show theoretically in a simplified setting that small batch-size can lead to instability. They then show that the anomalous generalization can also come from the discriminator not being able to discriminate pixel-wise combination of the real samples. They show on some toy dataset and CelebA that some of the generated samples are a pixel-wise combination of two training examples and that those combination can fool the discriminator, they also show provide a theorem confirming this observation. Finally they provide two methods to mitigate the previous issues: 1) They propose to add a loss term for the discriminator and force it to also learn to discriminate pixel-wise combinations of images. 2) They propose to remove the most realistic generated samples from the mini-batch and replace them with less realistic samples\n\nOverall I find the observations of the paper interesting and worth investigating. However I find the paper a bit weak especially about sample efficiency. Indeed the instability of GANs and how it relates to batch-size as already been studied, and the theoretical results provided are quite limited. Furthermore I'm not convinced by the proposed trick to fix that issue.\nThus I'm in favour of rejecting this paper.\n\nMain argument:\n- I think the results in section 3.3 should be put into context and mention related work. First of all this bilinear example as already been studied in the literature, in the full-batch and stochastic setting. In full batch it was already known that simultaneous gradient descent doesn't converge see for example [1]. In the stochastic setting it was also studied and an algorithm was proposed to fix that see [2]. I think this related work and other need to be mentioned in this section and the results need to be contrasted with existing results. In the current state it's not clear if this theorems bring anything novel.\n- I have also some concerns about section 4.3. At the beginning of training, it's clear that it's probably easy for the discriminator to classify real samples with a large margin, but as the training progress it becomes harder and harder for the discriminator to classify real samples so it's not clear to me that assumption 2 would hold. Can you discuss it ? it would be interesting to actually check experimentally how that assumption holds.\n- I'm also not convinced by the Sample Correction trick proposed.\n- PCR really seems to improve the performance, it would be nice to run all the methods with several seeds and also show the standard deviation.\n- Why is SC only computed on a really small subset of CelebA and CIFAR10 ? why not try on the full dataset ? It makes the conclusion not very convincing. \n\n\nMinor comment:\n- Figure 1 (right): what are the axis ? in particular does the axis represent the number of training iterations ? If so what is number of generated samples used to compute the proportion of correct generated samples. Also you don't measure if there is any trade-off between quality and mode dropping. Could you measure the number of different samples that are generated to have some measure of mode dropping ?\n- For the toy task how do you count the number of rectangles ?\n- Can you define what does DIF stands for and how it is computed ?\n\nReferences:\n[1] Gidel, G., et al. \"A variational inequality perspective on generative adversarial networks.\" arXiv (2018).\n[2] Chavdarova, T., et al. \"Reducing noise in gan training with variance reduced extragradient.\" arXiv (2019)."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper discusses two problems in GANs, termed as “sample inefficiency” and “pixel-wise combination” by the authors. The former is attributed to small batch size, whereas the latter is exhibited as classifying as real the linear combination of 2 real images. Unfortunately, both the theoretical analysis and the presentation of experiments are limited. Besides, the writing needs to be improved. The paper can be more concise by removing many redundant statements. For example, there are many repeated sentences in the second paragraph of page 2 and section 3.1.\n\nTheoretical results:\n\n-The analysis (section 3.3 and 4.3) is based on much simplified assumptions. Section 3.3 assumes linear generator and discriminator. In addition, the results that the variance of parameter update inversely scales with the batch size is well-known in optimisation literature for stochastic gradient descent (under milder assumptions). It is unclear to me why this result can not be directly applied in GANs (for each update step), and what new insight the new derivation with stronger assumption provides.\n\n-Theorem 3 assumes large classification margin (assumption 2). This assumption is used to analyse the behaviour of GANs after convergence, but it is unlikely as training progress, when the discriminator increasingly struggles to tell real and fake samples apart. There is also an additional assumption that |x_1 - x_2|_2 < \\delta, which means the 2 images are already close in pixel space, so the average of x_1 and x_2 are likely to be another real sample.\n\nTherefore, I think the main theoretical results in this paper rely on strong assumptions, and do not provide enough insight into GAN training in more general cases.\n\nEmpirical results:\n\n-The experimental setups (e.g., model architecture, hyper-parameters) for section 3.2 and 4.2 are not provided.\n\n-The sentence above section 4.3 reads: “some generated images are exactly the same as the pixel-wise averages of the training data”. How these (“some”) images were selected? And how the training images being averaged were identified?\n\n-Section 5.3 claims the proposed method could improve FID for up to 30%, but the baseline results are questionable. Although the paper reported using models such as LS-GAN and SA-GAN (without details about the architectures), the baseline FIDs were much worse than reported in these papers.\n\n-It seems unfair to study the effects of different batch sizes using the same learning rate, as learning rate generally needs to be reduced for smaller batch size. For example, similar study on batch size has been conducted in previous papers including Improving GANs using optimal transport (Salimans et al. ICLR 2018) and large scale GAN training for High Fidelity Natural Image Synthesis (Brock et al. ICLR 2019)."
        }
    ]
}