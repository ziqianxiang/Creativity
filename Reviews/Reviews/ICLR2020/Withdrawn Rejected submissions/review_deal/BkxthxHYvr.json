{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper aims to generate molecules with desired properties using a variant of supervised variational auto-encoders. Disentanglement is encouraged among the style factors.  The reviewers point out that the idea is nice, but authors avoid quantitative comparison with SotA Graph-based generative models.  Especially, the JT-VAE is acknowledged as a strong baseline widely in the community and is a VAE-based model, it is important to do these comparisons. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "# Summary\nThe paper considers the problem of generating molecules with desired properties using a variant of supervised variational auto-encoders. The key novelty is the idea to learn a representation with disentangled molecular properties which can be modified during generation of novel molecules.\n\nTo account for the diversity of molecules in a particular target class the conditional probability of a molecule x given a target property y is modelled with a latent variable z, i.e., p(x | y) = \\int p(x | z, y) p(z) dz.\n\nThe \"style transfer\" is achieved by taking an initial molecule x and computing the posterior of novel molecule x' with modified property y' via p(x' | y', x) = \\int p(x' | y', z) p(z | x) dz.\n\nThe latent representation is learned using a supervised auto-encoder (Kingma et al., 2014), formally specified in Eq. (2). The optimization problem is then further extended by imposing a soft constraint that the molecules from a particular class exhibit a certain property (Eq. 3). The reason for this constraint is to enforce that the conditional probability p(x | y, z) takes into account the information in label y. This part might not be sufficiently well-motivated and would benefit from strengthening the argument for the property predictor and soft constraint (would an illustration be possible here?). In particular, why should it not be possible to encode (and keep fixed during training) the information present in y into the sufficient statistics of p(x | y, z)?\n\nThe soft constraint involves an oracle function (see Eq. 3), which evaluates the designed molecules. Section 3.2 deals with the approximation of the oracle via a property predictor function to side-step the fact that the oracle is not necessarily differentiable or CPU-bound. Section 3.3 and 3.4 deal with joint optimization of the supervised variational auto-encoder and gradient estimates.\n\nThe approach is evaluated on two datasets, relative to state-of-the-art baselines based on variational auto-encoders and deep learning. The main goals of the experiments are to establish that the approach can learn useful disentangled conditional distributions over molecules and to assess the extent of improvement in the data generation process as a result of using the soft constraint term. The focus of the experiment is on generating molecules with certain range of logP values. The data generating process is simulated/evaluated while controlling for the logP value range and molecular structure.\nThe experiments provide an objective assessment of the merits of the approach and indicate clear advantages over prior work. \n\n\n# Recommendation\nThe paper is very well written, easy to follow, and properly structured. The work is novel and the idea to disentangle molecular properties from the target property is quite interesting. The experiments are detailed and compare to baselines based on variational auto-encoders and deep learning. The part that could be improved is the choice of the target property. In particular, it would be great to evaluate the approach on the problem of finding molecules binding to a certain protein site. Such a problem is likely to exhibit scaffold hops and activity cliffs which make drug design problems very difficult (e.g., see [6-8]).\n\n\n# Related work (additional references)\n- While mapping of the discrete space of molecules to a continuous latent space (in active learning approaches combined with VAE) allows for gradient computation and the use of active learning/optimization techniques, the very difficult problem of finding latent space pre-images persists (in general, the problem should be in the NP class). The latter refers to finding a molecule that maps to a fixed point in the latent space. There have, however, been some efficient approximations in special cases [1-3].\n- In [4] and [5], an approach for generation of molecules with desired properties has been pursued with posterior sampler based on a conditional exponential family model. The representation is based on a tuple kernel mapping mapping (x, y) to some reproducing kernel Hilbert space (without disentanglement). The approach provides a consistent data generation process and a mean to deal with the fact that designs are not independent and identically distributed (e.g., Eq. 2 assumes that examples are IID).\n\n\n# Black-box target property\n- The logP property might not be a good proxy for the effectiveness in generating molecules with desired binding activities (to some protein site) because (to the best of my knowledge) it does not exhibit the \"activity cliff property\" characteristic to drug design. This refers to the property that a small change in molecular structure results in a completely different activity level (e.g., see [6-8] and [4]). In this sense, the \"style transfer\" might be beneficial for finding molecules binding to a certain protein site that are structurally similar to some available molecules with bad activity levels (for which a synthesis path might be known or easy to derive).\n- An actual black-box property (realized via a docking program) that is expensive to evaluate served as a target property in [5]. That docking program (supplied with suitable binding/docking constraints) can provide a nice proxy for the actual generation of molecules.\n\n\n# References\n[1] G. H. Bakir, J. Weston, and B. Schoelkopf. Learning to find pre-images, NIPS 2004.\n[2] C. Cortes, M. Mohri, and J. Weston. A general regression technique for learning transductions, ICML 2005.\n[3] S. Giguere, A. Rolland, F. Laviolette, and M. Marchand. On the string kernel pre-image problem with applications in drug discovery, ICML 2015.\n[4] D. Oglic, R. Garnett, and T. Gaertner. Active search in intensionally specified structured spaces, AAAI 2017.\n[5] D. Oglic, S. Oatley, S. Macdonald, T. Mcinally, R. Garnett, J. Hirst, and T. Gaertner. Active search for computer-aided drug design, Molecular Informatics 2018.\n[6] G. Schneider and U. Fechner. Computer-based de novo design of drug-like molecules, Nature Reviews Drug Discovery, 2005.\n[7] P. Schneider and G. Schneider. De novo design at the edge of chaos. Journal of Medicinal Chemistry, 2016.\n[8] J. Scannell, A. Blanckley, H. Boldon, and B. Warrington. Diagnosing the decline in pharmaceutical R&D efficiency. Nature Reviews Drug Discovery, 2012."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a VAE-based conditional molecular graph generation model. For that purpose, the disentanglement approach is adopted in this paper: learn to separate property information from the structure representation of a molecular graph. The authors use the supervised VAE objective since the KL regularizer in the objective has reportedly disentanglement-promoting effect. The final objective function is a standard VAE ELBO plus a penalty term of the property value prediction error. Non-differentiable property estimation is conducted via stochastic sampling expectation with the help of an external program (RDKit).  \n\nThe proposed model directly optimizes the generation model conditioned on the property values in a single objective function. This is preferable compared to many existing molecular graph generations, where property optimization is often carried out after the core generative models are trained and fixed. \nDerivation of a stable lowerbound (Eq. 11) is another plus. \n\nMy main concern about the paper is a weak survey for disentanglement researches. Since the core of graph generation model is not original of the authors (adopted from Dai+, 2018), the main technical advancement of the paper should be related to the disentangling VAE modeling. \nCurrent researches in disentangling VAEs go beyond the InfoGAN and beta-VAE. I present a part of the must-referred papers below:\n\n[Gao19] Gao+, “Auto-Encoding Total Correlation Explanation”, AISTATS, 2019. \n[Kim_Mnih18] Kim and Mnih, “Disentangling by Factorising”, ICML, 2018.  \n[Ryu19] Ryu+, “Wyner VAE: Joint and Conditional Generation with Succinct Common Representation Learning”, arxiv:1905.10945, 2019. \n\nAmong them, [Ryu19] is closely related to the proposed framework. In my understanding, the variable dependency structure studied in this paper (Fig. 1) is studied by [Ryu19]. The authors should clearly state the novelty of the proposed work in the literature of these disentanglement VAE works. \n\nI think the L_disent (Eq.5) is not a penalty for disentanglement: it enforces the model to correctly predict target property values, and do not say anything for factor disentanglement, correct? \n\nI have a few concerns about the experiment designs. \nIn the table 1, reconstruction performance evaluations, the authors chose string(SMILE)-based molecular graph generation models. However, it is largely admitted in the molecular graph generation studies that the graph-based generative models generally performed better. I want justifications for the choice of string-based generation models. Extending the table 1 with graph?base SotA generation models will strengthen the manuscript greatly. \nFor example:\n\n[Jin18] Jin+, “Junction Tree Variational Autoencoder for Molecular Graph Generation”, ICML, 2018. \n[You18] You+, “Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation”, NeurIPS, 2018. \n\nEspecially, [You18] directly optimize the property values of the generated graph. This is closely related to the goal of this paper, thus a proper reference and discussions are strongly expected. \n\n\nFigure 5. is an attractive visualization of the latent (z, y) vectors. However, the authors do not provide how to understand the figure. I GUESS that the authors want to show that learned z and y are somehow disentangled: the structural changes of molecular graphs are less sensitive to the vertical axis (property value) than the horizontal column (different). Please clearly state key messages of each figure and table. \n\nThere are several presentation issues. They are details, but fairly degrades the readability of the manuscript.  \n- Too small letters (alphabets) in Figure 3, 4, 6, it is simply unreadable so I cannot tell main messages of these figures (So I do not evaluate these figures positively). \n- In table 1, what are the significance figures of the presented scores? Some scores have 3 digits, others have 4 digits. Some are rounded at 0.01 precision but others are round at 0.1 precision. \n\nEvaluation summaries\n+ A graph generation model combining conditional property optimization in a single objective function.\n+ A new lowerbound for stable training (Eq.11)\n+ The property and the structure of the graph are somehow disentangled in the experiment (Fig. 5). \n+- Disentanglement effect is not modeled directly in the proposal. L_disent is not for disentanglement but for property regression.\n-- Survey for disentangling VAEs is not enough. This makes the proposal less convincing in terms of the novelty and the contribution compared to the recent dissent-VAEs. \n- Not compared with SotA graph-based molecular graph generation models. \n- Key messages of figures and tables are not clearly stated (e.g. Fig.5). \n-- Some figures are simply unreadable. The significance figure of the table 1 is unclear. These are formatting details but essential for readability. "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The authors introduce a variational autoencoder for conditional generation of molecules. The model is borrowed from text-based style transfer, applied here on sequence (SMILES) representation of molecules rather than viewing molecules as graphs (as more recent approaches). From a modeling point of view, the main new part is an additional regularizer whose role is to 1) ensure that the property used as input during generation matches the property derived from the generated molecule, and 2) to dissociate the latent molecule representation in the autoencoder (loosely speaking, its overall structure) from the property being controlled. This regularizer is just a squared difference between predicted and actual properties, averaged over independent samples from latent states and properties which are parametrically mapped to predicted properties via the decoder state (so as to be able to backprop).\n\nThe resulting ELBO criterion for the VAE, together with the regularizer, could be in principle directly used to train all the model parameters (encoder, decoder, property mapping) but apparently this criterion does not result in a \"stable\" estimation procedure. The model is then amended in various ways. \n\nThe first change consists of introducing a property prediction part directly into ELBO so that the posterior over latent states will also be affected by property prediction (previously property prediction would only appear in the additional regularizer). Beyond stability that the authors cite as the reason, this is likely also needed because in eq (2) the posterior over latent states given the molecule does not depend on the property. The posterior is derived from a v-structure where latent states and properties appear as independent parents of the molecule. Since for the training cases the property is fixed, this would be fine with an arbitrarily complex posterior encoding. However, when the encoder is defined parametrically, this dependence may be needed, and may relate to the stability issues. It seems a bit roundabout way of putting the dependence back in, if so. The authors should comment on this further. \n\nThe other simplification that is introduced is that the prior over the latent states in the regularizer is modified to be the average posterior of training encodings, and later further collapsed (for computational reasons) to an adaptively estimated simple Gaussian with variance that matches the average posterior variance.\n\nIt's a bit unsatisfying that the authors only use and experiment with a simple logP as the property to control in the model. This is not really a particularly relevant property to control. The approach should in principle generalize to other properties but the many adjustments needed to make it work makes this a little questionable. Also, the results are a bit limited (only logP) and not encouraging (see comments below). \n\n- how is validity defined? SMILES syntax or validity as a molecule? \n\n- is table 1 reconstruction really autoencoder reproduction percentage? Is the correct property for the molecule being reconstructed offered as an additional input to the authors' model? isn't this a bit unfair?\n\n- the correlation between property given as input and the property derived from the generated molecule does not seem like a good metric. Wouldn't a model that overfits (essentially spitting out the nearest property neighbor from the training set) do particularly well according to this metric?\n\n- it seems Figure 6 can be interpreted as also indicating some degree of overfitting?\n\n- the stacked LSTM molecular generator baseline that takes the property as input seems to do much better in terms of maintaining the property in its generated molecules and it also realizes a more diverse collection of molecules. It's true that it doesn't support incremental molecular manipulation directly, eg, if one is keen on keeping the modified molecule close to a starting point. But it is so much better at maintaining the input property in its diverse realizations that perhaps one should instead invest in a tailored sampling procedure to obtain realizations close to a chosen reference.\n\nThe literature references seem to omit all molecular generation work since 2018 \n"
        }
    ]
}