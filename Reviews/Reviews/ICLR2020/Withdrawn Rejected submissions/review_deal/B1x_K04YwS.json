{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper describes a unified span-relation-based formulation that can be used to solve a range of structured prediction tasks, including coreference resolution, semantic role labeling, dependency/constituency parsing, relation extraction, open IE, etc. The authors consolidated 10 such NLP tasks into a benchmark dataset called GLAD. \n\nThe model described in this paper is a span-related-based model proposed in a series of previous works (Lee et al., 2017, 2018; He et al., 2018; Luan et al., 2019), which builds fixed-length span representations and classifies span-span relation labels or span labels. The model achieved near state-of-the-art results on the GLAD benchmark, and showed mixed results when trained with the multi-task setup.\n\nOverall, the paper pretty is well-written and well-organized. The technical contribution seems a bit thin, as this span-based multitask model has been introduced and well-explored in previous works. That being said, I have to give the authors credit for consolidating so many different NLP tasks and performing experiments on them. My other concern with this paper is that I'm not sure how to interpret the MTL results. Unsurprisingly, there is a mix of positive and negative results across the different task pairs. It would be nice to see a bit more analysis beyond sharing representations and comparing the final F1 numbers.\n\nOther questions/comments:\n- What is STL? Single-task learning?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "- Overview: This work presents a unified formulation of various (phrase and token level) NLP tasks. They use this formulation to devise a single model architecture that, when trained individually on 10 diverse NLP tasks, can get near SOTA performance on all of them. They additionally conduct experiments to determine when multitask learning is useful for this set of tasks, which is made easy due to the unified framework. The stated main contributions are\n    - unified formulation of 10 diverse NLP tasks, including NER, constituency + dependency, parsing, POS tagging, sentiment, coref., relation extraction, SRL, information extraction, and more\n    - a benchmark of pre-existing datasets for each of these tasks\n    - experiments on their benchmark in pushing the state of the art \n    - experiments leveraging their task formulation and benchmark to analyze the effectiveness of multitask learning under a variety of settings.\n\n- Review: The paper as-is is not as novel as the authors claim, and I recommend a weak reject. That said, I think there is a lot of interesting work and analyses in the latter half of the work, and I think with some reframing, this could be a really nice work.\n- I'm not convinced of the novelty of the formulating various NLP tasks as span and and relation labelling. As the authors point out (somewhat concerningly buried on a footnote on page 4), Tenney et al.[1] use present a very similar framework for these tasks, also with the aim of having uniform task format and model architecture. The authors rightly point out a key difference between their work and [1]: The latter's work uses this unified framework in the context of analyzing pertained models (with no training), rather than training a model to perform as well as on these tasks as possible. As a result, [1] uses gold span information, whereas this paper also needs to predict those spans. While this difference in setting is fairly substantial, it doesn't change the fact that [1] previously unified a similar suite of tasks with more or less the same framework, which I believe significantly weakens this work's main stated claim of novelty.\n- I appreciate the effort to standardize analysis tasks in NLP. However, I think there's some tension in trying to group the proposed set of tasks into a sub-sentence-level language understanding benchmark and calling it an analysis tool. In providing a training set for each task and expecting users of the benchmark to train/fine-tune on each task, we lose the ability to say a pretrained model contains information for X task (e.g. \"My model has a strong sense of named entities\"). On the other hand, I suppose you gain some insight into how well your pertained model can be adapted for these NLP tasks, which is also useful but I think not how analysis tasks are usually implemented.\n- Matching previous (single-task) SOTA on each of the 10 tasks individually doesn't seem hugely interesting to me. It doesn't seem too surprising that pretained LMs perform well regardless of the task format / architecture. We see this in question answering where concatenating the question to the context is as effective as QA-specific architectures [2].\n- That being said, I think there is still a lot of interesting work and results here. For me, the most interesting part is the analysis for which tasks are mutually beneficial or harmful. Determining and characterizing mutually beneficial tasks is a big open question, and these experiments are useful evidence and should be leaned into more, especially over the unified task formulation aspect. There's a lot more that could be done here, e.g. flushing out the experiments testing the various tasks under different amounts of training data or deeper analysis into why certain task pairs are helpful/harmful (genre? both are semantic-y or syntactic? lots of word overlap between the datasets? etc.).\n\nThings to improve + Questions\n- The paper would strongly benefit from a more in-depth explanation of the covered tasks, perhaps in an appendix. The explanations on p3 feel redundant. The examples in Tables 2a,b do a lot of work, but some of the tasks and labels still could use more explanation. \n- I didn't really understand the phrase \"all pairs of remaining spans\". Are those the top-K / lowest NEG_SPAN probability spans?\n- What do $j$ and $k$ index ?\n- The conclusions from Tables 5 + 6 seem to contradict one another. Table 5, as you say, seems to suggest that \"as the contextualized representations become stronger, the performance of MTL+FT becomes more favorable\". On the other hand, Table 6 says that there seem to be many more mutually beneficial task pairs when using GloVe than when using BERT. Any hypotheses as to why this is ?\n- Figure 2 is hard to read because of the size and the legend formatting.\n\n[1] WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipanjan Das, Ellie Pavlick.\n[2] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper generalizes a wide range of natural language processing tasks as a single span-based framework and proposes a general architecture to solve all these problems. Specifically, each task can be modeled as either span identification/prediction or relation prediction based on the spans. The authors adopt an existing approaches for span-based problem with minor modifications and show the final performances on various tasks. This paper is in general well-written and easy to follow. The problem and motivation are clearly stated. The contributions involve a unified NLP task modeling with extensive results, investigation on multi-task settings by jointly training the model on different tasks, and a collection of benchmark datasets annotated with span-based information to facilitate future researches. However, a few limitations are observed:\n\n1. Both span-oriented tasks and relation-oriented tasks involves heavy computation on all possible candidates. For example, the span prediction involves an MLP for each possible span with certain window threshold. This may lead to heavy computational cost and non-intuitive learning process. Many sequence labeling methods actually exists to provide more efficient learning strategy. Is there any reason on why this specific base model is chosen? What's the advantage of using this model compared with the others? Can you show the time complexity involved in the experiments?\n\n2. There is only minor adjustment made on the original model for span-based predictions, which limits the contribution of this paper. As shown in Table 4, the performance does not dominate most of the times.\n\n3. The authors claim that they investigate their general framework on multi-task settings. However, only statistical results are provided by using different combinations of tasks as source and target tasks without further analysis on how the model could benefit on this setting. In fact, it could make more contribution by providing a training strategy using a single framework on multiple tasks so that the performance for each task is increased compared to separate training, or a pretraining strategy might be involved (similar to BERT) for such span-based problems. This could bring much more attention to the NLP society."
        }
    ]
}