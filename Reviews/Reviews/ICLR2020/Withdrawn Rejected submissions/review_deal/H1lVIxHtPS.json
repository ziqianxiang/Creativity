{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The authors propose a multi-agent communication framework(SARNet) using memory-based attention network. The main framework is to facilitate inter-agent communication and prior\ninformation to help decision.\n\nMain issues:\n1. In the Introduction part, the author claim the TarMAC does not use memory in the communication step, however TarMAC uses GRU policy to generate action and message, it does use the memory to generate message. So maybe you should look up it again.\n2. The main framework is so similar to TarMAC, I can hardly to conclude the novelty in this paper. Can you make a more detailed comparisons with TarMAC? Can the Questiion Unit be considered as the Targeted Communication part in TarMAC? If so, the main difference between SARNet and TarMAC is they use different ways to aggregate information in Memory Unit?\n3. For the experiment part, there need more figures and analysis. I think there may be need some learning curve figure or the final reward to prove the SARNet outperformed baselines. For example, in POCN, the agents are guided by dense reward, the the reward do measure the performance. The paper just provided the collisions and average dist in the environment, which is a good way to analysis emergent behavior but not enough to evaluate the algorithm. The attention visualization in Figure3 should place the correspond scene to demonstrator the learnt attention is meaningful. The main contribution from my perspective is the Memory Unit, however the paper just do the ablation study with noise in memory unit, this is not enough. The noise in memory part can be reasoned to the robustness problem in neural network also. You can test whether agents choose different strategies for same situation when their experience is different."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary:\nThe paper deals with the problem of training cooperative agents in a multi-agent reinforcement learning setting. The agents are given the ability to communicate with each other and are trained using the centralized training, decentralized execution paradigm. \n\nArchitecture has been proposed that: (i) attends to relevant communication messages received by an agent using an attention mechanism similar to the one proposed in [1]; and (ii) endows each agent with a memory. The authors argue that memory assists the agents in performing explicit reasoning.\n\nThe proposed architecture consists of four units:\n1. Thought unit for processing local observation to obtain query, key and value vectors along with an observation encoding.\n2. Question unit for obtaining an attention distribution over all agents (using one's own query vector and the [key, vector] pairs received from all agents) via a modified version of attention mechanism proposed in [1].\n3. Memory unit for using the attention weights obtained in step 2 to update agents memory\n4. Action unit for picking action based on agent's memory and local observation.\n\nExperiments done with cooperative agents on three different tasks from multi-agent particle environments show that in some cases the proposed architecture outperforms other approaches. Two experiments that aim to understand the role of attention and memory have also been performed.\n\n\nComments:\n1. The Introduction section lacks motivation. An example to explain terms like structured reasoning would help a lot.\n\n2. At least to me, it is not apparent how the memory or attention leads to an \"explicit\" reasoning process.\n\n3. In section 2.1 A_i and O_i are sets and not actions or observations respectively.\n\n4. On p5, in the paragraph before Section 3.3, it is written that \"... it learns to assign importance to each element in the information vector.\". Further clarification is needed here. As the weights act on the entire vector in the memory unit, the statement appears to be a bit misleading.\n\n5. The current setup requires the key-value pairs to be broadcasted to all agents. This would be inefficient when a large number of agents are present - which is essentially the case where one would want to filter out information using such an approach.\n\nThe paper is overloaded with terminology that can be avoided and the presentation can be simplified. There is redundancy in writing and many statements are vague, e.g., the one that talks about performing explicit structured reasoning. The description can be condensed and space can be used for more informative ablation studies.\n\n\nQuestions to the Authors:\nSee the comments above.\n\n\nReferences:\n[1] Vaswani et al., 2017, Attention is all you need.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper presents a new method SARNet for multiagent communication which uses ideas from transformers as well as MAC to perform better than baselines. The model first extracts out important stuff from current information (broadcasted communication from other agents as well own private observation) and then extract important information from previous memories it has relevant to current step. The model is trained using centralized learning and decentralized execution approach. Then, paper empirically shows better results than baselines on OpenAI’s multi-agent particle environment.\n\nThe paper is well written and the flow is easy to read. The SARNet is usually better in larger scale experiments but it is hard to say if it will generalize to other environments as all experiments are on multi-agent particle environment. Therefore, due to lack of clear positive signal from few experiments and issues mentioned below, I provide an initial rating of weak reject which I would be happy to increase once following concerns are resolved. \n\nFor a proper comparison, it would be good to compare the number of parameters as well as plots for rewards and convergence with respect to baselines. Are the parameters among the agents shared in any way? \n\nHow many runs were used for the final runs? From the paper, it seems like it is only one run which won’t capture variance. [1] which came between CommNet and TarMAC is missing from baselines. Using [1] will allow SARNet to be extended to non-cooperative settings. Also, the setup optimizes global rewards, but [1] shows that individualizing the rewards helps in overall performance. It would be great to know if individualized rewards can help SARNet further.\n\nAn ablation that is missing from the paper is one where you only use memory unit and disable the attention mechanism. It would give a signal on how much memory helps. The paper is missing proper qualitative analysis on what agents communicate and if there are some sort of common signals there. Analysis like the ones in CommNet [2] paper would be helpful to understand what is being communicated. Surprisingly, in 4.4, MC-SARNet is performing better on #Captures ‘N’, any explanations on that. Do the titles of table in Table 3 need to be exchanged? \n\nAlso, how is the mean reward getting calculated in partially observable physical deception task? I didn’t understand that which confused me about Table 2(b) and results for section 4.3. The gap between Average distance for CommNet and SARNet is huge for N=3, but SARNet performs better for N=6. It is hard to derive a conclusion based on just these two. What happens when N=9 is tested, does SARNet again beat the baselines. If it does, we can make a more solid claim about performance. Same goes for the rest of the tasks.Section 4.3 is only evaluated on one setting and Section 4.2 is only evaluated on N=3 and N=6.\n\nI find the metrics for the particles environment a little coarse which might be adding noise to the overall signal. Something like Traffic Junction provides clear signal on the improvements.\n\nTypos/Clarification:\n\n3.1 Through a structured reasoning\n3.1 the model interacts with memories -> Are memories shared in communication directly?\nPage 4: The key and value are broadcasted\nI might be missing something but are the dimensions of a_i^t and similar variables correct, it seems like they should R^{NxQ}\nTable 3(a) - Memory in predator-prey\n\n[1] Singh, Amanpreet, Tushar Jain, and Sainbayar Sukhbaatar. \"Learning when to communicate at scale in multiagent cooperative and competitive tasks.\" arXiv preprint arXiv:1812.09755 (2018).\n[2] Sukhbaatar, Sainbayar, and Rob Fergus. \"Learning multiagent communication with backpropagation.\" In Advances in Neural Information Processing Systems, pp. 2244-2252. 2016."
        }
    ]
}