{
    "Decision": {
        "decision": "Reject",
        "comment": "There was extension discussion of the paper between the reviewers. It's clear that the reviewers appreciated the main idea in the paper, and the notion of an \"online\" meta-critic that accelerates the RL process is definitely very appealing. However, there were unanswered questions about what the method is actually doing that make me reticent to recommend acceptance at this point. I would refer the authors to R3 and R1 for an in-depth discussion of the issues, but the short summary is that it's not clear whether, if, and how the meta-loss in this case actually converges, and what the meta-critic is actually doing. In the absence of a theoretical understanding for what the modification does to accelerate RL, we are left with the empirical experiments, and there it is necessary to consider alternative hypotheses and perform detailed ablation analyses to understand that the method really works for the reasons stated by the authors (and not some of the alternative explanations, see e.g. R3). While there is nothing wrong with a result that is primarily empirical, it is important to analyze that the empirical gains really are happening for the reasons claimed, and to carefully study convergence and asymptotic properties of the algorithm. The comparatively diminished gains with the stronger algorithms (TD3 and especially SAC) make me more skeptical. Therefore, I would recommend that the paper not be accepted at this time, though I encourage the authors to resubmit with a more in-depth experimental evaluation.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "I am very torn about this paper as the proposed approach is a fairly straightforward extension of past work on the meta-critic approach to meta-learning and the results are pretty good, but nothing amazing. I tend to accept this paper because I like their general direction and think what they are proposing is pretty simple with broad applicability. It should be fairly straightforward to append this idea to most new off policy methods as they come out, so I find their consistent gains across 3 different popular models pretty convincing that this could have value to the community.  \n\nThat being said, the gains are not huge, which does make me think about the potential computational overhead. How much more run time per step does the meta-critic add to the models in the paper? I am a bit worried that the comparisons are not apples to apples from the perspective of the amount of computation/update steps per environment interaction due to the use of the validation data. I wonder if it changes the conclusion at all if the other approaches are given the same amount of computation on their replay buffer between interactions with the environment. For example, more optimization steps on the buffer is another plausible explanation why the meta-critic does better at optimizing the loss. \n\nI also should note that this paper is not the first to propose conducting online meta-learning over a replay buffer. A paper at last year's ICLR [1] did so in the context of lifelong learning, but does not need task labels or tasks and was tested on single non-stationary environments as well. The Meta-Critic approach of this work, of course, still is cool as it does for learning with a Meta-Critic what Meta-Experience Replay does for optimization based meta-learning. However, I thought this should be pointed out as the novelty can be a bit overstated at times. Additionally, the paper would be significantly improved by fleshing out the theoretical motivation for the Meta-Critic approach in more detail. What are the underlying reasons why we would expect it to generically improve single task RL? \n \n[1] \"Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference\". Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. ICLR-19. \n\nThoughts After Author Feedback:\n\nI really appreciate the response of the authors to my review, which included some interesting new experiments and explanations addressing concerns I raised. I do, however, also see where reviewer 3 is coming from with both major comments. \n\nI don't think that R3A1 is particularly clear and it is an important concern. I think reviewer 3 is saying that Delta(t) in R3A1i will eventually converge to 0 when the policy stops changing while the author argue that it \"need not converge if there are fluctuations at each iteration\". So it not converging seems to be tied to the existence of some source of non-stationarity in the problem. This doesn't seem to be coming from the environment as they are considering single task settings. As a result, I believe the source of the non-stationarity in this case is the fluctuating parameters. Looking at figure 6a it does not seem obvious that the policy has really converged in the traditional sense as its score does seem to be changing to some degree throughout the chart. My best guess is that this is the reason why the meta-loss does not converge. However, I still totally agree that this is a major concern that is very much under addressed. \n\nI also agree that I found the comments about what the meta-critic is doing unconvincing. The authors provided a few different kinds of explanations of what the model could potentially be doing, but this approach to the answer really highlights  how the theoretical benefits of this approach remain unclear. I think it should be possible to directly verify some of these theories with well designed experiments. It feels like a better explanation is necessary in light of the often small margin of difference with baselines before publication. \n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In actor-critic algorithms, the policy tries to optimize cumulative discounted rewards by a loss formed with components from the critic. In this paper, the authors propose a novel way, namely, meta-critic, which utilizes meta-learning to learn an additional loss for the policy to accelerate the learning process of the agent. There are several advantages of the proposed method, it's learned online for a single task, it's trained with off-policy data, it provides sample efficiency and it's generally applicable to existing off-policy actor-critic methods. \n\nOverall the proposed method is novel and the research direction is a very interesting one to explore. Eqn (3) and Eqn (4) explains the key idea of the proposed method. Eqn (3) describes the meta-learning problem as a bi-level optimization problem where the agent is updated with the main loss L^main with data d_train, in addition, it's updated with L^aux where the loss is learned and parameterized by \\omega. After the agent being updated, it uses L^meta and data d_val to validate the performance of the updated agent. Eqn (4) describes an explicit way to formalize the usage of the auxiliary loss, which is to accelerate the learning process. Thus the meta loss is whether the L^aux helps the learning process or not. \n\nHope that the authors could address the following issues in the rebuttal:\n1) Investigate why DDPG with meta-critic gets much more improvements than TD3/SAC;\n2) Show SAC (or TD3) could get better performance on harder task. (I can understand that they are strong baselines and hard to improve on the current environments, however, for harder environments, there might be rooms for improvements.);\n3) Investigate different ways to parameterize the meta-critic other than simple MLP;\n\nA few minor points:\n1) Page 2 first paragraph, \"This is in stark contrast to the\nmainstream meta-learning research paradigm â€“ where entire task families are required to provide\nenough data for meta-learning, and to provide new tasks to amortize the huge cost of meta-learning.\". Try to revise it and avoid the words like \"mainstream\". Think about the paper being read 5 years later or even more;\n2) Consider introducing a weighting hyperparam between L^main and L^aux in Eqn (3), these two losses might have different scales and it might be better to weigh them differently;\n3) Minor literature detail: Page 7 \"Comparison vs PPO-LIRPG\" mentioned that \"Intrinsic Reward Learning for PPO (Zheng et al., 2018) is the only existing online meta-learning method that we are aware of.\", however, AFAIK, \"Meta-Gradient Reinforcement Learning\" in NeurIPS 2018 and \"Discovery of Useful Questions as Auxiliary Tasks\" NeurIPS 2019 are methods where meta-learning is applied online and for a single task;"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a meta reinforcement learning approach where a meta-critic is trained in addition to a conventional actor and critic, so that the meta-critic boosts the training of the actor over a single task. The approach is combined to DDPG, TD3 and SAC and is claimed to convey superior performance (or learning speed) over these state-of-the-art actor-critic algorithms.\n\nAt first glance, the paper looks convincing, but closer inspection reveals a potential issue that I would like the authors to discuss.\n\nThe first cue is that in Fig. 6, the meta-loss does not seem to converge to anything, as the authors say it just fluctuates. Shouldn't it converge once the actor converges to close to optimal performance? \n\nThe second cue is that appart from the rllab tasks (ant and half-cheetah), it is not clear that the meta-critic approach brings some gain in the end of training. Particularly in Reacher (Fig 7), the performance seems to collapse faster with DDPG-MC than with DDPG, and on the rest of curves of Figs 7 and 8 it is had to determine whether the MC approach brings something significant or not. And in Fig. 3, in Walker2D, TD3-MC looks rather unstable.\n\nThe third cue is that in Section 3.2, the paragraphs \"Updating MC parameters\" and \"Designing MC\" are rather unclear (I'll come back to that) and lack a theoretical justification.\n\nSo I'm wondering what exactly MC is doing and I would like to see a more detailled analysis. Couldn't a similar performance improvement be obtained by just increasing the actor learning rate and addiing some noise to the actor learning gradient? Isn't this more or less what the meta-loss does, when looking at Fig. 6?\n\nTo me, unless the authors give a clear answer to the points above, the paper should be rejected as it does not provide clear enough evidence and justification in favor of the proposed meta-learning approach.\n\nAs stated above, I found the paragraph \"Updating MC parameters\" lacking a principled justification.\n\nAbout \"Designing MC\", it could be much clearer. You first express two requirements (i) and (ii). Fine.\nThen you explain how to meet requirement (i), but you say this is not what you are doing (!), and then you try to explain what you are doing but without going back to the requirements. In particular, it is not clear at all why your design is \"permutation invariant\". You should be much more direct. Besides, the way to extract features and the relationship to batch-wise set-embedding should be made more explicit. Well, it is complicated and should be explained more clearly...\n\n\"TD3 borrows the Double Q learning idea...\": no, TD3 does more than this, as it uses the min between both critics, which double Q-learning and DDQN do not.\n\nYou use HalfCheetah-v2 from gym-mujoco and HalfCheetah from rllab. Why? One might suspect this is because the performance of the MC appraoch is only better with rllab...\n\nYou say you are using 10 million steps, but this is not always the case.\n\nIt is interesting to see that the performance of TD3 on Half-Cheetah collapses at around 4 million steps. Any idea why? It seems that in many papers experiments are stopped before performance collapses, and a strong study about this phenomenon is missing as the authors don't want to show this. This is an open call to readers: a paper on that would be great! :)\n\n\"SAC-MC\" gives a clear-boost for several tasks\": well, looking at the figures it is not so clear. Do you mean faster learning, higher final performance or both? I would like to see this claim backed-up by some proper statistical significance test and a clear specification of the number of seeds, etc. Fig.5 conveys the adequate data for such test if your claim is that this is the final performance that matters (but beware that curves are crossing eachother, so the final performance depends a lot on where you stop...).\n\ntypos:\n\np1 \"For example, ... (Zheng et al., 2018).\" is not a sentence (no main verb).\np2 \"the on-policy (approach?) needs to interact with (the) environment...\"\np2 \"is less effective than off-policy transitions.\" => unclear statement\np3 to assist(s)\n\nEquations (2), (6) and (8) should finish with a dot as they close a sentence.\n\nEq X => Eq (X) : use \\eqref{label} instead of just \\ref{label}\n\np5 It's input => Its\np5 two key technologies: I would not call this a technology. Ingredients?\np5 \"computational cost is ? by using\": missong word\n\np6: asmyptotic\n\np9: we removing\n"
        }
    ]
}