{
    "Decision": {
        "decision": "Reject",
        "comment": "Main summary: Novel rule for scaling learning rate, known as gain ration, for which the effective batch size is increased.\n\nDiscussion: \nreviewer 2: main concern is reviewer can't tell if it's better of worse than linear learning rate scaling from their experiment section.\nreviewer 3: novlty/contribution is a bit too low for ICLR.\nreviewer 1: algorthmic clarity lacking.\nRecommendation: all 3 reviewers recommend reject, I agree.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Authors use the PL condition to motivate a formula for learning rate selection. (they bound their loss by a quadratic with fixed curvature alpha, and use this to get learning rate).\n\nTheir analysis very closely mirrors one presented in \"Gradient Diversity\" paper, it uses the same assumptions on the loss. IE compare A.1 in the paper to the B.5 in \"Gradient Diversity\"\n\nGradient Diversity solves for batch size B after which linear learning rate scaling starts to break down, while this paper instead fixes B and solves for learning rate. Two results are comparable, if you take their learning rate formula in section 3.2 (need formula numbers) and solve for B which gives learning rate halfway between B=1 and B=inf, you get the same expression as \"critical batch size\" in equation 5 of gradient diversity paper.\n\nIt's not immediately obvious how to invert formula in Gradient Diversity paper to solve for learning rate, so I would consider their learning rate formula an interesting result.\n\nI also appreciate the breadth of experiments used for evaluation. \n\nThe biggest issue I have with the paper is that I can't tell if it's better of worse than linear learning rate scaling from their experiment section. All of their experiments use more iterations for AS evaluation uses than for LSW evaluation. They demonstrate better training (and test) losses for AS, but because of extra iterations, I can't tell that the improvement in training loss is due to number of iterations, or due to AS scaling."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper presents and evaluate an algorithm, AdaScale SGD, to improve the stochastic gradient decent in distributed training. The proposed algorithm claims to be approximately scale-invariant, by adapting to the gradient's variance. The paper is well-written and generally easy to read, although I didn't check all theory in the paper. \n\nThe approach is evaluated using five benchmarks (networks / dataset combinations) from different  domains. The results are promising and seems solid. \n\nThe paper is good and I like it, although I think the novelty and contribution is slightly too low for ICLR. The kind of tweaking and minor optimizations to provide some adaptivity (or similar) in existing and established algorithms and approaches that is presented in this is paper is very important from a practical perspective. However, from a scientific perspective it provides no significant contribution.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a novel rule for scaling the learning rate, called the gain ratio, for when the effective batch size (induced by synchronous distributed SGD) is increased. This rule for the gain ratio uses estimates of the trace of the covariance and the norm of the true gradient to determine the appropriate steplength. This results in a method with a R-linear convergence guarantee to a neighborhood that is not dependent on $S$ (which is called scale-invariant). The algorithm additionally tracks the sum of the gain ratios in order to determine the \"effective\" number of iterations taken, and cut the steplength appropriately. \n\nStrengths:\n\nThe gain ratio proposed in this paper is intuitive. I particularly like how the algorithm estimates the mean and variance information in an unbiased manner to determine an appropriate scaling for the steplength. The method is able to attain a R-linear rate of convergence and appears to perform well in practice for a wide variety of applications. The gain ratio is simple to estimate within a distributed framework.\n\nWeaknesses:\n\nI found some of the terms in the paper to be unclear or ill-defined. The original use of the term \"scale\" was unclear to me. Does this refer to the number of nodes in the distributed implementation? What is its relationship to batch size?\n\nI found the definition of scale invariance in this paper to also be unclear on first read. The claim is that the algorithm is scale invariant if its final model does not depend on $S$. What is the \"final model\"? As currently defined, the current analysis does not guarantee that the algorithm will reach the same final model (assuming that $f(w, x) = \\ell(h(w, x), y)$, i.e. a composition of a loss function and model), as the PL condition only ensures that one reaches a global minimum, which may not be unique. In fact, the analysis only guarantees convergence to a neighborhood. The description within the analysis appears to imply that scale-invariance is a property of the algorithm attached to its convergence property. Is this the case?\n\nThe definition of scale invariance is also already used in optimization to mean algorithms that are not modified when the objective is multiplied by a constant or an affine transformation. This adds to the lack of clarity, and I would suggest the authors use a different term for this kind of invariance (batch size invariant, or something like that?).\n\nIs the theoretical comparison between SGD and AdaScale fair? Note that one can prove a stronger convergence result with SGD because one can actually attain a Q-linear rate of convergence to a neighborhood (for a proof, see for example, Bottou, et al. (2018)). In particular, one should have something like (in the paper's notation):\n$$\\mathbb{E}[F(w_T) - F^*] \\leq (1 - \\gamma)^T [F(w_0) - F^*] + \\Delta.$$\nThis means that one can actually guarantee a fixed ratio of decrease in expectation to a neighborhood, whereas AdaScale converges linearly but not with a fixed ratio. \n\nSome other small questions regarding the theory and experiments:\n- Is there a reason why batch normalization was not tried for the CIFAR-10 experiments?\n- Is it possible for $r_{t - 1} \\gamma > 1$?\n- Why was it necessary to estimate $\\sigma^2 (w_t)$ and $\\|\\nabla F(w_t)\\|^2$ by both aggregating at the current iteration and exponential averaging? What happens if exponential averaging is removed?\n- What are the limitations of this method? How large of a batch size can one use with AdaScale before the algorithm breaks down (if at all)?\n\nAdditional Comments:\n\nThe algorithm is quite reminiscient of the steplength prescribed in Bollapragada, et al. (2018), which consider the steplength: \n$$( 1 + \\frac{\\sigma^2(w_t)}{\\|\\nabla F(w_t)\\|^2})^{-1}.$$\nThis gain ratio prescribed in this paper is the ratio between this quantity for two different batch sizes. Is there a clear explanation for why the relationship between these two quantities would arise?\n\nThis method could also be used for determining an appropriate scaling of the steplength in the sequential setting, when a larger batch size is used. Has this been considered?\n\nDespite the concerns regarding the clarity in writing and the rigor in the theory of the paper, I think that the algorithmic idea proposed in this paper is interesting, novel, and practical. Because of the lack of clarity and rigor, I have given this paper a weak reject, but I would be happy to accept the paper if my concerns above were addressed in the final manuscript."
        }
    ]
}