{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "\nSummary\n\n This paper looks at the semi-supervised learning problem, where a combination of labeled and unlabeled data is available. It builds off of two major ideas which follow the general trend of perturbing unlabeled data and applying a consistency loss between the original and perturbed outputs: 1) virtual adversarial training (VAT) which picks an adversarial direction for perturbation, and 2) mixup which interpolates between images or features of real data. The method specifically pertubs unlabeled examples by interpolating between itself and the adversarial example. Results are shown on synthetic and standard semi-supervised image datasets. \n\n  Overall, my decision is to reject this paper. The idea is an extremely simple combination of two prior methods, with little justification for why it is the correct thing to do. Indeed the ablation shows that other methods work similarly (ICT+VAT) and newer papers such as MixMatch (which was out in May and accepted to NeurIPS) perform better than this method with normal augmentations and mixup (with multi-view pseudolabeling), showing that the underlying thesis of the work is not necessarily the most important part of the semi-supervised learning problem. \n\nArgument: \n\n  - The set of justifications put forth for the method are not rigorous. While I agree that combining multiple methods such that a larger space of augmentations is used (\"our AdvMixup explores a more comprehensive searching area\"), it is not clear to me why *this* particular combination is more justified than any others one can come up with (mixup between PI perturbation, random augmentations, etc.). In the end, various things are tried and some work better than others. Indeed ICT+VAT already does very well. It's not clear to me that this paper therefore discovers anything novel in terms of generalizable information that can be used for other problems (for example). \n\n  - MixMatch performs better than this method, and has been out since May and accepted at NeurIPS 2019. I would not say it is concurrent given that it has been out for a while now. Further, it shows that the underlying thesis of the method in this paper is not necessarily the crux of the semi-supervised learning problem. \n\n  - VAT is known to be extremely sensitive in terms of its hyperparameters (e.g. epsilon), which has to be tuned per-dataset (you use the value from the paper, which did this tuning to a larger validation set). Does this method offer any decreased sensitivity to that?\n\n  - I would not say VAT has a low computational overhead; you need an additional forward and backward pass per batch, which is not insignificant.\n\nSome specific questions:\n\n  - Why do you use 5k validation examples for cifar-10 and what size labeled data does that correspond to (1k, 2k, etc)? This seems to go against the Oliver et al. recommendations (which you cite).\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposes AdvMixup which unifies the local and in-between neighborhood approaches. The key idea is to interpolate between a real example $x_i$ and an adversarial example $x_j^{(adv)}$ and use the interpolation between soft labels from the EMA model as the virtual target. They give an intuitive explanation of crafting more aggressive adversarial examples as verified in Figure 3. The authors evaluate AdvMixup on both synthetic data and widely-used benchmarks. AdvMixup outperforms previous baselines. An ablation study and discussion are also provided.\n\nThe paper is clear and easy to follow. The proposed method is very simple and straightforward given the literature. However,  it is hard to say this is novel enough for ICLR since it looks like a combination of existing techniques. The novelty is my main concern.\n\nAnd I wonder whether it is better to generate stronger adversarial examples. It would be more insightful if the authors consider more attack methods than the one-step approach and see whether there is a trade-off.\n\nHow about the performance of AdvMixup when the classes are larger, e.g. CIFAR-100 and ImageNet?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposes a consistency regularization method, called AdvMixup, for semi-supervised learning tasks. The approach enforces similar predictions between an unlabeled sample and its neighborhood samples. Each sample of the latter is generated by mixing an adversarial sample and a synthetic sample created by linearly interpolating a real sample pair. \n\nThe paper is easy to follow, and the experiments on both the Cifar10 and SVHN datasets show promising results. Nevertheless, I consider the idea here is a minor modification of the approach introduced in Verma et al., 2019. Additional, the experimental section is weak in its current form. \n\nMain Remarks:\n\n1. The proposed AdvMixup is very similar to ICT as introduced in Verma et al., 2019, where the neighborhood samples are created by interpolating a pair of real samples. In addition, the experimental results as shown in the last two columns of Table 2 for the SVHN data set indicate that the improvement of AdvMixup over the ICT method is less than 1%. Similar patterns can be seen in Table1 for the CIFAR10 dataset. In this sense, I consider the proposed AdvMixup approach has limited technical novelty and with minor improvement over the exist methods. \n\n2. The proposed method is evaluated on two small, easy image tasks, namely CIFAER10 and SVHN, using a 13-layer CNN. Experiments on a variety of data sets such as CIFAR100, ImageNet, and MNIST and with some other network architectures such as DenseNet and  wideResNet could significantly improve the quality of the paper. \n\n3. The way the synthetic samples are created in the proposed AdvMixup approach seems a bit similar to that of the 3-fold Mixup as discussed in Guo et al. 2019 (Mixup as Locally Linear Out-Of-Manifold Regularization), where a synthetic sample is created by interpolating three real samples. Ideally, using a 3-fold Mixup as the comparison baseline for evaluating the AdvMixup makes more sense to me.  \n"
        }
    ]
}