{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors sample ~100k faces from the YFCC100M dataset and conduct an Amazon Mechanical Turk study to annotate the faces with race, gender, and age, and perform experiments in an effort to demonstrate its utility. It is unlikely that this dataset is useful to fairness community for several reasons: 1) There exist comparable datasets 2) The experiments are not rigorous 3) The race categories are irresponsibly constructed 4) The Mechanical Turk study is not rigorous. \n\nThe authors themselves note that the DiF (Diversity in Faces) dataset was also derived from the YFCC100M dataset and contains even more annotated images (~1million). Based on Figure 1, “Racial compositions in face datasets,” simply undersampling DiF for equal class sizes ought to yield FairFace. DiF and FairFace only differ in their treatment of race – DiF annotates skin color and FairFace annotates “race” categories. Despite these similarities, the authors do not compare DiF and FairFace in their experiment section. The justification for this is unclear as they include another dataset that doesn’t have race annotations (CelebA) in the experimentation. The training and testing sets for FairFace and the considered datasets seem to be generated randomly. Why wasn’t FairFace compared to under/over sampled versions of the other datasets with equal class sizes? If that can achieve the same effect why is FairFace needed? Including these experiments would help improve the paper.\n\nThe usefulness of the proposed “race” category annotations are unclear as they seem to be arbitrarily constructed without regard to the fields of race and ethnic studies, anthropology, or social science. In constructing these categories, the authors skim over separate definitions of race and ethnicity (Schaefer, 2008) but decide to conflate the two, stating “in practice, these two terms are often used interchangeably.” While this practice may be fine in colloquial English this is not sufficient for defining data class boundaries. For example, the authors construct a Latino category stating, “Latino is often treated as an ethnicity, but we consider Latino a race, which can be judged from the facial appearance” with no citations of a similar practice, which aspects of facial appearance are Latino in this context, and an alarming disregard that self-identified Latinx people also self-identify and/or pass as Black/Native American/White. It is unclear whether one face can be annotated as belonging to multiple categories. As these categories are intended to help understand AI applications in a societal context, they should follow the conventions employed by formal fields studying society. \n\nCommon convention in HCI/survey methodology studies is to provide a clear definition of the annotation task. For example in the construction of ImageNet dataset, a definition of the target synset was provided as a Wikipedia link. What is the definition of the annotation task here? Consider again the Latino category. If the Latino category can “be judged by the facial appearance” what are the facial features the workers are told to look for? If none are provided the annotations are subject to the workers’ bias. It is unclear if the workers self-reported demographics are collected or understood by the authors as a potential source of bias. Adding more details on how the study was conducted would help improve the paper."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This manuscript introduces a new face dataset, FairFace, that is balanced in terms of race composition. The authors also performed extensive empirical validations on the new dataset as well as several existing face datasets to show that models trained on this new dataset do have better balance in terms of recognition error across different demographic subgroups. \n\nFirst of all, there is no doubt that, as a community, we all should work together to mitigate existing bias (unbalanced ratio in terms of race) in the dataset used to train automated recognition system. From this perspective, the manuscript does a good job in building and introducing the FairFace dataset and conducted a thorough comparison with existing face datasets. I appreciate the efforts the authors put in building it. My main concern is that the contributions of this paper are not well-aligned with the Call-For-Paper of ICLR. Specifically, although ICLR does have sub-field that aims at applications in vision, audio, speech, natural language processing and robotics, it more or less focuses on novel applications in these areas with techniques related to representation learning. On the other hand, the main contribution of this paper is not about any specific representation learning techniques or applications, but rather a novel dataset. Hence I believe it may find a better fit at other conferences/journals that have specific focus on this respect, e.g., CVPR or FAT*. "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a new face image dataset containing 108,501 images which is balanced on race. The authors define 7 race groups: White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, and Latino. Images were collected from the YFCC-100M Flickr dataset and labeled with race, gender, and age groups. The authors find that the model trained on the proposed dataset is substantially more accurate on novel datasets and the accuracy is consistent across race and gender groups. The authors also compare several commercial computer vision APIs and report their balanced accuracy across gender, race, and age groups.\n\nThe writing and presentation are good and easy to follow.\n\nMy concerns regarding this paper are as below.\n1) The main contribution of this paper is the dataset for face attribute analytics. And I acknowledge the authors' efforts to the whole community. However, for ICLR submission, this is not enough, missing some novel contributions on the level of methodology.\n2) How did the authors handle noise during construction of the proposed dataset?\n3) What are the resolution comparison statistics between the proposed dataset and others?"
        }
    ]
}