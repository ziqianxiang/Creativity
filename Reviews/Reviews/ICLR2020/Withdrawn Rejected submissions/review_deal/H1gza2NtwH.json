{
    "Decision": {
        "decision": "Reject",
        "comment": "The reviewers all appreciated the importance of the topic: understanding the local geometry of loss surfaces of large models is viewed as critical to understand generalization and design better optimization methods.\n\nHowever, reviewers also pointed out the strength of the assumptions and the limitations of the empirical study. Despite the claim that these assumptions are weaker than those made in prior work, this did not convince the reviewers that the conclusion could be applied to common loss landscapes.\n\nI encourage the authors to address the points made by the reviewers and submit an updated version to a later conference.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper compares the true hessian and the empirical hessian of the loss function, showing the spectrum of the Empirical Hessian is generally broadened, and proposes a way to visualize the spectrum. The paper is well written with sound theoretical reasoning and empirical validation. However, the reviewer has the following concerns:\n\nAs to the assumption in this paper, it is too strong to assume independence between different elements of Hessian, Wigner ensemble seems to be a better model, since the only independence only comes from samples instead of the elements of the Hessian. This would require a completely different analysis to deal with.\nFurthermore, it seems that Lemma 1 and Theorem 1 are asymptotic results. I don’t see how these are related to the empirical validation, since in experiments the compared dataset are CIFAR-10/CIFAR-100 and augmented CIFAR-10/CIFAR-100, which only differs in the number of samples. It makes more sense to have a non-asymptotic analysis here.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "\nIn this paper, the authors uses the random matrix theory to study the spectrum distribution of the empirical Hessian and true Hessian for deep learning, and proposed an efficient spectrum visualization methods. The results obtained in the paper can shed some lights on the understanding of existing optimization algorithms (e.g., first order methods).\n\nWhile this paper is quite interesting, I kind of feel that it has limited value to the research community due to the following concerns.\n\n1)\tThe work is based on some assumptions, which is however not very reasonable.  Assumptions 1 and 2 are too strong – it is difficult to know and to guarantee that the elements of \\epsilon are i.i.d. Gaussian distributed, and it is not guaranteed that the true Hessian is of low rank. I feel these assumptions are key to the proof, but they make the impact of the work lowers on practical situations where we have no knowledge and guarantee on the true Hessian and its relationship with the empirical Hessian. Although the authors made some discussions on generalizing the assumptions, it is unclear from the limited discussions whether the same theoretical results could be obtained in the generalized setting.\n\n2)\tThe experimental verification of the theoretical results is not indirect and somehow not very convincing. Because the true Hessian is hard to obtain, the authors compared the empirical Hessian with respect to different sizes of the training data as a proxy. However, this is not convincing since we do not know whether the same results can still be observed if further increasing the data scale, and which is the trend with respect to the increasing scale. A better way would be to design a simulation experiments, in which we know the data distribution (and thus can compute the true Hessian). \n\n3)\t The experiments were only done on the CIFAR datasets – not at large scale and not for diverse applications. So it is hard to generalize the experimental results to the entire space of “deep learning” (as indicated by the title). \n\n4)\tThe practical value of the work in not very clear to me. The authors only made limited discussions on this – it can be used to explain why first-order optimization algorithms work well in practice. However, this is what we already know. It would be much better if some practical guidelines can be derived which can improve (but not just explain) the optimization process of deep learning.\n\n**I read the author response, however, I do not think I am convinced to change my rating.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper analyzes the spectrum of the Hessian matrix of large neural networks, both from the theoretical and the empirical perspective. The main contributions are (1) a theoretical analysis of the max/min eigenvalues, showing that the max/min are larger/smaller for empirical Hessians compared to \"true\" Hessians, and (2) an empirical analysis and visualization of spectra using a Lanczos quadrature approach for a variety of training methods and architectures.\n\nGenerally, analyzing the local curvature of (approximate) optima is an important problem for building a better understanding of neural network loss surfaces and for shedding light on how that geometry might be related to generalization performance. Therefore, I find the topic of this paper salient, and the questions analyzed are sure to be of interest to a broad range of theoretically-minded and practically-minded deep learning researchers.\n\nHowever, I did not find the theoretical analysis to be especially novel, deep, or informative. I also did not find the conclusions reached to be particularly compelling, especially insofar as they related to how the geometry might affect generalization performance. For these reasons, I think this paper is a bit below the bar for acceptance, and I'd encourage the authors to add more content and depth to their analysis in a future submission.\n\nLet me elaborate on my above comments in slightly more detail.\n\n\nOn the Theoretical analysis:\n\nThe general observation being made here is that adding more data will tighten the spectrum. This behavior is well-known, e.g. simply adjusting the aspect ratio of the Marcenko-Pastur distribution gives the same effect. Similar behavior is known in more complicated models. The result in Theorem 1 is not especially novel and is probably available in the literature in some form (I didn't find an immediate reference, but even without one, and even if the precise result isn't presented formally in prior work, this type of result is quite well-known). \n\nSome of the assumptions are very strong, especially the independence of the entries in \\epsilon. While I actually suspect that the overall conclusion of spectral broadening may be fairly robust, the argument in section 4.3 did not convince me. There could be very strong correlations in \\epsilon that get induced by the architecture and learned weights, and these correlations could become larger as the dataset size grows, leading to big discrepancies with the predictions. This would probably be most pronounced when transitioning from the under-parameterized to over-parameterized regimes.\n\n\nOn the Empirical analysis:\n\nAs the authors note, the main Lanczos quadrature method being utilized was introduced in several prior works, and the \"improvement\" offered here is basically a small tweak (reducing the number of random vectors). The argument in section 5.2 for reducing this number to one was not particularly compelling, and if the author's regarded this modification as an important contribution, I would have appreciated a more thorough variance analysis, both from the theoretical and empirical perspectives.\n\nAdditionally, related to error analysis, the method utilized in this work (and in the prior works that this work build upon) does not really have a robust way of reliably measuring the error in the spectral density estimation. There is no guarantee that the estimate will be accurate. Such an error estimate should depend on the ground-truth spectrum, and if the ground-truth spectrum is particularly pathological (say, having a large density near zero and a sizable number of large outliers  [which may in fact be the case that may happen in practice!]), the estimate may be quite bad. So I'm actually not completely convinced that the results presented here are accurate estimates of the actual spectrum.\n\nFinally, I found the method for generating the \"True Hessian\" to be quite ad-hoc and likely severely biased. To what extent are the results skewed by the particular data augmentation procedure being utilized? Any effort to control for this kind of bias would be an important addition to this paper. Additionally, it might have been useful to corroborate some of the main claims with regard to the True Hessian (vs Empirical Hessian) by utilizing a synthetic data distribution from which unbiased samples could be drawn. Lastly, even though the number of augmented samples was large, it was not very large compared to the number of parameters. Perhaps studying a regime in which the ratio was farther from one would have been useful.\n\n\nOn the conclusions:\n\nOne of the main important motivations for this analysis is to understand how curvature might relate to generalization. This motivation was put forward in the introduction and was the basis for the experiments comparing the Normal and Overfit model classes. However, I found this single type of comparison to provide very weak evidence in favor of the conclusion that flat minima generalize better. A more compelling argument in this direction would have included substantially more empirical evidence, or (perhaps in better keeping with the main perspectives of this paper) additional theoretical analysis of this particular point.\n\n_____________\n\nOverall, I think the problem studied in this paper is quite interesting, but I did not find the theoretical or empirical contributions to be sufficiently novel, deep, or informative to merit publication in their present form."
        }
    ]
}