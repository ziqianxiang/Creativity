{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper considers the problem of achieving robustness against adversarial attacks in a white box (as well as \"black box\") setting. The key idea is to use quantization layers within the neural network and represent the intermediate network features in terms of their codebook centroids. The paper also explores multi-head quantization where the features are first projected via transformation matrices and then quantized. Experiments are provided on MNIST and Fashion-MNIST datasets and show promising results. \n\nPros:\n1. The idea of quantization within neural networks, while not a new idea, however looks interesting from the perspective of achieving adversarial robustness.\n2. Experiments demonstrate some interesting trend.\n\nCons:\n1. The paper lacks any key insights into why quantization might be a better idea than standard schemes that seek robust-balls within which the perturbations are ineffective (see for example Parseval's networks, or convex outer adversarial polytopes, Cisse et al, or Wong and Kolter resp.). Intuitively it may be true that using quantization offers better robust feature regions, however, won't that be sometimes an overkill and may lead to poor performance? \n\n2. The paper is perhaps very unclear in certain parts. Some notable sections that I found hard to follow are listed below.\na. The Figure 1 is too cluttered and incomprehensible to me. The inputs are shows as very thin arrows and I believe they matter more than the (too many) backward arrows. This figure does not go well with the explanations in the subsequent sections.\nb. The paragraph in Section 3.1 \"Overview\" looks out of context and too cluttered. There is no formal introduction of the proposed method, which I would have expected to be in this paragraph. \nc. The section 3.4 on \"Optimization loss\" is too confusing as well. It is unclear to me what is going on. How are the centroids learned, how do you handle the non-differentiability of argmin, etc. What is the stop-gradient attempting to do? I assume this is to handle the gradient of the assignment step, perhaps use the standard straight-through operator in that regard?\n\n3. There are several technical errors in the paper. Some listed below.\na. What precisely is N_E in (3) ? The notation previously introduced in (1) is N_B. Is it the same as N_E? Or am I missing something here? \nb. The operator I in (3) should be a matrix with dimensions nwh x n_c, such that I(j,i*) = 1 as per the condition in (3). Otherwise I(N_F(x))Q won't match in dimensions. \n\nc. Won't you lose a lot of key information by hard quantizing the x against Q? While, this may allow robustness to perturbations, it looks to me to be detrimental to performance, esp. on large and complex datasets. Also, using the L2 norm to compare x to Q in (3) is also perhaps problematic, is the assumption of x and Q are in a Euclidean space, justified? If so, how?\n\nd. I could not quite follow the idea of multi-head quantization as suggested. Not sure if the authors are aware, but product-quantization is a very standard approach in quantizing input features for nearest neighbor retrieval problems. Typically, the inputs are first projected to subspaces of lower dimensions, before being quantized. This is also the case with recent multi-head transformer models. However, as proposed, the projection matrix W is of the same dimension as the input (dxd), so there is no low-dimensional projection in this step. It appears that all this operation is doing is some linear transformation of the input space, finding the max correlated rows in W with x, and then selecting those dimensions from z_e, on which the quantization is applied in (5). It is unclear to me if this is the right way to do the multi-head quantization or what would this approach lead to. Also, as is written, the multi-head quantized feature will be K times larger than the input feature. I would also add that the hadamard product in (4) will set a lot of data dimensions to zero. In the worst case, say softmax is a delta function, you would have only a single dimension in z_e_i as non-zero. Then, attempting to quantize such a vector with Q in (5) does not make a lot of sense to me.\n\n4. The experimental results, while showing some benefits on two datasets, are disappointing in showcasing the breadth and depth of the proposed scheme. To be specific, only two small scale datasets (MNIST and Fashion-MNIST) are used in the study. Why not CIFAR, SVHN, etc. or large scale models such as ImageNet? Also, only two adversarial attack methods are compared, (FGSM and BIM), why not more recent approaches?  There are no comparisons to other sanitization or defense techniques. \n\nOverall, this paper has some novelty in its core idea in the context of adversarial defenses, however, the exposition lacks conviction, and the experiments are lackluster.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Writing\n1. Not easy to follow. Not well written.\n2. The definition of quantization is not clearly clarified.\n3. The calculation of Q in eq.(2) is not clarified.\n4. Format errors: \nFor example, “The resulting defense is regarded as among the most effective defense\nmethods ().” (an extra brackets) in Section 2.2.\n5. Citation error:\nIn Section 2.2, the Pixel-Defend is wrongly referred to Defense-GAN.\n “Samangouei et al. (2018b) propose PixelDefend...”  while the reference of PixelDefend is \n“Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against adversarial attacks using generative models. In ICLR (Poster). OpenReview.net, 2018b.”\n\nExperiments\n3. Only FGSM adversarial training is used as a comparing method in the experiment. Comparisons with other relevant or prevalent defensive methods should be reported. (input quantization methods [1, 2], Defense-GAN[7], Pixel-Defend[3], and so on)\n4. FGSM training [4] is shown to be not general to new attacks, while PGD adversarial training [5] is stronger than FGSM training and is more robust against different attacks, which is a better choice for comparison.\n5. Results on Larger dataset like cifar10?\n6. The experimental results require more explanations.\nFor example, in Table 1, Q-large obtains much better performance than Q-base under large perturbations on MNIST, however on F-MNIST, it seems not, why? F-MNIST includes more details I think, and Q-larger is assumed to “can find more rich concepts that reflect the similarity and difference between samples”, thus Q-large should be regarded to perform better than Q-base.\nMeanwhile, when combined with adversarial training, Q-large performs worse than Q-base, why? \nIn table 2, Q-large drops to 16.95%, which is not evident to demonstrate the effectiveness of the method. Meanwhile, the results in white-box attacks are not consistent with the results in black-box attacks.\nIn summary, I am confused by the inconsistent results on the different datasets and under different attack scenarios (white and black). Meanwhile, some results can not support the argues in section 3. Thus, I hope that the authors can give more explanations.\n7. Since the quantization step introduces non-differentiability in the Q-path, thus the gradient-based attack methods cannot perform well on Q-base or Q-large. It can be regarded as a case of obfuscated gradients [6]. BPDA[6] was proposed to overcome obfuscated gradients, so it is a better method to evaluate the robustness of the quantization layer.\n\n[1]\tJacob Buckman, Aurko Roy, Colin Raffel, and Ian J. Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. ICLR, 2018\n[2]\tWeilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. In NDSS. The Internet Society, 2018\n[3]\tSong, Y., Kim, T., Nowozin, S., Ermon, S., & Kushman, N. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples. ICLR,2018\n[4]\tIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. ICLR , 2015\n[5]\tMadry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. Towards deep learning models resistant to adversarial attacks. ICLR, 2018\n[6]\tAthalye, A., Carlini, N., & Wagner, D. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. ICML, 2018\n[7] Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers againstadversarial attacks using generative models. ICLR , 2018.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review": "The paper proposes to add a 'quantization layer' to defend against adversarial examples. Unlike previous work where the input is quantized, the paper proposes to quantize somewhere between intermediate layers. The proposed layer projects the layer's input on to different directions and quantizes each projection according to a code. Numerical experiments on MNIST and Fashion-MNIST are provided.\n\n- The approach is very similar to previous work such as VQ-VAE, and the novelty seems to mostly lie in the application of the technique to defend against adversarial examples.\n\n- The paper is not very clear about training. The layer requires projection matrices (P) and the codes (Q), but does not really explain how these are learned (even if that means referring to precise sections of other work such as the VQ-VAE paper). Some insights as to how easy this type of model is train as well as details would help clarify this.\n\n- The proposed model ends up have two separate outputs: one through the quantized path, and one through the non-quantized path used for training. The paper could do a better job at explaining how this can be used. Before 4.1.1, the paper mentions that experiments focus on the quantized path for accuracy. Does that mean the paper is suggesting to only keep the quantized path for inference in general, or are there any upside in leveraging the non-quantized path? The presentation as is is a bit confusing.\n\n- The experiments are not all very compelling:\n\n\t- Regarding the distributions of pixels/concepts, the paper claims 'the distribution of pixels exhibits much more significant distortion than the distribution of learned concepts'. Although this is an interesting part of the analysis, Figure 2 (i) uses a large $\\varepsilon$ value which makes it harder to understand the impact in the more usual regimes of lower values, (ii) fails to provide adequate evidence that the difference is 'much more significant' (which is vague anyway).\n\t\n\t- \"Q-base\" is often better than \"Q-large\", both with and without adversarial training. This is a bit counter-intuitive and goes against the narrative put forward in the paper around multiple quantization heads. However, the discussions only mention \"Q-large and Q-base\" as a whole and do not provide any insight into this.\n\t\n\t- Generally, it is hard to make sense of the results - the proposed methods often make adversarial training worse, although not always. The benefits seem to be stronger when not using adversarial training.\n\t\nMisc. comments:\n\n\"the non-quantized path is introduced ... to regularize the quantization operation\": what does that mean in this context?\n\"typically imperceptible *to* human visual inspection\"?\n\"and the other is the Non-quantized output\"\n\"online available\" -> \"available online\"?\nFGSM does not 'scale the \\ell_infy norm of the gradient'?\n\"regarded as among the most effective defense methods ()\": missing reference\n"
        }
    ]
}