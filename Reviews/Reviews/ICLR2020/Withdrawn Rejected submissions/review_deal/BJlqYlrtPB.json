{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes to improve VAEs' modeling of out-of-distribution examples, by pushing the latent representations of negative examples away from the prior.  The general idea seems interesting, at least to some of the reviewers and to me.  However, the paper seems premature, even after revision, as it leaves unclear some of the justification and analysis of the approach, especially in the fully unsupervised case.  I think that with some more work it could be a very compelling contribution to a future conference.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "Summary:\nThe authors propose augmenting VAEs with an additional latent variable to allow them to detect out-of-distribution (OOD) data. They propose several measures based on this model to distinguish between inliers and outliers, and evaluate the model empirically, finding it successful.\n\nUnfortunately, the method in this paper is developed unclearly and incorrectly. Although their experiments are somewhat successful, the problems with the text and method are severe enough to justify rejection.\n\nSpecifically, the authors' method proposes adding a term to the loss of the VAE that encourages the variational posterior (q) to distribute latent codes (z) for inliers and outliers differently. The equation which defines their new objective is unclear -- specifically, it is not clear whether the added KL term is computed for inliers and outliers both, or whether it is only computed for outliers. If it is the former, then the method does not make sense. If it is the latter, then the equation is incorrect or at the very least not clear in the extreme.\n\nFurthermore, the term is added without consideration of whether or not the method is still optimizing a sensible variational lower bound. The authors attempt to justify the objective by writing out a variational lower bound for a VAE with a mixture prior where inliers and outliers are generated from different mixture components. However, their equations are incorrect -- the equation that is called the log likelihood is not the log likelihood, and the ELBO is similarly wrong.\n\nTheir empirical evaluation is reasonable, although the measures they propose to distinguish between inliers and outliers (i.e. the kl from the approximate posterior to the prior) is not thoroughly justified.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "The paper proposes to counteract OOD problem in VAE by adding a regularization term to the ELBO. The regularizer is defined as the Kullback-Leibler divergence between a variational posterior for a negative sample and a marginal distribution over latents for negative data. The authors present experiments on MNIST and MNIST-like datasets, and CIFAR10 with SVHN. Unfortunately, I do not find the paper especially interesting. The motivation for adding the regularization term is not convincing. The experiments are insufficiently discussed.\n\nRemarks:\n- The paper proposes to ad a regularization to ELBO, namely, the Kullback-Leibler divergence between a variational distribution for a negative sample and a marginal distribution over latent variables for negative samples. I do not fully understand the motivation given on page 3. The authors show that including the negative data yields a new objective that is a sum of two log-lihelihood functions for \"real\" and negative data. However, later they propose to skip a (negative) reconstruction error term for the negative data. As a result, the authors obtain the objective they proposed. This explanation is very vague and I do not see what it adds to the story. Contrary, it causes new questions about their model and whether it is properly formulated.\nI suggest to look into the following paper to see whether the model could be re-formulated:\nHu, Z., Yang, Z., Salakhutdinov, R., & Xing, E. P. (2017). On unifying deep generative models. arXiv preprint arXiv:1706.00550.\n\n- I do not understand why the authors used Bernoulli distribution to model color and gray-scale images. The Bernoulli distribution could be used only for binary random variables. This is obviously flawed.\n\n- In general, the results seem to partially confirm claims of the paper, however, they are quite vague. First, utilizing a wrong distribution is demotivating (see my previous remark). Second, I miss a better description of models and, in general, experiments' setup. Third, all results are explained in a laconic manner (e.g., \"The other results in the table (...) confirm the assymetric behaviour of the phenomenon (...)\"). There is neither deeper understanding nor discussion provided.\n\n- Why there are no samples for CIFAR or SVHN provided?\n\n======== AFTER REBUTTAL ========\nI would like to thank the authors for their rebuttal. I really appreciate that the paper is updated and some concerns are solved. After reading the updated paper again, I tend to agree that the proposed idea is interesting for the problem of OOD detection using generative models. Therefore, I decide to update my score.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper discusses the detection of out-of-distribution (OOD) samples for variational autoencoders (VAE).\nThe idea is to train the encoder such that its output variational distribution q(z|\\bar{x}) is pushed away from the prior of latent z. \nI think the paper needs more clarification and investigation for being published in the conference. \nMy major concern is that more empirical investigation is necessary since the formulation provides a minor novelty. \nSpecific points are given below. \n\n1) Weak novelty in terms of model design. \nThe objective function consists of the standard (negative) ELBO term and additional KL term to modify the variational posterior of negative samples. \nThis modification can be regarded as a form of outlier exposure (Hendrycks et al. 2018) specialized for VAE. \nThe choice of \\bar{p} is not much investigated. \nAny discussion if we use a more sophisticated model such as VampPrior* for stronger modeling capacity. \n* J. Tomczak and M. Welling, VAE with a VampPrior, AISTATS 2018.\n\n2) The use generated samples as negative samples is interesting but mysterious. \nThe authors conjecture that this works because the generated samples come from near the data manifold, but in-distribution samples and negative samples can be indistinguishable when the generative model is very well trained. \nWhat happens if, for example, the negative samples are generated by data augmentation techniques (such as cropping, rotation, mirroring, though mirroring and much rotation may be unsuitable for text images)? \nThis can also produce near-manifold points. \nA deeper analysis why generated samples can improve the OOD detection performance is necessary. \nFurthermore, why does not this approach impact much for color images in Table 4?\n\n3) More details of experimental procedures. \n3-1) How was data points are generated from VAE as negative samples? \nPossible ways are:\n* sample z ~ p(z), then draw from the decoder x ~ p(x|z).\n* use negative prior z ~ \\bar{p}(z), then draw from the decoder x ~ p(x|z).\n* this seems weird: use variational posterior z ~ q(z|x), then x ~ p(x|z).\n\n3-2) Latent dimension of 10 for grayscale images seems small. \nDoes the size affect the OOD detection performance when the size is 50 or 100 to make the model richer. \n\n3-3) How was the variance obtained when the decoder uses the Gaussian likelihood?\n* fixed value?\n* learned for each pixel?\n* output from the decoder?\n\n4) If we have access to diverse negative datasets, can the ODD detection perform better? \nMixing multiple datasets or using both available dataset and generated samples can improve the performance while the test OOD samples are kept unseen. \nFor example, train VAE on MNIST while using KMNIST and EMNIST as the negative sets to detect Fashion-MNIST as ODD. "
        }
    ]
}