{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper proposed a framework to solve semi-supervised learning problems using self-supervised learning, By implementing it through  rotation angle prediction, called CRAP. It predicts the image rotationangle conditioned on the candidate image class. Experimental evaluations show that CRAP achieves superior performance over the other existing\nways of combining SlfSL and SemSL. \n\nOverall I feel that the paper is not pleasant to read and leaves the reader a lot of unclearities. For example, if the task consdiered using the benchmark image data sets classification, then why and where do you need to estimate the rotation angle; is rotation needed in augmenting the training data set; what is the relation between the image classification task and the estimation of image rotation angles?\n\nIn equation 1, is you have already been able to estimate p(y|x,theta), then why bother estimating the other term since you final goal is exactly the semi-supervised learning task of estimating the probability of the label y for each sample x? The authors claimed that the p(z|x,theta) term will also back propagate to p(y|x,theta). does it depend on the pre-task chosen?\n\nIn figure~1, why do you have multiple sematic classifiers? \n\nEstimating the joint conditional distribution p(z|x,y,theta) can be considered as  creating an ensemble of diversified pretext target predictors, as authors discussed. However, the prediction requires summation over y, namely differen sample classes --- how can we consider each term as a base classifier if it is only related to one class?\n\nOverall the paper is very difficult to understand and authors can definitely improve their presentation, and in particular spend more effort explaining things in a coherent and clear manner. \n "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a modification of the S4L method [1], combining self-supervised learning and semi-supervised learning. S4L introduces trains the network jointly using the cross-entropy loss on training data and an auxiliary task on all (labeled and unlabeled) data. The auxiliary task is to predict the rotation angle of a randomly rotated image. CRAP modifies S4L by conditioning the predictor of the rotation angle on the output of the classifier. \n\nFor me the main strength of this paper is the empirical evaluation The authors test CRAP on a wide range of image classification datasets and obtain strong results. The method is also quite different from other successful semi-supervised learning methods.\n\nThe main weakness for me is that the modifications from S4L seem to be fairly minor, and they are not very well motivated. For example, when I first read the method section, I thought that the idea of making the rotation angle prediction class conditional was to make sure that the classifier receives gradient both on the labeled and unlabeled data. However, in the best model, CRAP+, there is a separate classifier that is used to make predictions, and that doesn’t get gradient from the unlabeled data.\n\nIf I understand correctly, the main differences between CRAP (or CRAP+) and S4L then is a slight modification of the auxiliary task (described in Section 4.2) and a modification of the architecture of the rotation predictor. In other words, CRAP is the same as S4L, but with a different auxiliary task and architecture. Given the large empirical improvement over S4L, I don’t think the lack of differences is a huge issue. However, the current presentation doesn’t provide a good motivation for the introduced changes, and the presentation can be simplified if CRAP is indeed a special case of S4L. I encourage the authors to comment on the differences between CRAP / CRAP+ and S4L and the motivation for introducing these changes in the rebuttal.\n\nBelow I describe some other, more minor issues and questions I have.\n\n1. The experiments are generally solid, and the results are good. However, CRAP introduces changes in architecture compared to S4L, and the other methods it’s being compared against. Did the authors try to match the number of parameters for CRAP and S4L, or just tried to optimize the performance for both methods?\n\n2. As a suggestion, I would report results for CRAP+ on all datasets, and had a comparison between CRAP and S4L as a part of ablation study. I think this presentation would be more clear, as readers mostly care about the best performing version of the method. I don’t insist on this change.\n\n3. For MixMatch [2], what results are being reported? In [2] the results on CIFAR-10 with 4k labels seem to be better than what’s reported in the paper. \n\n4. The authors seem to provide a wrong citation for the fast-SWA method in the footnote on page 7, which should be [3]. The authors could also include the results of [3] for 4k labeled data in Figure 2 or Table 9.\n\n5. In section 5.2 fairly surprisingly S4L performs poorly on CIFAR and SVHN. It often performs worse than just training on the labeled data. CRAP outperforms S4L and other baselines by a large margin. At the same time, on ImageNet the difference is much smaller. What is the reason for that? Could it be that the hyper-parameters for S4L were not tuned well enough on other datasets?\n\n6. Overall, the method feels somewhat ad hoc. Can the idea be extended to other types of auxiliary tasks? What other tasks could we use? Can this method be extended to other types of data? \n\n7. GAN-based method were very popular in deep SSL before consistency-based methods. There the discriminator is a classifier that is trained for classifying labeled data bit also for an auxiliary task of discriminating between real and generated data on all (labeled + unlabeled data). [5] was one of the first papers to implement this and [4] achieved strong results. I believe CRAP and S4L are related to GAN-based methods in the sense that both  types of methods train representations for the classifier using an auxiliary task. So, it would be good to add a brief discussion of GAN-based methods to the related work.\n\n*Conclusion*\n\nFor me this paper is borderline. While the results are good, the changes compared to S4L seem minor, and, more importantly, not motivated very well. I encourage the authors to comment on the differences between CRAP and S4L in the rebuttal, and the motivation for these changes, and I am willing to update my score based on the response.\n\n[1] S4L: Self-Supervised Semi-Supervised Learning\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, Lucas Beyer\n\n\n[2] MixMatch: A Holistic Approach to Semi-Supervised Learning\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel\n\n\n[3] There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average\nBen Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson\n\n[4] Good Semi-supervised Learning that Requires a Bad GAN\nZihang Dai, Zhilin Yang, Fan Yang, William W. Cohen, Ruslan Salakhutdinov\n\n[5] Improved Techniques for Training GANs\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary & Pros\n- This paper proposes a simple yet effective framework utilizing self-supervision in semi-supervised settings. When predicting self-supervision, this framework uses its conditional probability p(z|y,x) given a primary label y. This scheme can formulate the relationship between labels and self-supervision.\n- This paper also proposes two additional techniques for improving performance with (1) the average prediction across four rotated samples and (2) mixed samples.\n- The proposed framework achieves better performance than the prior work, S4L, and it is also comparable with SOTA method, Mix-Match.\n\nConcerns #1: The claims for the proposed method are not clear\n- On page 3, the authors claimed that the benefits of the proposed method come from three perspectives (1)-(3). I think the claims are not clear because p(z|x) does not depend on y, so it is hard to say that maximizing p(z|x) can encourage maximizing p(true y|x). For example, p(z|x) can be maximized even if p(true y|x) is small.\n- To verify the claims, experimental supports should be provided.\n\nConcerns #2: Why the semantic classifier p(y|x) in the auxiliary branches is required?\n- The authors said that the reason is self-supervision z is noisy compared to the ground-truth y. But this reason is not clear because the self-supervision is always assigned correctly.\n- On page 3, the authors said that the proposed method provides the gradient of p(y|x) while S4L does not. But if use the additional semantic classifier for p(y|x) in (1), the claim becomes not true.\n- Moreover, the semantic classifiers in main and auxiliary branches aim to model the same distribution p(y|x). Why separate branches are required for the exact same purpose?\n\nConcerns #3: Writing is not well-organized.\n- Overall, writing could be simplified and well-organized.\n- When constructing self-supervision, the input images are also modified, e.g., rotated. The notation x in p(z|x) and p(y|x) is confused, so using another notation such as \\hat{x} for the case could be better.\n- What are the exact training objectives? I think they should be stated explicitly in the main paper, not appendix, because it is important that which sample x is used for p(z|x) or p(y|x), especially in this semi-supervised setting.\n\nConcerns #4: Why Fine-tune is worse than Labeled-only?\n- The result in Table 2 is weird because there is no reason that pre-trained parameters are worse than random parameters. So evidence supporting the result should be provided.\n\nThe proposed method is simple and effective and provides meaningful gain, but verification and explanation seem to be not enough and writing could be improved. So I will increase the score if a strong rebuttal is given.\n"
        }
    ]
}