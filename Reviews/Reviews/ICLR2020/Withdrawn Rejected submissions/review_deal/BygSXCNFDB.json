{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper applies the Go-Explore algorithm to text-based games and shows that it is able to solve text-based game with better sample efficiency and generalization than some alternatives.  The Go-Explore algorithm is used to extract high reward trajectories that can be used to train a policy using a seq2seq model that maps observations to actions.\n\nPaper received 1 weak accept and 2 weak rejects.  Initially the paper received three weak rejects, with the author response and revision convincing one reviewer to increase their score to a weak accept.\n\nOverall, the authors liked the paper and thought that it was well-written with good experiments.\nHowever, there is concern that the paper lacks technical novelty and would not be of interest to the broader ICLR community (beyond those that are interested in text-based games).  Another concern reviewers expressed was that the proposed method was only compared against baselines with simple exploration strategies and that baselines with more advanced exploration strategies should be included.\n\nThe AC agrees with above concerns and encourage the authors to improve their paper based on the reviewer feedback, and to consider resubmitting to a venue that is more focused on text-based games (perhaps an NLP conference).",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes an exploration approach of Go-Explore together with imitation learning for playing text games. It is shown to outperform existing solutions in solving text-based games with better sample efficiency and stronger generalization ability to unseen games.\n\nPros:\nSeq2seq imitation learning + Go-Explore is applied to more challenging text games and achieves better performance, higher sample complexity and better generalization ability.\n\nCons: \n•\tFrom modeling perspective, the policy network uses the standard sequence-to-sequence network with attention. And it is trained on the high-reward trajectories obtained with Go-Explore method using imitation learning. From this perspective, there is not much novelty in this paper.\n\nDetailed comments:\n•\tMore details about the mapping function f(x) in Phase 1 should be given.\n•\tIt is not clear why Phase 2 should be called “Robustification”. It seems to be just standard imitation learning of seq2seq model on the high-reward trajectories collected in Phase 1.\n•\tIn the paragraph after eqn. (1), H is defined to be the hidden states of the decoder. Shouldn’t it be the hidden states of the encoder?\n•\tIt seems to be unfair to compare the proposed method with advanced exploration strategy to other model-free baselines that only have very simple exploration strategies (e.g., epsilon-greedy). It is not surprising at all that Go-Explore should outperform them on sparse reward problems. More baselines with better exploration strategies should be compared."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "\nThis paper considers the task of training an agent to play text-based computer games. One of the key challenges is the high-dimensional action space in these games, which poses a problem for many current methods. The authors propose to learn an LSTM-based decoder to output the action $a_t$ by greedily prediction one word at a time. They achieve this by training a sequence to sequence model on trajectories collected by running the game using a previously proposed exploration method (Go-Explore). While the results are promising, there might be limited novelty beyond training a sequence to sequence model on pre-collected trajectories. Further, the experiments are missing key elements in terms of proper comparison to baselines. \n\nPros:\n1. Nice idea for tackling the unbounded action space problem in text-based games. \n\nCons:\n1. The method depends on the assumption that we can get a set of trajectories with high rewards. This seems a pretty strong assumption. In fact, the authors use a smaller set of admissible actions in order to collect these trajectories in the first place - this seems to not be in line with the goal of solving the large action space problem. If we assume access to this admissible action function, why not just use it directly?\n2. Some of the empirical results may not be fair comparisons (unless I'm missing something). For example, all the baselines for the CookingWorld games use $\\epsilon$-greedy exploration. Since the Go-Explore method assumes access to extra trajectories at the start, this doesn't seem fair to the other baselines which may not observe the same high-reward trajectories.\n\nOther comments:\n1. Do you use the game rewards to train/finetune the seq2seq model or is it only trained in a supervised fashion on the trajectories? (like an imitation learning setup)\n2. How critical is the first step of producing high reward trajectories to the overall performance? Some more analysis or discusssion on this would be helpful to disentangle the contribution of GoExplore from the seq2seq action decoder."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "This paper applies the Go-Explore algorithm to the domain of text-based games and shows significant performance gains on Textworld's Coin Collector and Cooking sets of games. Additionally, the authors evaluate 3 different paradigms for training agents on (1) single games, (2) jointly on multiple games, and (3) training on a train set of games and testing on a held-out set of games. Results show that Go-Explore's policies outperform prior methods including DRRN and LSTM-DQN. In addition to better asymptotic performance Go-Explore is also more efficient in terms of the number of environment interactions needed to reach a good policy.\n\nBroadly I like how this paper shows the potency of Go-Explore applied to deterministic environments such as the CoinCollector/CookingWorld  games. It is an algorithm that should not be ignored by the text-based game playing community. I also like the fact that this paper clearly explains and demonstrates how efficient and effective Go-Explore can be, particularly when generalizing to unseen games.\n\nThe major drawback of the paper is a lack of novelty - the Go-Explore algorithm is already well known, and this paper seems to be a direct application of Go-Explore to text-based games. While the results are both impressive and relevant for the text-game-playing community - it's my feeling that this work may not be of general interest to the broader ICLR community due to the lack of new insights in deep learning / representation discovery. However, I am open to being convinced otherwise.\n\nMinor Comments:\n\nThe textworld cooking competition produced at least one highly performing agent (designed by Pedro Lima). While I'm not sure if the code or agent scores are available, it would be a relevant comparison to see how well Go-Explore compared to this agent. (See https://www.microsoft.com/en-us/research/blog/first-textworld-problems-the-competition-using-text-based-games-to-advance-capabilities-of-ai-agents/)",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}