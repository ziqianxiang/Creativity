{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposes a method to make the filters of the last conv layer more class-specific. The motivation for this is to improve upon the interpretability of the CNN, which is empirically shown by comparing the class activation maps (CAMs) of regular CNN and the proposed LSG-CNN. While the idea is interesting, one of the concerns from reviewers is about limited applicability of the method, at least the way it is shown in experiments -- a concern that I tend to agree with. As primary goal of the work is improving interpretability of CNNs, authors should test LSG-CNN with some more recent methods for producing the saliency maps other than CAM to convincingly establish the value of the method. Authors also mention lack of hyperparameter tuning and the use of SGD with limited training epochs as a reason for the drop in accuracy. It will be worth spending some effort so the accuracy matches the standard benchmarks -- this will help in arguing more convincingly about practical benefit of the method. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a method to improve the interpretability of a convolutional neural network (CNN). The main idea is to force the CNN filters to be class specific, i.e. to be associated to a specific class. This is accomplished by a gating function that enforces filters to be sparsely activated.  This would make the model more interpretable by allowing to check which filters/classes have been activated. Results are evaluated in terms of performance, sparsity of the filters and localization accuracy on CIFAR10.\n\nI lean to reject this paper because I consider the proposed motivations not clear and partially misleading. In the introduction it seems that the idea of enforcing class-specific filters makes sense in general because it avoids filters ambiguity and it reduces redundancy (not clear how). Instead, in the actual implementation, this idea is applied only at the last layer of a CNN. This makes sense, because in a CNN filters need to be shared among classes. Itâ€™s an important form of regularization, especially when the amount of training data is reduced. \nAdditionally, the advantage of enforcing the last layer filters to be class-specific is not clear. To me, instead of evaluating the activation of the classification layer, it is possible to check the activation of the filters of the last convolutional layers. However not much more interpretability is added. Furthermore, the improvement on localization as shown in Fig. 6 is only qualitative as the images could have been cherry-picked and there is no real localization measure.\n\nAdditional comments:\nIn table 1, performance of the proposed training is comparable to a standard training on CIFAR10. However, evaluating the proposed approach on a single dataset and only one network is not enough for a clear evidence. Additionally, the obtained performance is below the standard performance of ResNet on CIFAR10 without a clear reason.\nAnother way to enforce a similar pattern on the gating function would be by maximizing the mutual information between selected filters and classes.  \n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed an interesting idea of using Label Sensitive Gate (LSG) structure to enforce models to learn disentangled filters for better interpretability of the DNN model. By periodically training with the sparse LSG structure, the model is forced to extract features from only a few classes. The model is trained efficiently in an alternate fashion (with respect to both the network parameters and the sparse gate matrix.) By disentangling the class-specific filters, the model becomes less redundant and more interpretable.\n\nOverall the paper is well-written and well-organized. I  like the idea of imposing a class-specific gated structure for disentangling the representation. And numerical experiments verify the effectiveness of the proposed method in terms of\n1) Improved performance\n2) Disentangled representation (a small L1 norm on the gate matrix G)\n3) Consistent class activation map for different inputs.\n\nI do have some question though\n1) Table 1 also reports the L1 norm and \\Phi of a STD CNN. What is the gated matrix G here for a STD CNN?\n2) Is it very important to have a constraint $\\|G^k\\|_{\\infty}$ in the model? What if an L-1 norm is used directly?"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "Contributions: The paper proposes a novel Label Sensitive Gate (LSG) structure to enable the model to learn disentangled filters in a supervised manner. The novelty of the paper is to introduce the Label-Sensitive Gate path during the training, on top of the standard training path. This encourages the filters to extract class-sensitive features. \n\nThe highlight of the experimental results is that by incorporating the LSG structure, the author is able to achieve better localization with CAM because the filters are trained to extract more relevant features. Additionally, filters from different classes are kind of \"orthogonal\" to each other.\n\nHowever, I have a few questions for the author:\n\na) In Figure 5, the author mentions \"TP/FN\" samples in the caption. However, for the plot at the right, it is titled with \"False Positive\". Is this a typo or do you talk about something else? \n\nb) In Figure 7, the author shows the correlation matrix of filters. Since the filters belonging to one class are highly correlated (block matrices in plot d), why do not train even less filters for each class? If we train less filters for each class, will that hurt the performance? It might be a good direction on how to interpret filters from one class and how to make intra-class filters more interpretable.\n\nOverall, the novelty of the paper is limited by the scope of the experimental results. Why would people care about interpretability of CNN filters is not explained clearly. The paper shows interpretable filters could help with localization. However, this is the only example in the paper I find useful. It would be better if the author could show any other applications. Moreover, I wish the author could spend more paragraph discussing why filters belonging to different classes tend to be orthogonal to each other. This is not clearly written (Eq(3)?). A more theoretical/intuitive explanation would also make the paper stronger as well.\n\nIf the paper could omtroduce more real applications of the proposed LSG model and give more theoretical/intuitive explanations of why the inter-class filters are orthogonal, I would be willing to raise up my scores.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}