{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This manuscript proposes an over-sampling method for dealing with the imbalanced classification and long-trailed problems. The authors refer to their work as Advserarial Minority Over-sampling (AMO). \n\nThe interesting aspect of this paper is that it explores adversarial perturbation (possibly of majority class) as a means of over-sampling for the minority class. The findings suggest that it could improve imbalanced learning. However, there are several major issues with the paper in its current form:\n-\tThere is a recent publication with almost the same topic in ICCV 2019 that also explores using adversarial minority over sampling frameworks (published on arxiv on Apr 3, 2019 https://arxiv.org/pdf/1903.09730.pdf). Although that one is a bit different methodologically, the authors have not mentioned it, compare with it, nor discussed it. It is not clear where the current manuscript stands in comparison with the ICCV 2019 paper. \n-\tGiven the above paper, the novelties of the proposed technique become marginal. \n-\tAnother major issue with the paper is its methodological limitations. As the authors have also mentioned, it looks a bit like learning to classify the adversarial examples. It seems like this is a very effective method if we want to classify a majority (normal) class versus a minority (anomaly) class. Because when the model generates adversarial examples for any specific class the adversarial examples may cover all space of the samples minus the samples of that (normal) class. Therefore, the model does not learn the geometry of the minority class (as opposed to many state-of-the-art long-trailed classification models) and only learns the majority class. This is not itself a positive characteristic. \n-\tThe authors have not provided any theoretical discussions/guarantees why adversarial examples should be a good means of learning the imbalanced distributions. Everything in the paper seems to be experimental and heuristic. \n-\tThe accuracy metric provided by the authors is a bit misleading. For these cases of long-trailed classification models, Average Class Specific Accuracy and Geometric Mean (analogous to F1-score) are the most relevant ones to report, especially because experiments are conducted on multi-class settings. \n-\tThe results in Table 2 show a highly imbalanced classification rate between majority and minority classes. This means that neither the proposed method nor the baselines could solve the problem. The differences between the proposed method and the baselines do not seem to be statistically significant. So, what is the purpose of this experiment!? \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The authors propose to tackle imbalance classification  using re-sampling methods. The idea is to generate adversarial examples of a model trained on the imbalanced dataset, in the least frequent classes.\n\nTo the best of my understanding the rational behind this is that models usually overfit the least frequent class. Hence adversarial examples in the minority class would help to train a new model that generalize better.\n\nQ1. If a model is trained to perform well on the class-imbalanced dataset and is robust to adversarial attacks / does not overfit the minority classes, what would happen?\n\nMy decision is weak reject for the following reasons.\n\nWhile the idea seems appealing, intuitive ideas on why such a methodology should work. Moreover rather than trying to give more understanding on why such a methodology works, the experiments only illustrate that the propose algorithm outperform the state of the art. While such positive results are always welcome, it is also useful to propose experiments to gain a better understanding on the performances/limits of an algorithm.\n\nAlso, the proposed algorithm depends on some hyper-parameters that are never empirically nor theoretically studied.\n\nSuggestions\n-----------------\nwhile it is nice to report standard deviations, the authors could increase the number of runs to make the differences more significant and eventually provide a statistical test. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a new oversampling method for class-imbalanced datasets, which is called Adversarial Minority Over-sampling (AMO). Its contribution is that it performs over-sampling using the samples of the majority classes, not the minority classes. It generates adversarial examples of the majority class and labels them as one of the minority classes. The purpose of using the majority class samples is that it can solve the problem of overfitting minority classes by using samples from minority classes too much, which is a typical problem of ordinary over-sampling techniques. For this purpose, the authors propose a new optimization objective that generates synthetic samples by transforming the majority class samples and devises a rejection criterion to determine whether the generated sample is appropriate, motivated by the concept of effective number of samples. They also proposed the distribution of the appropriate initial seed point of the generation through newly designed criteria.\n\nI do not think this paper is acceptable in the current state, and there exist some ambiguous issues where clarification and more supporting evidences are required. However, as a new novel approach to mitigate the over-fitting issue, if the following questions are answered reasonably, this paper may become acceptable. \n\n1.\tAs the authors mentioned consistently, the classifier g that generates adversarial examples is unreliable and potentially over-fitted to minority classes, which means that the classifier g cannot learn the general features for the minority classes. It is clear that the classifier g would make reasonable adversarial examples for the majority classes, but it is unclear that the generated examples by g are reasonable to be labeled as one of the minority classes, a target class. Although the classifier g would classify the generated example as the target class, it does not mean that it captured the general features of the target class and generated the adversarial sample using them. Instead, I think it is possible that the resulting synthetic example can be labeled as any minor class, not just as the target minor class. In the reported results, the proposed method would learn how to distinguish each major class from the last and fail to generalize in minority classes and make any example with the only constraint that it does not have the feature of starting (major) class, regardless what the target class is. Thus, I ask the authors to a) provide the evidence that the model g generates the samples with general features for each minor class and b) explain clearly how it is possible. c) I also suggest the authors to show the recall in addition to the accuracy in Tables 1 and 2. \n\n2.\tI am wondering why f does not have to classify synthetic examples as their target classes, as stated right below Equation (2) in page 3. It seems reasonable for f to classify the example exactly like g. Even in Figure 4, it seems that the classifier g attempted to generate ‘Truck’ class example, but it failed, as the model f classified it as ‘Bird.’ It would be great if the authors clarify how the classifier work in this manner and what is the expected outcome of f.\n\n3.\tAlso, I am wondering why the accuracy of major class examples decrease from 2% to 7% consistently if AMO is applied to ERM or LDAM in CIFAR-10 and CIFAR-100 dataset. I conjecture it is because the features of the starting example are not fully ‘erased.’ Also, I think that the proposed method affects the performance of the majority class in page 2. In this respect, I would like to see how many samples of major classes are correctly classified previously but misclassified after the model was trained with synthetic adversarial examples, to check the effect of adversarial examples on learning the major class. \n\n4.\tIt is a minor point, but it needs clarification on how the train/test set was constructed in both g and f, e.g., whether they have the exactly same training set.\n"
        }
    ]
}