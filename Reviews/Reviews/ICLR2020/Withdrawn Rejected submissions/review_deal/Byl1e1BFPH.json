{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Summary:\nThe authors propose Dual Sequential Monte Carlo, a method to jointly perform filtering and planning on continuous POMDPs by nesting two smc filters.\n\nThe authors use a variety of pieces in their model. \nFirst, filtering is used to represent the posterior over current state states (or: belief state).\nSecond, the authors use the control-as-inference formalism in order to perform planning using SMC by sampling from the posterior of optimal trajectories where optimality is defined via auxiliary binary latent variables proportional to the reward state action pairs yield (and as such trajectories).\nThird, the authors propose to use \"Adversarial Particle Filtering\" as a method to train their observation models.\nFourth, the crucial difference to the main baseline is the inclusion of the mean belief state into the planner by using it both as an auxiliary input for the policy network (as a conditional action prior) and for state propagation during planning. The rationale for this is the hope that this will represent uncertainty in the filter better during planning.\n\nIn the experiments, the Dual SMC method tends to outperform the baseline, though at times it is unclear if this is owed to the adversarial model or the actual planner.\n\nComments:\nMy main issue with the paper is the role of the mean belief state used for dual smc. This has two major appearances:\n1.  I understand the idea that including moments of the state distribution particles together with each individual particle could inform a learned policy network better as to whether states are uncertain or not. However, it is unclear if the first moment is sufficient to do this or if another method would have been more advisable? In the end, little proof is given that this actually achieves the stated goal.\n2. The second use of the mean belief state, however, is hard to justify in my opinion. In algorithm 2 line 6, the mean belief state is used to propagate the planner forward given the current action. While I understand that this may save computation compared to using sampled state particles to propagate the posterior forward, this choice ultimately is not principled and causes the planner to be planning for a different model than the one one may want it to plan from, as the state is systematically reset to the mean. This seems to be a particularly optimistic planner and it is unclear how it would respond to multi-modal or long tailed state-spaces during planning. Mainly, it simply is not guaranteed to be planning according to the Bellman equations anymore but generates \"some plan\" which is close to the model but also departs in a way that is hard to quantify or to have guarantees on. Why change the model during planning?\nWhy is this choice necessary and what would have been the alternative? What does it buy us?\n\n3. The authors are not doing a good job explaining model training rigorously. A nice way to explain how to train parameters in a state space model while performing SMC on it is via variational inference with adaptive SMC proposal distributions. But even if that were not the chosen way here, the paper is not doing a good job clarifying the objective function for model training.\n\n4. Adversarial Particle Filters are presented as a contribution by the authors. However, these objects have been proposed before and explained, for instance in \"Adversarial Sequential Monte Carlo\" by Kempinska and Shawe-Taylor. This reference is clearly missing here. In addition, the presentation of the section on adversarial particle filters is confusing. The adversarial training in this case has one role only as far as I can tell: building an implicit observation model in order to achieve more accurate representation of observations through latent states. It would be easy to consider this a particular modeling choice (as opposed to a Gaussian or a Bernoulli observation model), as implicit observation models are quite common in the current probabilistic literature. Presenting it as an adversarial filter is not conveying that accurately.\n\n5. Neural Adaptive Sequential Monte Carlo by Shane Gu et al is also a missing reference here.\n\n6. In experiment 2, the model with a density-based observation model (presumably Normal) and Dual SMC wins. Why is the adversarial aspect so important to this paper if it is not even consistently the winning model?\n\n7. I am quite uncomfortable with the missing comparison to a planner that would keep the mean belief state for the policy network but actually plan from the model during the transition T_psi. I would have found that to be the most natural baseline here to also address my comment #2. As it stands, the two main contributions of the paper (adding mean belief state to policy network and planning with mean belief state transitions) are not individually well explained and individually evaluated.\n\nDecision:\nThe paper proposes an interesting and technically challenging attempt to jointly perform filtering and planning. However, some of the choices in the paper are not well motivated or technically corroborated, as outlined in my comments. I hope the authors can clarify these weaknesses of their paper, as particle-based methods are promising avenues to rigorous model-based planning in POMDPs and the authors are tackling an interesting problem. As the paper stands, I lean towards rejection due to the technical questions about the method."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper generalizes the differentiable particle filtering (DPF) to POMDP setting. Moreover, it introduces an extra differentiable SMC component as planner into the model, which together with the DPF will be learned together. The authors demonstrated the effectiveness of the proposed algorithm on 2D floor positioning, 3D navigation with image input, and Reacher in MuJoCo.\n\nAlthough the learning through the planner in an end-to-end fashion has been investigated in RL community extensively, e.g., [1, 2], extending it to POMDP is relative novel. However, there are several issues need to be addressed:\n\n\n1, The optimization objective (1) seems come from nowhere. The motivation of such objective is very unclear. Why this objective is selected? Why not to adapt other alternative objectives, e.g., maximum log-likelihood? Without such justification, the learning target is not convincing. Meanwhile, whether $\\psi$ is learned or not. If it is learned, what is the learning objective. \n\n2, The authors claim that by twisting two SMCs, for planning and state prediction, respectively, the algorithm will lead to better performance. However, after twisting the planner into the learning, the proposed method is only applicable for on-policy setting, while the major advantage of model-based RL is for off-policy. This effect has not been discussed explicitly. I am not sure whether the benefit of the proposed algorithm comes from the coupling of two SMCs or on-policy learning.  \n\n3, The empirical experiments should be improved to be more convincing. One of the major contribution of this paper is using POMDP, rather than traditional MDP. To justify the benefits of the complicated model, a direct comparison between model-free (DQN) model-based (DeepMDP [4] and PlaNet [3]) algorithm upon MDP with same inputs as the inputs of the proposed model is necessary. Moreover, just one MuJoCo is not sufficient to demonstrate the advantages. Please evaluate the proposed algorithm on more environments in standard benchmarks, e.g., MuJoCo and Atari.\n\nMinor:\nThe terminology \"dual\" has rigorous definition in math. Please use the terminology following the common knowledge.\n\n\n1, Okada, Masashi, Luca Rigazio, and Takenobu Aoshima. \"Path integral networks: End-to-end differentiable optimal control.\" arXiv preprint arXiv:1706.09597 (2017).\n\n2, Pereira, Marcus, et al. \"Mpc-inspired neural network policies for sequential decision making.\" arXiv preprint arXiv:1802.05803 (2018).\n\n3, Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., & Davidson, J. (2018). Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551.\n\n4, Gelada, C., Kumar, S., Buckman, J., Nachum, O., & Bellemare, M. G. (2019). DeepMDP: Learning Continuous Latent Space Models for Representation Learning. arXiv preprint arXiv:1906.02736."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The contribution of the paper is a method of filtering and planning as joint probabilistic inference. For that, the duality of inference and control is used, such that an adversarial particle filtering approch can be employed. Several experiments that support the claim are presented.\n\nI recommend to reject the paper from publication. Main reasons are the sub-standard presentation of the idea as well as not justifying design decisions.\n\nI find the paper confusing. While the high-level idea is simple, the authors chose not to start the presentations from a reasonable point (i.e., the graphical model) and subsequently introduce the necessary approximations to make the problem tractable, several things are introduced ad hoc. \n\nE.g. what is the motivation to chose the mean state and the top-M particles? Why was that representation chosen? Were other representations tried? Under what conditions is this representation reasonable, when might it fail? The answers given are not sufficient. \nThe text and the algorithm boxes do not serve complementary purposes. Instead, the text merely reiterates what is in the algorithm box. \n\nFurther points for improvement.\n- Language. There are grammatical mistakes in the paper, e.g. missing pronouns, some sentences \"do not compile\", the language is sloppy (\"[...] had better be really sampling efficient [...]\")\n- Eq 1 has two expectations over s. Where does the first s come from? \n- \"Only requires a learned Q function\" does not seem very \"only\" to me. These are typically hard to obtain! Also, why is this better than using the true model (which can equally be required to be available) for Monte Carlo approximations?\n- When I started reading the paper, I was under the impression that it is about continous time POMDPs. Could be clearer.\n- For p(O_t = 1) \\propto exp(R) the assumption that R <= 0 is required.\n- page 2, paragraph 1: it reads as if the history grew exponentially. I assume the authors were referring to the policy, but that is an unnatural assessment, as policies in continuous spaces are typically continuous as well, making a cardinality argument cumbersome.\n- The literature on varitional particle filtering is not respected. Starting points are [1, 2, 3]\n- Regression does not imply root mean squared error. Counter example: logistic regression.\n\n\nReferences\n[1] Gu, Shixiang Shane, Zoubin Ghahramani, and Richard E. Turner. \"Neural adaptive sequential monte carlo.\" Advances in Neural Information Processing Systems. 2015.\n[2] Naesseth, Christian A., et al. \"Variational sequential monte carlo.\" arXiv preprint arXiv:1705.11140 (2017).\n[3] Maddison, Chris J., et al. \"Filtering variational objectives.\" Advances in Neural Information Processing Systems. 2017.\n\n\n"
        }
    ]
}