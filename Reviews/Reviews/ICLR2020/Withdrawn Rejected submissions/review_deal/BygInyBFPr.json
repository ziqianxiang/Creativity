{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper first formulates quality-diversity trade-off in text generation as a multi-objective programming problem, such that the Pareto  frontier is sought to explore. An efficient algorithm to reach the frontier is proposed and experiments are conducted to show the effectiveness of the proposed approach.\n\nSeveral issued should be addressed or clarified. In section 3.1, the population probability of text is assumed to be known, and the size of the true distribution and model distribution is N=|V|^L, which is exponential, so constrained optimization problem in section 4.1 has exponential number of constraints, thus MOP is an intractable. How to overcome this? In Section 5, the authors turn to an optimization problem which only depends on training data. Then what's the purpose to have sections 3 and 4? \n\nIn sections 3 and 4, why not use empirical distribution in stead of true but unknown population distribution?\n\nIn section 6, the experiments on synthesis data should be done in the same way as in SeqGAN's paper, such that the results can be compared. For experiments on real data, only experiments on middle-length data set COCO are performed, experiments on long-length dataset such as EMNLP News 2017 should be conducted too, since it is harder to get good results. \n\nMany important papers are missing, specially the language GANs falling short paper. Experiments should be compared with this paper and some representative language GANs. \n\nM. Caccia, L. Caccia, W. Fedus, H. Larochelle, J. Pineau, and L. Charlin. Language GANs falling short. In Neural Information Processing Systems Workshop on Critiquing and Correcting Trends in Machine Learning, 2018.\n\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong Yu, and Jun Wang. Long text generation via adversarial training with leaked information. AAAI, 2018.\n\nZhongliang Li, Tian Xia, Xingyu Lou, Kaihe Xu, Shaojun Wang, and Jing Xiao. Adversarial discrete sequence generation without explicit neural networks as discriminators.  AISTATS, 2019.\n\nKevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang, and Ming-Ting Sun. Adversarial ranking for language generation. NIPS, 2017.\n\nSidi Lu, Lantao Yu, Siyuan Feng, Yaoming Zhu, and Weinan Zhang. COT: Cooperative training for generative modeling of discrete data. ICML, 2019.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nContributions:\n\nThis paper studies an important problem, i.e., how to provide a theoretical framework to understand quality and diversity in text generation. To this end, the authors first provide a general definition of quality and diversity, and then study a MOP problem which tries to maximize both quality and diversity. By theoretical analysis, the authors concludes that: (i) there is truly a trade-off between quality and diversity; and (ii) quality and diversity can be combined to be a divergence metric. Further, the QDTC method is proposed for text generation. \n\nStrengths:\n\n(1) Novelty: I think this paper is novel. While most previous work on this topic are empirical, this work tries to formally define quality and diversity, and studies the Pareto-Optimality solutions. The authors also proposes a new QDTC algorithm for text generation. All these seem to be a quite novel perspective. \n\n(2) Writing: The paper is carefully written, and clearly presented. \n\nWeaknesses:\n\n(1) Experiments: My biggest concern of this paper lies in its experimental design. I understand that this paper is more like a theoretical paper, but proper experiments are still needed to justify your theory. Details are shown below. \n\na) MSCOCO is a very simple dataset. The sentences inside are simple. I believe the use of MLE for training already provides good results. Therefore, I think only reporting results on simple MSCOCO dataset is not enough. It will be good to add experiments on other datasets as well, as what have been used in the literature.\n\nb) What are the generated text looking like? Do they show qualitative difference when compared with MLE baselines?\n\nc) Experiments are only compared with simple baselines. Also, only BLEU-3 and Disinct-3 are reported. Comparing with other related work will be appreciated, such as SeqGAN, LeakGAN, TextGAN, MaskGAN etc. \n\nd) Two variants of the QDTC algorithm are provided. In practice, which one performs better? \n\ne) Since this task is hard to evaluate by nature, it will be good to include human evaluation, and demonstrate how well your proposed quality and diversity measures align with humans' preference, and how well your proposed QDTC methods compare with baselines.\n\n(2) Clarity: \n\na) I feel section 4 and 5.1 is a little bit challenging to follow, especially all the Lemma and Theorems. Though hard to follow, I still appreciate the authors a lot for all these derivations. Can the authors provide a concise summary on how these Theorems guide the algorithm design? \n\nb) The proposed method and theoretical analysis seem not restricted to text generation problems, and can be applied to image domain as well. Any comments on this?\n\nc) In Algorithm 1, it mentions that MLE pre-training is just an optional step. But in Section 6, it mentions that all the models are MLE pre-trained first. Can the authors show some results without MLE pre-training? Otherwise, it would be better to delete \"(optional)\" in Algorithm 1. \n\nOverall, I think this paper provides a novel perspective on quality and diversity for text generation. However, the experiments are not that satisfying, and could be much improved. "
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper looks at the perceived trade-off of quality and diversity in text-generating models. The paper proposes a lens by which to view this tradeoff, mathematically quantifying quality and diversity, and showing that there is a Pareto frontier associated with these quantities. The paper proposes a method for controlling quality and diversity of a learned model and evaluates it on a few datasets.\n\nOverall, I found several gaps in the story of the paper which makes me skeptical of the entire paper, leading to my recommendation of reject.\n\n-- In Eq 2, x is used both as an argument to a function and as a free variable in an integral. The intended equation is unclear.\n-- This happens again in Eq 3.\n-- It is unclear how the trade-off of the learned model is controlled exactly. For example, in Alg 1, how does one control for different desired quality/diversity trade-offs?"
        }
    ]
}