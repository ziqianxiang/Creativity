{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "In this paper, the authors propose the EfferenceNet, which first exploits some pre-trained network to obtain a continuous representation of the state, the dynamics model, which is parametrized as a neural network, will be learned by matching the temporal difference. Then, a best-first search algorithm with a heuristic criterion is conducted to plan in the latent represented space. \n\nOne of my major concerns of this paper is the novelty. Planning in latent space has been investigated for a long time with rich historic literature. Each component in this paper can be dated back to the classic text-book. The Koopmann operator [1], which considered to use 'infinite-dimension' pre-fixed nonlinear mapping to obtain the latent continuous representation, has been invented in 1932. Using temporal-difference to fit the dynamics also appears in classic textbook. The 'best'-first searching is just a variant of Breadth-first search with a heuristic ranking criterion. Maybe the only novel part of this paper is the connection to the efference copy and sensory reafference theory in neuroscience. However, this part is not clearly and rigorously justified. \n\nThe other major concern of mine is that experiments. The proposed method has neither been evaluated appropriately on the current benchmark, nor been compared with current state-of-the-art algorithms. The benefits of the proposed algorithm is not convincing. \n\nIn sum, I think this paper is not ready to be published.\n\n\n[1]  B. Koopman and J. v. Neumann,  “Dynamical systems of continuous spectra,”\nProceedings of the National Academy of Sciences of the United States of America\n, vol. 18, no. 3, p. 255, 1932. "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper learns a next-step transition model over embeddings produced by a pretrained CNN (specifically VGG16) and then performs greedy best-first search over this transition model to find a sequence of actions that will go from a start state to a goal state. This approach is evaluated on rotating cars in the NORB dataset from one viewpoint to another.\n\nThis paper demonstrates a nice implementation of a simple model-learning plus planning system. However, (1) it does not differ substantially from existing methods already in the literature, (2) the evaluation is insufficient, and (3) the proposed method is unlikely to scale to even moderately challenging domains. Thus I recommend rejection.\n\nFirst, learning and using models within planning systems has a large and rich literature, and links have already been drawn multiple times between human cognition and machine learning (see [1-2] for two recent examples). It is very common to learn action-conditional state-transition models---both from pixels, states, or state embeddings---and use them in a planning procedure [1]. On the model-learning side, then, it does not seem to me that the present paper is really different from what has been done before. Similarly, the planning algorithm is a very classic best-first search algorithm. While today more complex search algorithms are typically used (e.g. MCTS) the idea is roughly the same.\n \nSecond, the evaluation in the paper is minimal and insufficient. The model is only tested on a single example---one of the cars from the NORB dataset. The particular setup with viewpoint matching within NORB is in and of itself also very simple. To be more compelling, the paper would need to demonstrate results on much more challenging environments (at a minimum, rotation of a much wider range of objects; even better, a non-rotation task such as navigating through a maze).\n \nFinally, I do not think as presented the proposed method will scale to even moderately challenging domains. Greedy search is typically not used because it does not scale to cases with many local minima and maxima. Indeed, the results of Figure 3 demonstrate exactly the problem with using a greedy search.\n\nSome additional comments:\n \nIt is unclear how the algorithm terminates---there is no termination condition in Algorithm 1.  Since the model takes as input high-dimensional continuous inputs (images), it is unlikely to reach exactly the same state as the goal state. So, how is completion defined? For example, is there a threshold on the similarity to the goal state, which if exceeded would indicate that the goal state has been reached?\n\nThe first four paragraphs contain no citations yet make many claims about the brain (e.g. “the cerebellum receives 40 times more information than it outputs”) and about machine learning (“there has been much recent work on methods that take advantage of compact representations”). These statements need to be supported with citations.\n\nI didn’t really understand the justification for the experiments with the Laplacian Eigenmaps.\n\n[1] Hamrick, J. B. (2019). Analogues of mental simulation and imagination in deep learning. Current Opinion in Behavioral Sciences, 29, 8-16.\n[2] Pezzulo, G., Donnarumma, F., Maisto, D., & Stoianov, I. (2019). Planning at decision time and in the background during spatial navigation. Current Opinion in Behavioral Sciences, 29, 69-76."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes to train a one-step transition model given fixed pre-trained representations, and shows how to leverage it in a best-first search to reach goals in toy grid-world like environments.\n\nThis paper tries to tackle an interesting problem, but there is unfortunately not much novelty in its approach, and the use of pre-trained modules is rather unsatisfactory for ICLR. In this current state I do not believe this paper is appropriate for publication at ICLR.\n\n1.\tTheir use of the cerebellum literature analogy was quite unhelpful, and did not actually leverage any of the recent discoveries of the cerebellum community [see 1, 2, 3]. There is nothing in their model that actually computes prediction errors (i.e. “sensory discrepancy” in Figure 1), and in effect they just feed the action as any transition model would do.  It is not appropriate to point to such neuroscience literature in such a light-hearted fashion, I found this quite misleading and below the expectations of a paper submission to ICLR.\n2.\tThe target of training a transition model in a fixed representation state is not particularly novel, and the architecture is quite standard as well. It is unlikely to handle any stochasticity, or to perform well when used in a multi-step fashion.\n3.\tA comparison to some other recent publications would have been helpful:\n     a.\tVariational state tabulation: https://arxiv.org/abs/1802.04325\n     b.\tUPN: https://arxiv.org/abs/1804.00645\n     c.\tFloyd-Marshall Reinforcement Learning: https://arxiv.org/abs/1809.09318\n     d.\tSub-goals trees: https://arxiv.org/abs/1906.05329\n4.\tThe problems considered were really about moving on a grid with fully connected actions. The distance metric shown in Figure 3 did not seem particularly well-behaved, or indicative of this approach scaling up to more complex planning problems.\n5.\tThe Laplacian Eigenmaps usage was not described in enough details. How was it trained, what happened if you chose the wrong number of dimensions?\n6.\tThe paper is rather too sparse on implementation and hyperparameter details, which may hinder reproducibility.\n\nReferences:\n[1] Cerebellum, Predictions and Errors, Laurentiu S. Popa and Timothy J. Ebner, Front. Cell. Neurosci., 15 January 2019, https://www.frontiersin.org/articles/10.3389/fncel.2018.00524/full\n[2] Most of Reza Shadmehr work: http://www.shadmehrlab.org/publications \n[3] Older works by Daniel Wolpert et al: http://cbl.eng.cam.ac.uk/pub/Public/Wolpert/Publications/WolMiaKaw98.pdf\n"
        }
    ]
}