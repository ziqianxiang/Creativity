{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a family of new methods, based on Bayesian Truth Serum, that are meant to build better ensembles \nfrom a fixed set of constituent models.\n\nReviewers found the problem and the general research direction interesting, but none of the three of them were convinced that the proposed methods are effective in the ways that the paper claims, even after some discussion. It seems as though this paper is dealing with a problem that doesn't generally lend itself to large improvements in results, but reviewers weren't satisfied that the small observed improvements were real, and urged the authors to explore additional settings and baselines, and to offer a full significance test.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "Update after author response:\nI would like to thank the authors for the thoughtful response, and for addressing several of the concerns raised by the reviewers. The updated draft look cleaner and conveys the value of the proposal better. I am changing my assessment to \"6: Weak Accept\" (the smallest jump from 3 is to 6 in the portal. I would choose a 5 if that was possible). My concerns for the significance of results and lack of a baseline that takes worker quality into account still stand, but the results look better and the authors make a more convincing case prompting me to change my rating.\n---------------------------\n\nIn this paper, the authors propose a way to measure a notion of surprise (disparity between prior and posterior) and using that as a classification rule. So, if the posterior for a class is larger than its prior, the method outputs that class label. In order to estimate the average prior for a pool of models, the method recommends building additional models to predict these “peer-priors”, whereas the posterior is simply estimated by a maximum likelihood model (P(y=1) = Indicator(y=1)/K). The authors also propose a variant that directly tries to predict when the majority answer might be wrong. The authors show that such methods can do better than majority voting and some ensemble methods on a few datasets.\n\nThe paper looks at an interesting problem of when a majority vote response might be wrong by extending the notion of surprise previously defined for human labelers to machine learning models. The strong points of the paper:\n1. Simple and interpretable extension of a previously studied method. \n2. The method can be plugged into existing frameworks to replace majority voting.\n3. The results on the datasets considered seem good.\n\nHere are some of my concerns:\n1. The baseline of majority voting is fairly weak, since there are several models that take worker quality into account when aggregating responses. For example, the classic Dawid-Skene (1979) model. It’s not very promising to see a model just beat majority voting.\n2. The presentation of results in Table 1 and 2 can be improved. Even though it’s good to know how many predictions were corrected, it will also be good to see the overall accuracy numbers (like the ones Table 3).\n3. DMTS doesn’t really have the same underlying machinery as HMTS, since it doesn’t operate on the surprise measure, and putting them together in the same paper dilutes the focus. \n4. What are the weights in the weighted majority?\n5. The results in Table 3 show that HMTS is better than other ensemble methods but only marginally. Since HMTS uses more complex intermediate models (MLPs), I am not convinced whether the small improvement is from the proposed method or just more expressive models. For example, what would happen if the base classifiers in Adaboost were MLPs?\n6. How is minority defined in multi-class scenario?\n7. Minor formatting and grammatical issues: “likely to the correct”, “majority is tending to be correct”, “seemingly irrelevant topics”, “whether adopting the minority”, “aim to provide instruction to cases”, “linear regression” (should be “logistic regression” since it’s a classifier), and so on.\n\nIn summary, I think the paper has an interesting approach to an important problem, but with results that are only marginally convincing. I would have liked to see a more thorough empirical investigation to clearly establish the value of the proposed method. Based on these observations, I think the paper misses the mark and is slightly below the acceptance threshold for me.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Inspired by work in ensembling human decisions, the authors propose an ensembling technique called \"Machine Truth Serum\" (based off \"Bayesian Truth Serum\"). Instead of using majority vote to ensemble the decisions of several classifiers, this paper follows the \"surprisingly popular\" algorithm; the ensembled decision is the decision whose posterior probability (based on several classifiers) most exceeds a prior probability (given by classifier(s) trained to predict the posterior predictions). It's quite a nice idea to bring this finding from human decision-making to machine learning. If it worked in machine learning, it would be quite surprising, as the surprisingly popular algorithm risks that the ensemble makes a decision against the majority vote, which is usually consider the safe/default option for ensembling.\n\nOverall, I did not find the experiments (in the current state) to provide compelling enough support for the claim that MTS is a useful approach to ensembling in machine learning. \n* Unless I am mistaken, the authors use a more powerful model (an MLP) as the regressor compared to some of the models they ensemble over. In practice, people ensemble the most powerful models they have available, so it's unclear if using a regressor with the same capacity as the ensembled classifiers will provide any additional benefit. On a related note, it would be nice to know what is the classification performance of each individual classifier? As well as how often the regressor correctly choose to go with several weaker models rather than the strongest model. In particular, I am concerned that the performance of the ensemble might be less than or equal to the performance of a single MLP classifier (or whatever other model does best).\n* \"In this paper, each of the datasets we used has a small size - we chose to focus on the small data regime where the classifiers are likely to make mistakes.\" Why not try large data tasks that are challenging for state-of-the-art models? The paper makes a general claim that MTS is a good way to aggregate predictions, so only evaluating on small datasets seems to be a limitation\n* As I understand (correct me if I am wrong), the reported results are only on examples with \"high disagreement\" between classifiers. However, for practical use cases, it is useful to know how the overall accuracy compares. One major risk of using the \"surprisingly popular\" algorithm is that the algorithm may cause the ensemble to make many incorrect predictions when the majority is right (but the minority prediction is selected). If you have those numbers, I would be interested to see them added to the paper.\n\nI am also unsure about if applying the \"surprisingly popular\" algorithm in machine learning makes sense. The algorithm is motivated by the fact that difference agents have different information. However, in the ML setting, various classifiers usually have the same information. It's possible to restrict the information given to each classifier, but that would limit the performance of each individual classifier (and hurt the ensemble). I would be curious if the authors have any thoughts on this point.\n\nI also have a few questions/concerns about how the approach is implemented:\n* Why not train a single model to predict the average prediction of all models and use that model's prediction as the prior? This approach seems simpler but equivalent to the approach currently taken.\n* Why not use model distillation (predicting all output logits/probabilities, or an average thereof) rather than just predicting an average of 0/1 predictions?\n* For HMTS, why do the regressors for each of L labels need to be separate? It seems more efficient to use a multi-class model (as many model distillation approaches do)\n* If DMTS can learn to predict when most classifiers are wrong, why wouldn't the original classifiers themselves learn to predict the answer correctly? It seems to me that the reason the experiments show that DMTS/HMTS work is that some/many of the underlying classifiers are weaker than the model that is used to ensemble the predictions (an assumption that doesn't hold in practice).\n\n\nOverall, I really like the high-level idea, and a better ensembling approach promises to bring empirical gains across many ML tasks. However, I have several concerns about the experiments, motivation, and algorithmic decisions which make me hesitant to recommend the paper for acceptance."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary: This paper proposes two machine learning adaptations of the Bayesian truth serum approach to aggregating predictions from human experts. The first method proposed involves training two regression models for each classifier in the ensemble that predicts the proportion of other classifiers that assign the same label to a novel instance. The second approach is to train a binary classifier that, based on the features associated with an instance, determines whether the most common or second most common prediction made by individual ensemble members should be the prediction made by the ensemble.\n\nPros:\n+ The core idea of adapting Bayesian truth serum to ensemble prediction in machine learning seems sensible\n+ There is some evidence that the methods have an advantage over other common ensemble approaches in practice\n+ Although there are quite a few small English mistakes, the paper is well structured and generally quite easy to follow\n\nCons & questions:  \n1. The theorem statements and proofs are underwhelming. First they seem quite vague. Second, it’s not clearly spelled out how the theorems relate to the presented method, and whether they really says anything useful about its correctness or efficacy. At face value they do not obviously analyse the correctness of the algorithm as claimed on pg6. \n2. The paper contains some simple experiments, but I do not believe they are an adequate enough evaluation of the proposed approaches. The most useful comparison are the results given in Table 3, but only three datasets are used, the margins are small, error bars are not provided, and no significance testing is performed. \n2.1 Standard practice when comparing multiple classifiers on multiple datasets would be to employ Friedman/Nemenyi post-hoc tests to determine relative performance of methods---see \"Statistical Comparisons of Classifiers over Multiple Data Sets\" by Janez Demšar (JMLR, 2006).\n2.2 At minimum we expect Tab 3 to report results for all datasets used in the earlier experiments.\n2.3 The experiments compare with AdaBoost, Random Forest, and Weighted Majority. I feel that stacking is probably the most interesting baseline to compare with, as this is a method for learning how to aggregate predictions from ensemble members.\n\nOther: \nA. This paper does not directly deal with representation learning, so is only loosely relevant to ICLR.\nB. Not clearly unpacked why regression models need to be trained to predict \\hat{y}_i^j (Equation 4) when this quantity can be computed at test time without knowing the ground truth label?\nC. I do not understand the \"X out of Y\" description given in the caption to Table 1, which makes the results in this table hard to interpret. What is meant by \"classifiers' disagreement is high enough\"?\n"
        }
    ]
}