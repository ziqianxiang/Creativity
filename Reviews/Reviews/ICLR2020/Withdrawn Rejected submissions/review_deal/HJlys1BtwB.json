{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "- Summary \n\nThis paper proposes “data annealing”, a simple and intuitive way to leverage data in the source and target domains, which gradually increases the proportion of training data from the target domain. Such an approach helps achieve state-of-the-art results on three informal language processing tasks (named entity recognition, part-of-speech tagging and text chunking) with BERT-large as the base model. \nHowever, the scientific contribution of this paper is relatively minor. The experiments are lack of details and therefore not clear enough to support the arguments: i. the author(s) did not mention how they chose or tuned hyperparameters, for both their method and baselines---the comparison is not guaranteed to be completely fair; ii. the author(s) made some vague arguments that are not supported or fully supported by experiments; iii. it is hard to follow the error analysis section without knowledge of the dataset. Besides, a lot of relevant references are missing. \nI generally suggest that this paper should be rejected, even if it achieves new state-of-the-art results. \n\t\n\n- Detailed comments and questions\n\nAlthough the general idea is simple and easy to follow, there are a few unclear points:\n1. In the main text, D_S “represent the size of source data”. Does this mean “the number of instances in the source-domain training set” or “the number of instances in the source-domain training set times the number of epochs”? Note that it should be interpreted as the latter to ensure the correctness of Equation (3) when the model is trained for multiple epochs (at the transfer learning stage), though such an interpretation is definitely not natural. Also, how did you determine the value of D_S in Equation (5)? \n2. While alpha is chosen w.r.t. source data size (Eq. (4)), how was lambda chosen? Similarly, how did you choose about T_0 and lambda_0 for INIT and MULT? Did you tune these hyperparameters? \n3. Besides INIT and MULT, DA should outperform the following baseline (say, shuffle) before being claimed effective: (i) combine the source training set and the target one; (ii) at each step, randomly pick a batch from the combined training set. This is a strategy that looks like MULT, but the proportion of data from source/target domain could vary across batches. \n4. Sec. 3: “Linear decay is popular… initialization from source data.”\nThere is no experiment supporting the reason why linear annealing is not chosen . Have you tried linear data annealing scheme? \n5. Table 2 reports a single number for each model on each task---can the model be affected by any random factor (e.g., random initialization, random shuffle of training instances)? If so, how stable is it across different random seeds (in terms of standard deviation across several runs with different random seeds)? \n6. Sec. 4.2.2: “in order to have a fair comparison with LSTM, it is necessary that we also use the CRF classifiers in BERT baselines.” \nSuch an argument is not rigorous. It is hard to say BERT can have a really fair comparison with LSTM---there are a lot of other factors that affect the fairness (e.g., BERT has seen much more data than LSTMs at the pretraining stage).\n7. Sec. 5.2:  “We notice that the performance improvement is not as large as in the NER task … This suggests that the similarity between source data and target data is an important factor to consider in the transfer learning process. “ \nNER and POS tagging are completely different tasks---it is unfair to compare the absolute F1 scores across different tasks and draw any conclusions based on that comparison. \n8. Sec. 6: “we randomly sampled 30 sentences…” \nIt would be good to show the randomly sampled sentence in appendix. \n9. Sec. 7: “we show that when data annealing is applied with LSTM or BERT, it outperforms different state-of-the-art models on different informal language understanding tasks”\nDA LSTM did not outperform state-of-the-art model on POS tagging. \n\n\n- Missing Reference \n\nThe paper proposes a transfer learning/domain adaptation technique, but few work on the topic was cited. To name a few in NLP, https://www.aclweb.org/anthology/W19-4302.pdf\nhttps://www.aclweb.org/anthology/D17-1038.pdf\nhttps://www.aclweb.org/anthology/P07-1033.pdf \nI strongly recommend the author(s) to also take a look at the papers cited by them and those cite them. \n\nIt would be good to state the relation and difference between the proposed approach and curriculum learning (Bengio et al., 2009). \n\n\n- Minor comments\nTitle: “learning” -> “Learning”. \nEquations: * -> $\\cdot$, to avoid confusion with convolution. \nJust introduce the abbreviation at the first mention. \nTable 2: “BERT_{BASE}” -> “Vanilla BERT_{BASE}”. \n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors present a method that they call \"data annealing\" to help improve the performance of pretrained models on informal data. The method presented consists merely of giving progressively more importance to informal data over formal data. They evaluate their approach on sequence labeling tasks such as named entity recognition (NER), part-of-speech (POS) tagging and chunking.  The paper reads more like an analysis of one hyperparameter of a model than an actual contribution, and presents little to no novelty. The algorithmic or technical contributions of this paper are not sufficient for a venue like ICLR."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper tackles the problem of NER, POS and chunking in informal text data. The work attempts to leverage the much better performance that is achievable in formal text data by using an annealing technique to accomplish transfer to informal language tasks. The proposed method works by feeding in formal language data, and then slowly adjusting the proportion of informal language data that is fed to the BERT or LSTM network in order to allow it to initially learn parameters based on the source task and then update to the target task.\n\nThe authors show that the data annealing procedure results in slightly better performance than competing transfer learning methods. However, the increment seems very small. Looking at the results, it appears that the chosen domain may already be close to some theoretical maximum performance as the gains achieved are quite small.\n\nTo me, the method seems to be incremental. While there is a performance increase, the method does not seem to be particularly well motivated theoretically, and it is also not completely clear to me why DA would perform better than MULT. As such, I do not find the approach convincing. Is there some strong argument for why reducing the proportions is useful?\n\nMinor issue:\non pg. 7 - RiiterPOS should be RitterPOS\n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a data annealing strategy for transfer learning. This strategy controls the proportion of source data and target data, using more source data during the early training stage and exponentially decrease the amount of source data. Unlike the conventional transfer learning methods which use fixed amount data from source and target domain, the proposed data annealing strategy adjust the proportion of data to utilize knowledge from the source domain knowledge while focusing the target domain.\nThe reviewer has following arguments:\n1. The novelty of the paper is very limited. The presented data annealing procedure is similar to the fine-tuning strategy on motivation and behavior. The difference is that the fine-tuning uses all source samples to pre-train the model parameters and apply it for the initialization of target training, while the presented annealing strategy cuts down the proportion of source data progressively. Another refining strategy which iteratively moves more pseudo-labeled target instances into the training batches equivalently decreasing source data proportion. These strategies are commonly seen in transfer learning methods. Based on these considerations, the novelty of the proposed data annealing procedure is limited.\n2. The presented annealing strategy brings a very empirical hyper-parameter lambda. According to the derivation, the initial source proportion and the size of source dataset are all critically dependent on this parameter. While there is little description about choosing the hyper-parameter in Section 4.\n3. The experiment scenarios are limited in NLP, while the proposed data annealing procedure is claimed to have comprehensive ability in utilizing source domain knowledge.\n4. In the second term in equation (3), the subscript of the summation should be t=1.\n5. In the last paragraph of Section 3, it is the source data proportion that will decay to 0 in a short time rather than alpha, the fixed initial proportion."
        }
    ]
}