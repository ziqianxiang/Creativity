{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper studies the problem of catastrophic forgetting and proposes a number of methods for overcoming this issue, under various circumstances. The crux of the proposed methods is the a criterion in eq. 1 for minimizing the change in likelihood over the previous datasets. The authors use this as a regularization term and compare and contrast various versions of it with previous work such as EWC.\n\nOn the surface of it, the proposed method(s) are sensible, though if I understand things correctly, the main proposed criterion (eq 2) is very similar in shape to EWC, with the main change being L1 vs L2. I can buy that L1 produces different solutions compared to L2, as argued by the authors around eq. 2, so that's fine. On the other hand, I find the contribution of this work relatively minimal. The dissection of case I-IV is good though these are basically small iterations of eq. 1. If I understand it correctly, most (all?) of the cases could have been analyzed with an L2 penalty instead of L1 -- I think this would have added some value to the overall theme of L1 being a better choice  than L2, no?\n\nA few other comments:\n\n* I have a small but potentially important reservation about the experimental protocol -- I don't see any mention of a validation set, it's not clear to me how the hyper-parameters were actually chosen?\n* While I appreciate the stds being provided over 5 seeds, there are many cases in which the confidence ranges between various methods actually overlap: this is not made clear in the tables themselves (there are cases in which the proposed methods overlap in confidence intervals with the other baselines)\n* There's some discussion in section 6 about the magnitudes of optimal L1 and L2 hparams found by the authors, but I don't know if the \"stronger\" lambda is really an indication of anything (it could be due to optimization quirks related to gradient descent).\n* Are the baseline results (EWC and SI) lifted from the previous papers or done by the authors themselves? It's not immediately clear.\n* I don't know how excited I should be about the fact that all results are on variations of MNIST, it does not instill a lot of confidence that these results would generalize to other domains.\n* I would have liked the authors to create at least some toy examples or datasets where the proposed methods should shine -- right now it just looks like a list of four semi-sensible alternatives that are basically treated as hyper-parameters in the results table and it's not obvious why someone would choose one of these vs another.\n\nAll in all, I can see some value to this work -- certainly a relatively small modification that has some practical benefits, but I'd like to see this better positioned -- empirically, especially -- and better analyzed (cf. my comments about L2)."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "In this paper, the authors try to solve continual learning prolem, which is a longstanding goal of AI. There are 3 ways to mitigate forgetting: 1. architectrual approaches, 2. regularization approaches, 3. memory approaches. The authors focus on the second one, that is, reguarization approaches. In order to mitigate catastrophic forgetting problem, L_2 strategies was proposed previously. In this paper, they use L_1 strategy instead. I think the mothod is not novel, which does not reach the level of ICLR."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper introduces a framework for combatting catastrophic forgetting, based upon changing the loss term to minimise changes in classifier likelihood, obtained via a Taylor series approximation. They compare different versions of their method with EWC and a method based on freezing important weights on three benchmarks.\n\nI like the core concept that the paper proposes. It is explained well, and the paper is well-written. It is interesting to see EWC explained in this framework. However, although the paper starts off with some nice theory (Equations 1 and 2), it then becomes a heavily empirical paper, with many seemingly arbitrary decisions made (and only some high-level comments provided explaining them), for example for Case II and IV, and for the 'constrained' method. Why were these changes to the loss function made? Can they be justified somehow theoretically?\n\nI also have some misgivings regarding the experiments, as they are only performed on MNIST-sized datasets. The proposed method should (if I understand correctly) be scalable to larger datasets/architectures, which most other works in this field do. Additionally, other methods (there are many, but for example VCL [1]) significantly out-perform the proposed method.\n\nI am on the fence about this paper. Although I like the core concept and some of the experimental ideas (such as the 'freezing' method), this paper makes many decisions about its loss function that I do not understand theoretically. Doing this means that empirical results should be strong to back up these decisions, but the experiments are only on MNIST and, although they out-perform EWC, do not out-perform other continual learning works. This means that I am leaning towards reject. I hope to see a future version on this paper with improvements!\n\n[1] Nguyen et al. Variational continual learning. ICML, 2018."
        }
    ]
}