{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes an interesting algorithm towards domain adaptation that combines graph learning and label propagation along with cycle consistency. The proposed model is simple and interesting making clever use of labels in the source domain. While the theory is motivated well and seems convincing, I am not entirely sure of the empirical results.\nThere are no significant improvements of the proposed model over the baselines considered except on STFL-CIFAR data set. An analysis of why this improvement is observed would be good.\n\nI also wonder on how did the cluster quality on the target domain compare with that obtained by the AssocDA baseline. Also coming to the parameter sigma in the Gaussian similarity function, it would be nice to see the values of sigma reported in either the body of the paper or in a supplement.\nI will admit I have not thoroughly checked the math in Theorem 1, but the supporting mathematical equations seem plausible."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this work, the authors propose a new domain adaptation framework through label propagation. Specifically, the labels are propagated using a manifold structure. The clusters of features in each domain are aligned to be close by enforcing the cycle consistency. Moreover, DANN and LGAN is used in the final objective function to align the distribution and enable local manifold consistency. Experimental studies have been done to verify the effectiveness of the proposed method.\nOverall, the paper is easy to follow. The idea of label propagation by enforcing the cycle consistency through graphs is interesting. The proof of theorem is simple and correct. My major concern is on the significance of the paper, which is limited. The overall objective consists of four important terms in which L_class, L_dann and L_local are from the existing works. The only contribution is on the L_cycle. However, the cluster alignment idea has been studied in [ref1], while the current work completely ignores [ref1]. Although the ways of aligning the clusters are different between the two works, more discussion and comparison with [ref1] are necessary. Here are some detailed comments:\n(1)\tThe authors claim the scale parameter \\delta is learned. Can you show the learned \\delta for different domain pairs and discuss how the \\delta affects the final performance?\n(2)\tIt is not clear how matrix T and W are associated. It seems that T is the normalized W. However, they are completely in the same form. Why does W still preserve the identity block matrix I after normalization?\n(3)\tThe theorem 1 is for ideally clustered source data, which may not be possible in reality. How the perturbation of unideal clustered source data, e.g., some clusters have overlapping, affect the final performance? The discussion is necessary as this is the case for the complex real-world data.\n(4)\tEq.(3) uses L_1 norm. Have you tried any other norms?\n(5)\tThe results of table 1 for the other baselines are directly from the paper. It is not clear whether the same experimental setting is applied, which makes the comparison less convincing. \n(6)\tTable 2 demonstrates L may not bring benefits. Although the authors analyse the reason, i.e., the performance of the approximation of the local manifold, it is necessary to give some principled guidelines to guide the usage of L or not when given a new task.\n(7)\tThe comparison with [ref1] should be given. \n(8)\tRegarding the selection of the Hyper-parameter in appendix, the authors discuss some possible ways. Which one is used in this work? Is it following (Bousmalis eta al. 2016)? Moreover, why set different ranges for different hyper-parameters? Table 3 shows different tasks use different hyper-parameters. How to set these hyper-parameters given a new task, especially when no target labelled data is available?\n(9)\tHow the batch size is selected? Is it based on the final performance? The transfer performance seems to be sensitive to the batch size. The authors should give more details on this point. \n[ref1] Cluster Alignment with a Teacher for Unsupervised Domain Adaptation. \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper involves several common-seen techniques of domain adaptation, source label guided classifier training with a softmax cross entropy loss, GRL guided domain discrepancy minimization with a domain discriminator, and label propagation with manifold learning.\n\nThe label propagation is the novelty of this work, which proposes a cycle consistency on the source labels, for local class manifold alignment.\n\nThe local manifold consistency loss in Eq.(4) seems to have the similar physical insight with Eq. (3), but introducing the GAN model, which is similar to the perceptural loss. \n\nI have some concerns.\nThe LGAN from the paper is a pre-trained generator without re-training, as can be seen from Eq.(5) and (5). So, how do you have a control on its adaptation of LGAN to the DA problem of this paper? I also think that this seems to be an incremental method by simply introducing LGAN into the traditional DA model for data augmentation.\n\nFrom Eq.(6), the L_cycle and L_local are the new terms based on the traditional DA. Actually, I have some concern that because of the domain distribution difference, the similarity matrix Wts between target and source data cannot well reflect the true similarity. I understand that the L_dann loss can align the marginal distribution across domains. So, this is an problem of \"chicken first or egg first\", which depends on the training strategy of the model.\n\nThe experimental comparison is not strong between the proposed and the CDAN. Actually, there lacks of an ablation analysis, by removing the L_local and L_cycle.\n\nAt least, from the given experiments, the performance of the proposed losses is not significant, with at most 0.5 percent improvement."
        }
    ]
}