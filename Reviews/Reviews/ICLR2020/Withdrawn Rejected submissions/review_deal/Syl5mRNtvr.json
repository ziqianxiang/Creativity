{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed an adversarial generative model of a formal grammar, called “adversarial grammars”, whose generated grammars have multiple possibilities. Thanks to multiple options of generated grammars, the proposed method can produce a long-term future prediction. The proposed method achieved state-of-the-art performance on the future 3D human pose prediction task and the future activity prediction task.\n\nThe paper tackles an exciting topic to attendees of ICLR, but the paper contains several issues that I will try to cover in the following.\n\n(1) (No clarity about contributions)\nFirst: The related works section is not well described. Several relevant papers are shown in the related works section, but the relationship is ambiguous. How are grouped existing works which tackle the same problem? Where is placed the proposed method in that grouped literature?\nSecond: Contributions are not clear. At the beginning of the introduction section, the authors noted that they addressed the problem of modeling sequential dependencies and multiple possible long-term future predictions. However, those challenges are well studied in the machine learning literature, and treated by a lot of works such as RNN. For example, a simple 1-layer RNN can model the sequential dependencies as its latent representations and can generate multiple possible predictions with multiple evaluations. Please explain your contributions more clearly.\n\n(2) (Incremental proposed method)\nThe proposed method can be regarded as a simple extension of GAN to produce formal grammars. This type of extension is already well-studied in other application areas such as CV or NLP. I’m afraid of the low impact of this paper in its current form.\n\n(3) (Presentation)\nThe presentation is slightly confusing.\n- In equation (5), what is “min max =“? Should we remove the “=“?\n- The type of each variable or function is ambiguous. For example, what is the type of the function “G”? Does “N \\rightarrow {(N, \\Sigma)}” mean “N \\rightarrow 2^{N \\times \\Sigma}”?\n- From the introduction section, it seems that the proposed method can be applied to so many future prediction problems. However, it verified only on the pose prediction task and the activity prediction task. I’m afraid of the statement by authors too strong.\n- The architecture of the proposed method is explained in detail in the approach section. The overall architecture figure helps readers understand the proposed method."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary: \nThe paper introduces a probabilistic grammar formulation for the task of predicting labels of future timesteps. The grammar is simplified to only contain productions of the form A -> Bc with nonterminals A and B and terminal c. The current nonterminal encodes the current ‘state’ and the current terminal encodes the label of the current timestep. The production encodes the transition from one state to the next as time proceeds. The grammar is formulated using a number of neural feed-forward networks in order to make the rules, nonterminals, and terminals all differentiable and thus learnable via backpropagation. Since the grammar can generate samples, it can be (and is) trained in a generative adversarial framework where a learned discriminator D must determine if a pair of sequences of nonterminals and terminals are generated by the model or from the data. The model is evaluated against baselines on two activity-forecasting / -recognition datasets and outperforms all baselines. It is also evaluated on a pose forecasting dataset and outperforms a number of prior works here, particularly at longer time horizons.\n\nOverall:\nReject. While I think there are merits to the approach, the explanation of the model was opaque and did not sufficiently compare and contrast to existing work both in the model definition and in the experimental evaluation. In particular, from my understanding of this model (made challenging by the explanation) and the limited probabilistic grammar that it encodes (only A -> Bc), it seems nearly identical to a (neural) hidden Markov model (i.e., a hidden Markov model that uses deep networks to parameterize the conditional distributions, e.g., [1]). Second, empirically, from a quick google search, other papers have published results on both the MultiTHUMOS (e.g., [2]) and Charades datasets (for the latter, there was even a competition [3]), none of which are mentioned here. \n\nClarity: Well written but not particularly clear. Model section is somewhat confusing and needs clarification, and the experiment section is missing details on the variants of the proposed model. It was also difficult to determine the specific way that adversarial training was performed; the explanation given was of high-level adversarial training but lacking sufficient specifics to reimplement.\nSignificance: Potentially useful but quite limited by the above-mentioned issues. I am not an expert in this application domain, however.\n\nDetailed comments:\nSection 1.\nThe introduction should better clarify what is meant by ‘adversarial sampling’. I was uncertain whether this meant a GAN-like training procedure or something else entirely until later sections.\n\nSection 3.\n- The model, as far as I can tell, does not learn transition between continuous events in time. It is given a sequence of discrete timesteps and learns discrete labels for those. Specifying ‘continuous events in time’ is misleading.\n\nSection 3.1.\n- It would be helpful to include an example of a nonterminal, terminal, and production at the beginning when the grammar is defined.\n- The grammar defined is non-probabilistic; however, the grammar used later is probabilistic. The definition should match the actual model.\n- Is a nonterminal a vector in R^D? And a terminal a vector in R^|\\Sigma|? This should be said explicitly.\n- r is used to denote both a “set of rules” and later “a specific rule”. Please use different notation or at least boldface to distinguish these.\n- Further, r is said to be a “set of rules” but then said to be a vector whose elements specify the probability of a set of rules. It is one or the other, not both. Does the latter mean that there’s a learned matrix (or tensor) of rule vectors that the probability vector indexes? Or are the rules themselves just indicators in a vector and this is the probability of each one? Please make this more clear.\n- The notation G(A) = { (f_N(f_R(A)), f_T(f_R(A))) } produces a set containing one pair containing two lists, which does not match the prior notation of {(B_i, t_i)} = G(A).\n- This whole section would be made much more clear by explicitly defining the vectors as members of R and the mappings defined by the functions. Currently, it is quite confusing and I had to go over it many times to try to understand what exactly was being proposed.\n- Does the model not take in images after the encoding of the initial frames? Is it simply used to roll out future labels? This seems like a missed opportunity for a more useful model. \n\nSection 3.2.\n- This reads as a high-level description of applying GAN-style training to the proposed approach but does not provide the reader with enough information to know how this was actually done. For example, were sequences subsampled? What lengths of sequences were used? Was anything done to prevent overfitting, which can be quite problematic in these setups (e.g., [4])? This is unfortunate because I think that this training approach could be a useful contribution, as previous methods for training these styles of model rely on variational inference or other complex methods. \n\nSection 4.\n- I was glad to see some minor ablations of the grammar without the adversarial training in the experiments. However, I could not find an explanation of how it was trained if not using an adversarial method. A better description of the variants of the proposed approach should be added.\n\n\n[1] Structured Inference Networks for Nonlinear State Space Models. Krishnan, Shalit, and Sontag (2016).\n[2] https://paperswithcode.com/sota/action-detection-on-multi-thumos\n[3] http://vuchallenge.org/charades.html\n[4] Task-Relevant Adversarial Imitation Learning. Zolna et al. (2019).\n"
        }
    ]
}