{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper presents the argument that ReLU networks do not have accurate estimates of confidence in areas that are far from the training data. They suggest explicitly estimating the Covariance of the final layer of a network as a way to more accurately estimate the confidence.  They provide theoretical results that bound the logit predictions and use this to choose their covariance matrix. \n\nThe clarity of the paper is not great. They do not discuss the relevance of many of their theoretical results and instead present them in a large list. The experiments are limited, but the results are promising, particularly Figure 3. \n\nOverall the paper feels like it has a lot of content without a lot of substance. This may be because they spent a lot of time listing all their theorems, but not as much motivating why these theoretical results add novelty and are important. Many of their theoretical results are rather obvious (and use well-studied techniques), but they have not been presented explicitly for neural networks."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper addresses the issue of overvonfidence of ReLU nets identified in [1], where the predictive probability distribution collapses to one-hot encoding of the most probable class as the input goes to infinity. The authors argue that by modeling a distribution over the weights of the network, this issue can be alleviated.  \n\nI start by summarizing the results of the paper. As far as I understand, all results are specific to the case, where only the distribution over the weights of last layer is being modeled. Also, all the results are derived for binary classification and assuming an approximation to the predictive distribution (equation 4). For this setting the authors show that if we use a Gaussian distribution over the weights (of last layer), the predictive distribution in the limit of input going to infinity is not collapsing to a one-hot vector, and is determined by the parameters of the Gaussian. In particular, the probability of any particular class is bounded away from 1, as the input is going to infinity. This is the main result of the paper, presented in Section 2. In Section 3 the authors reason about what happens specifically when the Gaussian is constructed using the empirical covariance matrix of the feature vectors (inputs to the last layer), and the Laplace approximation. The result of this section, summarized in Corollary 2.8, isn’t stated very precisely. Finally, the experiments are evaluating a laplace approximation applied to the last layer of ReLU nets on out-of-domain detection for image classification datasets.\n\nThe main issue I see in the paper is the following. The paper does indeed show that modeling a distribution over the weights of the last layer of a ReLU network fixes the asymptotic over-confidence, and without affecting the accuracy of the classifier. Notice that this result doesn’t imply anything for the quality of uncertainty estimates close to the data. It is possible to come up with other trivial modifications of ReLU networks that would satisfy the same property. For example, we could replace the sigmoid nonlinearity of the final classifier with something like h(z) = max(min(0.9, z), 0.1), which would also lead to uncertainties that are asymptotically bounded from 1, and would not change the accuracy of the classifier. For this reason, the significance of the main result of the paper isn’t clear to me.\n\nAlong the same lines, I think the title of the paper is misleading. The title suggests that Bayesian inference provably fixes the predictive uncertainties for ReLU networks. However, the analysis is only concerned with asymptotic overconfidence as inputs are going to infinity. Further, the results hold for any distribution, and the connection to anything Bayesian is only made in Section 2.4. Finally, the analysis is about modeling distribution only for the last layer of the network, not all layers in the network, which would be assumed by default from the title. \n\nFinally, the experiments are performed using a Laplace approximation for the last layer (LLLA) of ReLU nets on multiclass image classification problems. The method is used to perform out-of-domain detection and compared against the methods proposed in [1]. I don’t think the authors frame LLLA as a contribution, because both being Bayesian over the last layer [2] and Laplace approximations for neural networks are well-known techniques.  Further, the theory only guarantees that the uncertainty will be present as the input goes to infinity, but doesn’t guarantee that the uncertainty will increase monotonically as the input moves away from the data distribution. So, for finite inputs, the theory doesn’t guarantee that Bayesian last layer would work for out-of-domain detection. \n\nTo sum up, I am in favor of rejecting the paper in its current state. I agree that the observation that virtually any distribution over parameters fixes asymptotic overconfidence of ReLU networks is interesting. However, I don’t think that on its own it is sufficient for accepting the paper to ICLR. If the authors could also obtain sharp results for the predictive uncertainty in non-asymptotic regime, or use their observations to motivate a new method, it would be a really strong paper.\n\nMinor issues: \n- In Section 4 CEDA and ACET are introduced in Section 4.2, but ACET is first mentioned in 4.1\n- On page 11, in the proof of Theorem A.5, in the first line of equations after equation (14) in the denominator you have (delta U x + b) instead of (delta U x + c).\n\n\n[1] Why ReLU networks yield high-confidence predictions far away from the training data and how to mitigate the problem\nMatthias Hein, Maksym Andriushchenko, Julian Bitterwolf\n\n[2] Scalable Bayesian Optimization Using Deep Neural Networks\nJasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Md. Mostofa Ali Patwary, Prabhat, Ryan P. Adams\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper argues that an approximate Bayesian treatment of the parameters of neural networks with RELU activation improves predictive uncertainty and reduces the problem of asymptotic over-confidence for out-of distribution data. \n\nI have mixed feelings regarding the theoretical analysis. The paper is well written and the theoretical analysis is done thoroughly. \nHowever, the results are hardly surprising for anyone working with Bayesian neural networks. Furthermore, I am concerned regarding the usefulness of this analysis, since it is pursued for the trivial case of a Laplace approximation applied to the last layer only. This is neither novel nor convincing compared to previous approaches such as Kronecker-factorized Laplace approximations or variational approximations applied to all parameters. \nThe approach can be seen as just a Laplace approximation of Bayesian logistic regression applied to features extracted by a “MAP-pretrained” neural network. \n\nI suppose that the last layer Gaussian approximation can be seen as a sufficient condition for preventing the problem of asymptotic over-confidence in neural networks with RELU activations. If so, it might make sense to emphasize this. However, the significance of this result alone is questionable since one should not expect well calibrated uncertainties from a last-layer approximation.\n\nThe authors assessed correctly that a MAP-centered Gaussian preserves the decision boundary, whereas other methods such as MC Dropout do not. However, this is only the case for the *last layer* approximation. Furthermore, this is not even a generally desirable property (as presented in the paper). The decision boundary from the expected posterior predictive distribution (the quantity of interest) can be very different from the MAP decision boundary for non-linear models such neural networks for which all parameters are treated as random variables. \n\nRegarding the experiments: Given that the theoretical contributions are a little weak, I would have expected at least a stronger experimental evaluation. \n\nMinors:\n- Fig. 1 b) and 1c) are identical. \n- the probit approximation was first proposed by Spiegelhalter and Lauritzen 1990.\n- Laplace approximations are hardly “full Bayesian”; it is just one type of approximation as so many other variants.\n"
        }
    ]
}