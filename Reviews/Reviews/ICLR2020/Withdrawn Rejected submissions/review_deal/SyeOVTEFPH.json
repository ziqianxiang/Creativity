{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper addresses a limitation of adversarial training: in the most common formulation, the radius of L_p balls used to constrain the adversary during training is shared by all training points. Instead, this paper proposed to set multiple radii, where each point is given a radius appropriate given how far it lies from the decision boundary. The intuition behind this paper is correct and confirms recent findings: e.g., on excessive invariance in adversarially trained models. \n\nTo set the radius for each example, a heuristic is introduced in Section 3. Because the heuristic is introduced without justification, it is difficult to assess the main contribution of this paper. Simple additions to the paper could help revise the submission. For instance, were other heuristics considered? If yes, why was this one chosen in particular? \n\nIt may be problematic to set the value of the radius based on the approach that will be used to attack the model: perhaps other prototypicality metrics could be used for setting the value of this radius independently of any algorithms for finding adversarial examples? It would be interesting to use a different attack to set the radius during training and to attack the model at test time.\n\nIn other words, given that the value of the per-example radius is set based on the model being trained, it is difficult to appreciate how much Algorithm 2 over or under estimates radii. The inadequacy of certain models in setting radii appears to be the motivation for the warm up step. Expanding Section 3 a bit to help the reader build their intuition for this aspect of the approach would help.\n\nClaims of robustness are made empirically. Unfortunately, results presented make it difficult to assess the validity of the approach. What is the value of epsilon chosen to attack the model in Figure 3? Does the adversary adapt and compute a per-example epsilon instead of a fixed epsilon? This should help attack examples whose radius during training was larger.\n\nHave you considered how the approach interacts with algorithms that achieve certifiable robustness?\n\n\n1: repetition in “and enforcing large margins around these samples produce poor decision boundaries that generalize poorly”\n\n1: The formulation of adversarial training was introduced in earlier papers from the 2000s, prior to Madry et al. \n\n2: The result described in Figure 1 was presented in Jacobsen et al. [https://arxiv.org/abs/1903.10484]\n\n2: The relationship to Ding et al. is not entirely clear, and would benefit from additional clarification.\n\n4: Typo WideRenset \n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Adversarial training typically trades off generalization accuracy for robustness to adversarial attacks. This paper introduces a simple but seemingly effective heuristic to improve this trade-off. Instead of performing adversarial training for a fixed radius $\\epsilon$, the authors propose to use a datapoint specific radius which is adapted during training: the radius is increased or decreased based on whether adversarial attacks (generated by k-steps of projected gradient descent) succeed in correctly classifying the label at the updated radius value. At a high-level this captures whether an example is close to the decision boundary or not. Only “safe” adversarial examples, which do not cause an error in the prediction and are thus “far from the decision boundary”, are then used for adversarial training. The method, while being a heuristic in nature is shown to be effective for various architectures in the ResNet family, on CIFAR-10, CIFAR-100 and ImageNet.\n\nWithout being an expert in the domain of adversarial attacks, the method and evaluation appears sound. While the proposed method scores low in terms of novelty and would benefit from added theoretical justification (e.g. an adaptation mechanism which exploits local geometry of the loss surface), there is something to be said about simple and effective method.\n\nThat being said, I do have two main reservations which I would like to see addressed or discussed in the rebuttal. With respect to the targeted ImageNet evaluation, which generates adversarial attacks using PGD-1000 for various values of $\\epsilon$, the proposed method only seems effective for small values of $\\epsilon$: larger values of $\\epsilon$ are much less robust than standard adversarial training. Without being familiar with standard evaluation protocols, why should we care more about robustness at $\\epsilon=4$ vs $\\epsilon=16$ as used in [Xie, 2019] ? It seems that for a method to be effective, we should not be comparing single point estimates but the whole accuracy-robustness (i.e. showing better natural accuracy for any value of “target” robustness). Second, the ablative analysis is unfortunately missing the two most importance ablations: namely the impact of the hyper-parameters controlling the update step and moving average coefficients of the instance adaptive radius parameters. Finally, it is also regrettable that the “simple heuristic” is in fact complicated by a warmup schedule which starts adversarial training with fixed radius, before starting the tuning process after a fixed number of updates / epochs.\n\n\nDetailed Comments / Questions:\n* Have you considered multiplicative updates to $\\epsilon$ as in the Levenberg–Marquardt algorithm?\n* Background section should acknowledge the existence of black-box or non-gradient based attacks, as discussed in [R1]\n* Equation 1: should read $f(x_i + \\delta_i)$ and not $f(x_i) + \\delta_i$\n* Algorithm 1, line 9, 10: should this not read f(x_i^{adv}) ?\n* Algorithm 1, line 11: please change to reflect that a partial minimization is performed wrt theta (e.g. a single SGD step).\n* Tables: for readability, please highlight in bold “best” methods for each column.\n* Figure 4: change y-axis label to “Adversarial Accuracy”.\n* Figure 5(b): plotting epsilon trajectories for samples having highest and lowest epsilon values (at convergence) could be more informative.\n* Figure 6: “visualizing training samples with their corresponding perturbation”. Please change caption as you are not plotting the perturbation, but rather showing the instance adaptive radius obtained at convergence, for each training sample.\n\n[R1] Adversarial Risk and the Dangers of Evaluating Against Weak Attacks. Uesato et al."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a new adversarial training method, which uses an instance-specific perturbation magnitude for each training instance. This methods can alleviate the accuracy-robustness tradeoff observed in previous works. The adaptive instance-specific perturbation magnitude is updated in each training iteration. Extensive experiments on CIFAR and ImageNet prove that the trained models has better natural performance and similar adversarial robustness with previous works.\n\nThe paper is generally well-written. The motivation behind the proposed method is clear. The experiments can consistently prove the effectiveness of the proposed method on training robust models. However, I have some concerns about the proposed algorithm.\n\n1. In algorithm 1 step 9-11, the authors split the training batch into the correctly classified samples and incorrectly classified samples, and train the model based on robust loss or natural loss for each part. However, there is no explanation on why it is necessary to do so, since previous works either directly optimize the robust loss for every sample or optimize the natural loss and robust loss together. The authors could discuss more on this choice.\n2. In algorithm 2, the \\epsilon selection procedure needs to generate adversarial examples for \\epsilon_1 and \\epsilon_2, which requires at least 2 times of computation complexity compared with the previous works. The efficiency of the proposed method could be poor. Are there any accelerating algorithms?\n3. How do you set the hyper-parameters \\beta and \\gamma? Is the result greatly sensitive to the specific values of these parameters?\n\nOverall, the authors could elaborate more on the technical details to help to better understand the proposed algorithm. "
        }
    ]
}