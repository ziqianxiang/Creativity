{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a cyclical training scheme for grounded visual captioning, where a localization model is trained to identify the regions in the image referred to by caption words, and a reconstruction step is added conditioned on this information. This extends prior work which required grounding supervision. \n\nWhile the proposed approach is sensible and grounding of generated captions is an important requirement, some reviewers (me included) pointed out concerns about the relevance of this paper's contributions. I found the authors’ explanation that the objective is not to improve the captioning accuracy but to refine its grounding performance without any localization supervision a bit unconvincing -- I would expect that better grounding would be reflected in overall better captioning performance, which seems to have happened with the supervised model of Zhou et al. (2019). In fact, even the localization gains seem rather small: “The attention accuracy for localizer is 20.4% and is higher than the 19.3% from the decoder at the end of training.” Overall, the proposed model is an incremental change on the training of an image captioning system, by adding a localizer component, which is not used at test time. The authors' claim that “The network is implicitly regularized to update its attention mechanism to match with the localized image regions” is also unclear to me -- there is nothing in the loss function that penalizes the difference between these two attentions, as the gradient doesn’t backprop from one component to another. Sharing the LSTM and Language LSTM doesn’t imply this, as the localizer is just providing guidance to the decoder, but there is no reason this will help the attention of the original model. \n\nOther natural questions left unanswered by this paper are:\n- What happens if we use the localizer also in test time (calling the decoder twice)? Will the captions improve? This experiment would be needed to assess the potential of this method to help image captioning.\n- Can we keep refining this iteratively?\n- Can we add a loss term on the disagreement of the two attentions to actually achieve the said regularisation effect?\n\nFinally, the paper [1] (cited by the authors) seems to employ a similar strategy (encoder-decoder with reconstructor) with shown benefits in video captioning.\n\n[1] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Reconstruction network for video captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7622–7631, 2018.\n\nI suggest addressing some of these concerns in a revised version of the paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes an architecture that grounds words from a captioning model, but without requiring explicit per-word grounding training data. Instead, they show that it is sufficient to use cycle consistency, verifying that by predicting word->grounding->word the two words are the same. \n\nGeneral: \n\nCycle consistency has been shown to be very useful in replacing explicit paired data, for eample in image-to-image translation (CycleGAN, or the more recent FUNIT). This paper takes it to the domain of vision and language. While the novelty is not very large it seems like a solid step in an interesting direction.  Evaluated on both image and video captioning with substantial localization improvement in the specific relevant eval settings. \n\nSpecific comments: \n\n-- In Table 1, the part of \"Caption Evaluation\" the proposed method is in bold, but it seems that \"Up-Down\" method out-performs the proposed method in B@1 and B@4. \n\n-- Are words that are not nouns/verbs (the/a/are/with/etc) handled differently? It doesn't really make sense to localize them just like object words. \n\n-- The localization model is linear? What would be the effect of richer models on localization accuracy? \n\n-- Qualitative analysis: It would have been useful to add evaluations by human-raters to measure the perceptual quality of the localization. \n\n-- Error analysis? examples and analysis  of failure cases? \n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper addresses captioning generation for images and videos, by proposing a novel cyclical training regimen consisting of three steps: decoding, localization, and reconstruction. The experimental results show that the performance on image captioning and video captioning are improved without grounding supervision.\n\nI lean to accept this paper. The motivation using cyclic feedback itself is not so novel for language generation, but focusing on grounding without localization supervision for visual captioning is interesting. The experimental results show that the proposed method can boost performance both qualitatively and qualitatively. I have several comments and questions below.\n- Why do the authors introduce GVD without self-attention as a baseline? Table 1 and 2 show that removing self-attention degrades the performance. If the combination of self-attention in GVD and cyclical training proposed in this paper is complementary to each other, it does help to improve the overall accuracy.  \n- While the authors develop a cyclical training pipeline, including decoding, localization, and reconstruction, Figure 1 does not show which part corresponds to the decoding phase. The authors should clarify it to make the paper easier to be understood.\n- Equation (5) seems to be strange. $\\theta^*$, a sum of two parameters for each arg max operator, doesn't guarantee that each term in the right side of Eq. (5) keeps its max. This equation seems to be a conceptual one, and the actual training would be performed according to Eq. (7). Therefore, the experimental results might not be influenced by the error in Eq. (5).\n- $\\hat{r}^l_t=\\beta_t^\\top R$ between Eq. (6) and Eq. (7) means that $\\hat{r}^l_t$ is a row vector while $r_n$ seems to be a column vector. Since $R = [r_1, r_2, ..., r_N]$ for $N$ regions, $\\hat{r}^l_t= R \\beta_t$ seems to be appropriate. The authors should correct it. I have a similar comment for $\\hat{r}_t = \\alpha_t^\\top R$ in Eq. (2).\n- In the caption of Table 5, the number equal to or smaller than ten should be spelled out; \"5 runs\" should be \"five runs.\" There are similar errors, such as \"5 GT captions\" and \"1 GT caption\" in Sec. 4.\n- The format of items in References is not consistent.\n- According to Sec. A.4.1, $\\lambda_1$ and $\\lambda_2$ are tuned between 0 and 1. How are the experimental results sensitive to these hyperparameters? Additional experiments using different $\\lambda_1$ and $\\lambda_2$ would be helpful."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "# 1. Summary\nThe paper deals with the problem of learning grounded captions from images without joint text-location information, but instead texts (words in captions) and locations are provided independently and the model needs to figure out their link. The model is built upon GVD (Zhou et al., 2019): each word generated by the encoder is grounded to the locations provided by the region proposal module (Up-Down model (Anderson et al., 2018)), used to reconstruct the ground-truth caption.     \n\nMy weak reject decision was guided by the following strengths and weaknesses of the paper.\n\nStrengths:\n* The reconstruction formulation of the problem is interesting and relevant for the task\n* Proposed new metric to measure grounding performance\n      \nWeaknesses:\n* Questionable motivations: it is not clear what application is grounding text to the image useful for?\n* Marginal (not statistically significant?) improvement on image captioning metrics by using grounded text (proposed method) compared to not grounding (GVD)\n* Limited novelty: extension of GVD, where attention is removed and object locations are used instead\n     \n      \n# 2. Clarity and Motivation\nThe paper is generally well written, however there are some concerns on motivations. \n\nOne concern is related to the motivations of the paper. The authors use grounding as a proxy to improve image captioning results, which improvement is marginal wrt GVD (see Table 1). Why do we need to localize text if this has very marginal impact on the captioning metrics? It is missing the link between the potential applications where the localization of words is relevant. \n\nThe authors claim that they do not uses any grounding annotation, however the pre-trained Faster-RCNN has been trained using annotations which consist of bounding boxes + categories. Therefore, the model do (in an implicit way) rely on grounding annotations, especially because there might be an overlap between the words (classes) used to pretrain the detector and the words in the captions. The authors should assess if this ovelap/bias in the pre-trained Faster-RCNN exists or not.\n\nSome other questions are still to be answered:\n* How are the regions R parametrized? Is it the visual representation or bounding box locations?\n* What happens to words that are not grounded to the image (e.g., verbs or articles)? Do you have a special way to deal with those?\n* What is the intuition of multiplying word embedding and region embeddings to generate z in Eq. 6? \n\n\n# 3. Novelty\nThe proposed method is an extension of the existing model GVD, where the attentional module is removed and its functionality is replaced by the cyclical training mode with reconstruction of the object locations. From the technical point of view this is limited novelty, but still an interesting improvement of the model; however the experiments and results do not support the claim that using such model improves image captioning result in a significant way. One way to answer to this question would have been by showing an application where the outputted locations are used for downstream tasks.\n\n\n# 4. Experimentation\nThe experiments are carried out in a scrupulous way, by showing the comparing with GVD (with and without attention; with and without grounding supervision). The non-convincing part of them (as mentioned above already) is the fact that the improvements on these datasets might be non significant for image captioning. For example, let's consider the image captioning results in Table 1 (Flickr30k Entities): cyclical have a max improvement of 0.7 (CIDER) and min of 0 (B@4) when compared with GVD without grounding supervision. There is an obvious huge improvement on the grounding evaluation, which is obvious since GVD does not do it explicitly. The same trend is in Table 2. \nThese results are not convincing, combined by the fact that it is not clear in which applications one would want a very accurate grounded text.\n\n\n# Minor Points\n* Sec. 3.1: it is not clear that the Language LSTM is the decoder. Please explicitly say it before Eq. 1\n* Caption of Table 3: which dataset is this?\n"
        }
    ]
}