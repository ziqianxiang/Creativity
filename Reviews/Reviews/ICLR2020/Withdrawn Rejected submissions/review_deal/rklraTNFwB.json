{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper examines whether it is possible to train agents to follow synthetic instructions that perceives and modifies a 3D scene based on a first-person viewpoint, and have the trained agents follow natural language instructions provided by humans.\n\nThe paper received two weak rejects and one weak accept.  The main concerns voiced by the reviewers are:\n1. Lack of variety in natural language\nOne of the key claims of the paper is that previous work on instruction following can only handle instructions generated from templates and cannot handle ambiguous expressions used by real people, and that the contribution of this work is that it can handle such expresssions.  However, as pointed out by R1, the language considered in this work is very simplistic in form (close to being template based) with the main variation coming from synonyms.  Even the free-form natural instructions that are collected, are done so with very specific instructions that restrict diversity of language (e.g don't use colors or other properties of the object). R1 also point out that there are prior work that handles much more diverse language.\n\n2. Limited technical novelty and questions about how much the proposed CMSA method actually contribute\n\n3. Overclaims and lack of precision when using terminology\nThere is concern that the task that is addressed is not actually that complex.  The environments are simple (with just 2 objects) and not that realistic.  Tackling 2 tasks is barely \"multi-task\", and commonly, \"manipulation\" refers to low-level grasping/picking up of objects which is not how it is used here.\n\nWhile the paper has many strong elements and is mostly well written, considerable improvements still need to be made for the paper to have claims it can support.  It is currently below the bar for acceptance. The authors are encouraged to improve their paper and resubmit to an appropriate venue.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper considers the task of instruction following where an agent navigates/interacts with a 3D environment conditioned on goals provided in natural language. While several existing approaches use synthetic language for instructions, the authors tackle this problem under the setting of noisy instructions provided by humans in natural language. For this, they use large-scale pre-trained representations (e.g. BERT) as initial parameters for representing the textual instructions. Their main result is the demonstration of transfer from agents trained using synthetic instructions to environments with more variation (e.g. synonyms) or natural instructions provided by humans on two tasks involving object manipulation. \n\nPros:\n1. Nice application of BERT to grounded instruction following tasks\n2. Good empirical results\n\nCons:\n1. Not much technical novelty\n2. Empirical experiments could use a bit more rigor in terms of disentangling the major factors that contribute to performance (e.g typo noise)\n\n\n\nOther comments:\n1. Are the BERT weights frozen or finetuned along with the rest of the model? Does the performance depend on this?\n2. The typo noise (TN) seems to be a key driver of performance. Have you tried adding it to the other baselines like wordPiece Transformer? \n3. What are the scores when training on the test tasks (D.O synonym, natural instructions, etc.) directly? It would be good to establish how well the transfer setup is doing compared to the best RL agent trained directly on the test scenarios.\n\n—————\nPost rebuttal update:\nThanks to the authors for their response and for updating the paper! I especially appreciate the additional experiments, but I’m still confused why the authors do not perform a clear ablation study to support their claims. It seems like the main claimed novelty of the paper is the proposed CMSA method. However, the empirical results are not convincing/rigorous enough to provide the reader information on 1) whether CMSA is a useful method (since it is used only with BERT and does not seem to affect results on its own compared to MP, SA, TN, etc.) and 2) when should one use/not use BERT and CMSA (BERT+CMSA actually does quite poorly acc. to table 5). Further, the other reviewers also pointed out concerns regarding the difficulty of the task and complexity of language used. Hence, I feel the paper still requires some revision to form a coherent story — updating my score accordingly.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "The authors present a method of transferring template-based instruction following agents to natural language instructions by using language encoders trained on large text corpora. They explore different ways of combining text-based language encoders with visual representations and compare them. They find that contextual phrase-based representations learned by BERT significantly improve the performance on natural language instructions. \n\nStrengths:\n- The paper is written well, it is easy to understand and follow.\n- The task setup is good, authors collect natural language instruction data from humans.\n- The paper presents several language encoding methods for the task and systematically evaluates them in a scientific manner.  \n- The experimental results indicate that it is possible to transfer an agent trained on template-based instructions to natural language instructions using language models trained on large text corpora. It is not necessary to train the agent on natural language instructions. I find this result important and useful.\n\nWeaknesses:\n- The paper lacks significant technical novelty. It essentially combines known reinforcement learning based instruction following agents with known language models. The different ways of combining language encoding with visual representations are either trivial or adapted from prior work.\n- A major concern is that the natural language instructions considered in the paper do not have much diversity with respect to language. The paper only considers lifting and putting tasks and trains a separate model for both the tasks.  \n-- The lifting task always uses the verb 'lift' and replaces the object word with synonyms or referring expressions. There are 80 objects in the lifting task and I suspect there are very few referring expressions for these objects and they mostly involve a synonym. Furthermore, at test time, the agent only needs to distinguish between 2 objects. The performance with random embedding is around 50% for this task and the best model is around 76% which means the agent is not recognizing the correct object around 50% of the time. \n-- For the putting task, authors consider synonyms for object words and natural instructions which involve changing the verb ‘put’. It seems like humans mostly use only 4 verb words for this task, ‘put’, ‘keep’, ‘move’, ‘place’. This might be an artifact of the examples given to the human annotators. In any case, this word is inconsequential as the agent always lifts one of 3 available objects on one of 2 fixed objects. \n- It seems like the most diversity is coming from synonyms which can probably be handled with a dictionary or wordnet rather than requiring a language model. There is also some prior work on handling synonyms (https://arxiv.org/pdf/1902.04546.pdf). I would have liked to see many more tasks and a multi-task learning model which is also able to distinguish between the task based on natural language instructions in addition to understanding object word synonyms and referring expressions. More objects would also help.\n- The authors claim to tackle “more behavioural and environmental challenges than previous work”. I do not agree with this claim. It is true that this paper handles object interaction and natural language instructions in a partially observable setting, however, previous work has tackled other challenges which this work does not tackle. For example, Oh et al. 2017 generalize to new sequence of instructions, Hermann et al. 2017 and Chaplot et al. 2018 also handle compositionality and generalize to unseen instructions referring to new objects, Hermann et al. 2017 handle negation, Chaplot et al. 2018 handle instructions involving ‘largest’ or ‘smallest’ objects, Misra et al. 2018 handle more diverse natural language and so on.\n- I wouldn’t call moving objects using high-level symbolic actions as ‘manipulation’. This is a whole research area in robotics involving taking low-level actions to move an object. Also, the environment used in the paper is not visually realistic in my opinion. It looks game-like and visual encoders trained in this environment are unlikely to generalize to the real world. This is fine as it is mostly irrelevant to handling natural language instructions, but authors should not claim visual realism and object manipulation in my opinion. \n\n\nComments/Questions\n- I do not understand the meaning and purpose of some actions. Why are there GRAB + actions? Doesn’t the object move with the agent once it is grabbed? What is SPIN_OBJECT? Why is it needed? It seems like there is no ‘place’ action, how does the agent place the object? I am guessing when the agent stops taking GRAB+ actions. If that is the case, then wouldn’t it be easier to just have Grab and Place actions rather than 16 GRAB+ actions?\n- The meaning of ‘(sub)-’ in (sub)-word is not described.\n- What is the probability of typo noise in the experiments?\n- Many experimental details are missing. How long was the model trained for both the tasks? How many training samples/episodes? What were the hyperparameters used for reinforcement learning? Learning rate, optimizer, discount value and so on.\n\nUpdates after author response:\nI have examined the author response and additional experiments carefully. I am maintaining my score due to the following reasons:\n- The authors seem to agree that the paper does not provide any 'substantial algorithmic advance'.\n-  The authors argue that the number of objects in the paper is much more than prior work. However, the focus of prior work (referenced by authors) was not to tackle natural language. Since the focus of this paper is to tackle natural language, I believe the number and diversity of objects and tasks need to be much higher than 80 objects and 2 tasks used in the paper.\n- \"We would language referring to an even wider range of motor-behaviours, e.g. more 'verbs'\" -> I do not understand what the authors are trying to say here, but if \"learning the motor programmes for such concepts in an environment\" is not \"the focus of the present paper\", I believe the focus is only handling synonyms and referring expressions for objects. In my opinion, these are relatively easier to tackle, (for example using a dictionary or wordnet) than grounding 'verbs' into sequence of actions, which limits the scope of this paper further.\n- The authors still claim to tackle object manipulation and visual realism which I do not agree with. I do not believe taking high-level \"GRAB\" action can be called object manipulation. The objects are taken from shape net with relatively realistic shapes, but neither the appearance of objects (textures, shadows) nor the relative arrangement of objects is realistic. In my opinion, a model trained in this environment has no hope of generalizing to the real-world. \n- I do not agree with the authors' argument against chance performance in the lifting task. From my experience, I believe an RL agent trained without any language input would perform at 50% if it receives a reward for lifting one of the two available objects. I do not understand why the authors chose to put only 2 objects in the environment. Why not put 5 or more objects?\n- I appreciate authors' efforts towards multi-task learning results, however, tackling only 2 tasks is not convincing enough.\n\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "This work proposes applying natural language encoders pre-trained on a large text corpora (e.g. BERT) to training agents to follow natural language instructions in a simulated environment. Overall, I greatly enjoyed reading this paper: clear exposition of the idea, sensible model architecture, reasonable baselines and good experimental performance. I only have minor questions & feedback, see below.\n\nPros\n- Well designed experiments with sensible baselines.\n- Strong transfer learning results.\n- Illuminating analysis where using BERT indeed performs better on capturing phrasal and sentence-level equivalence in natural language instructions.\n\nQuestions\n- In Table 5, BERT doesn't give performance improvement on natural instruction (over Word embedding+Transformers) until BERT+CMSA+TN. Why do you think this is the case? To phrase this in a different way, why do you think BERT+MP doesn't perform well on this?\n- Are the results in Figure 2 computed from a BERT+MP model or a BERT+CMSA model?\n- In the lifting results in Table 4, why doesn't BERT+CMSA+MP outperform BERT+MP?\n\n------ \n\nUpdates:\n\nHaving read other reviewers' comments and also the authors' response, I would also like to call into question the difficulty of the experiments in the paper -- for the lifting task, the model is always presented with a command \"Lift X\", and essentially only needs to identify the correct object out of two at test time. Also, for the putting task, the model only needs to disambiguate between 6 possible combinations (3 movable and 2 fixed objects). Especially with a sophisticated model like BERT, I would have liked to see tasks where human instructions are more complex than this simple task. Hence, I'm changing my score to 6.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}