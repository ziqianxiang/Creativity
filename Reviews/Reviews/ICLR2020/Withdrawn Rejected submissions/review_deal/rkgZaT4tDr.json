{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper explores the application of RL to the problem of match plan generation for answering queries in Microsoft Bing.\n\nA potential strength of this paper is to draw attention to a new application of RL, one that potentially has significant social and financial impact. The paper provides some insights into how to cast the problem into the RL framework, in particular the definition of the parameterized action space.\n\nThe paper’s weaknesses include:\n-\tInsufficient detail on some aspects of the problem specification, including a clear explanation of match plan generation, definition of the state space variables, and an explanation of what the transition function might correspond to.\n-\tThe learned policy is only tested on a held-out test set, not in a simulator.  This is not convincing  for RL applications, where the distribution of states changes as a function of the policy, so the estimate of the learned policy will be inaccurate on a policy collected from a different policy (unless correction is applied, e.g. importance sampling, which is not mentioned in the paper).\n-\tInsights as to how the findings from this new application of RL might inform other applications of RL.  This would be necessary for publication in a venue such as ICLR, where most participants & readers are primarily interested in new methods.\n-\tThe dataset on which training is done is not public.  This is not absolutely necessary, but would certainly increase the value of the paper for the ICLR community.\n\nAdditional comments:\n-\tYou seem to use a reward shaping function (bottom of p.6, “Reward design”).  Is this just used for training (as a shaping signal), or also in testing the policy?   It is somewhat concerning that you need to add this explicit signal to modify the policy.\n-\tThe results on the games from Bester et al. (2019) do not necessarily strengthen the paper.  It’s not clear what is “your algorithm” that you test and compare to others, and how the games considered are similar/different than your target application.\n\n\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper presents a parameterized action RL match plan generation method which extends the\nplan generation to the general case without any predefined knowledge. Key to address the problem\nare normalized softmax values of discrete actions to enable gradients backpropagation, parameter\nspace noise on parameters of the policy for unifying the exploration direction in both discrete and\ncontinuous spaces, and recurrent deterministic policies with prioritized replay buffer to accelerate\nand stabilize the training.\n\nThe paper is well written.\n\nStrength: This paper applied RL method on match plan generation. It shows superior performance than manual methods. \n\nWeakness: Although the experimental results are exciting, the method itself is nothing novel. I do believe this is a good paper but not sure it is the best fit to this conference. \n\nI am not an expert in this specific area. I'll leave the the decisions to other reviewers and ACs."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "This work presents an approach for training policies to generate match plans using reinforcement learning. Match plans are used in search engines to retrieve documents for a given user query. One of the challenging features of this problem is that the action space is a mix of discrete and continuous parameters. This work formulates the task as an RL problem, and proposes a number of design decisions for training an effective policy.\n\nThis paper presents an interesting application of RL, but overall the technical innovation of the method is fairly modest, and mainly consists of different components that are already commonly used in previous work. The writing also has a fair bit of awkward phrasing and grammatical errors that hurt the clarity of the exposition. For these reasons, I do not think this work meets the bar for ICLR in its current form.\n\nMore specific notes:\n\nAs someone who is not familiar with search engines, I have some trouble understanding the concept of a match plan from the description provided in the paper. I’m not sure how the rewrite process works. Perhaps adding more details of this, maybe in the appendix, can help readers better understand the task.\n\nThe captions for the figures are not very informative.\n\nIn Table 2, what is the difference between “prioritized” and “prioritized sample”?\n\nIn Figure 3, it is not clear why it is necessary to show the embedding. Most samples seem to show similar performances and it’s not clear if there is any obvious structure. Perhaps this figure can be simply replaced with a history of performances. While Section 4.2 suggests that some clusters with worse or better performance exists, this is not really evident in the plot itself. I’m also not sure what “We guess the reason is that the baseline with hand-crafted rules do not consider the embeddings, thus it is less affected by embeddings.” means. How does the fact that the baseline does not use the embedding affect any structures in the embedding?\n\nThe paper provides a fairly thorough review of a lot of different design decisions, but perhaps it is not necessary to go into such excruciating details for all of the different components. Some details such as padding short episodes with zeros are probably not vital for the overall narrative. It might be appropriate to relocate some of these details to the appendix.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}