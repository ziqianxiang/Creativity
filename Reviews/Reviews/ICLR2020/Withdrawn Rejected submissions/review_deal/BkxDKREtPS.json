{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper describes a systemic approach for detecting the type of a classifier based on the computation and use of a per-pixel signature image feature. It claims that a specific classifier, e.g., a VGG, or other type of CNN, will produce a different signature image from another classifier, or from the same classifier when trained with manipulated data, e.g., by an unknown  Trojan. Some results are shown that illustrate how this signature feature can be used to differentiate between 4 different CNNs, or when data are manipulated.\nAlthough the paper has some merit, in creating and using the specific signature, there is no deeper analysis of why and how this feature can differentiate between classifiers, or of the dependence on the problem under investigation. It cannot be claimed to produce a robust feature that can be applied to quite diverse cases and environments (the inability to handle, for example all-to-all attacks is admitted in the paper). Moreover, the title is misleading, since the method does not necessarily refers to CNNs, but in general to some classifier. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper presents a signature that summarize the structure and weights of deep networks for MNIST images. This signature is then used to determine if a training set contains a backdoor or not. The signature takes the form of a single image, but does not depend on individual training examples. The authors show that the signature is not model specific and generalizes within reason.\n\nStrength:\n + Novel and simple idea\n + Good results on MNIST\nAreas of improvement:\n - Approach lacking motivation\n - Limited to MNIST\n\nI found the idea presented on the paper interesting and haven't seen this before. The paper is well written and easy to understand, and the technical section contains sufficient details to reproduce the paper. The evaluation shows that their method works well on MNIST, and even generalizes between classifiers.\n\nThe technical section lacks some motivation. Why is (1) a good signature? Why not just use the max, instead of the value at the argmax? This would be easier and save some space. Other alternatives include taking the difference between min and max, or starting with random noise or random images. It would be nice to know that what is presented in the paper is (locally) optimal among similar design choices.\n\nThe second area of improvement is in experiments. The current version of the paper seems tailored to MNIST (and only MNIST). In fact, the formal definition of the signature doesn't even contain a color channel used in most deep network inputs. Using this signature on any other dataset would require training 500+ networks on a single dataset. For anything other than MNIST this seems infeasible (e.g. on CIFAR training time for a single network is 4+hours leading to 2k+ hours to train a classifier for this signature). It would help if the authors could comment on how they plan to generalize this to real datasets (e.g. CIFAR, ImageNet, COCO, ...), where larger models (e.g. ResNet-152, ...) are required. The current detection results in section 6, don't actually defend the detector from backdoor attacks, but rather classifiers trained on the result of a detector.\n\nMinor:\n * Why is the signature computation O(m x n x K x V) instead of O(m x n x V). A single forward pass should provide all K probabilities required for the computation of the signature.\n * The writing is a bit defensive at times. e.g. first paragraph in Sec 3: Conceptually, we are inspired by ... but these two also have a large difference to your work. No need for the \"but ...\".\n * The correct ImageNet citation is \"ImageNet: A Large-Scale Hierarchical Image Database\""
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a one-pixel signature of a CNN classifier, which the paper suggests can be used to detect classifiers that have been trojaned -- meaning that unknown to the CNN provider, the CNN developer has put in a weakness so that the CNN will systematically misclassify if you add a marker to an image, such as a red diamond in the top left. The signature is computed by finding out for every pixel and every class, what input value would most change the class assignment.\n\nThis really isn't an area that I know well, but here are my thoughts:\n\n- From the distance at which I'm working, all 4 of the listed contributions of the paper seem to be basically the same contribution, which is perhaps best expressed as the second contribution. That is, I take section 5.2.3 as the key results of the paper.\n\n - The signature idea seems to basically work, and can detect attacks of the sort described. It can also be used for other things, such as distinguishing a ResNet vs. AlexNet classifier when you're treating it as a blackbox, but I'm not sure how useful this is.\n\n - Indeed, I'm not really sure how useful this line of work is period, but I guess I can see some applications for making sure that no one has monkeyed with a classifier that is being provided to an end user by a third party.\n\n - Computing the signature still seems to me like it could be pretty expensive (image height x width x num channels x num input values), though they claim that's not so bad.\n\n - It seems to me that there are lots of other things you might do to create a signature of a classifier, like looking at it's response to a set of test patterns. I wonder whether they mightn't work just as well as the method presented here, though I really have no idea.\n\n - But this does seem a real weakness of the paper: There are no baselines. If the argument is that this is a good method of backdoored classifier detection, and given that there is not at present any significant literature on this problem, surely the reader deserves to be shown a few candidate simple baseline methods for backdoored classifier detection, both to prove that the problem is not generally trivial and that the method advanced here works significantly better than these baselines.\n\nSo, overall, this paper wasn't clearly interesting and well-done to me, and so I lean to leaving it out, but someone invested in the area might feel differently."
        }
    ]
}