{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper proposes a gradient-based method to optimize architecture hyperparameters of convolutional neural networks such as the number of channels and depth dimensions. The proposed method utilizes the natural evolutionary strategy to make the optimization of hyperparameters differentiable.\n\n- It would be better to compare the proposed method with state-of-the-art methods such as AMC and MetaPruning in terms of the optimization cost.\n\n- To make the contribution of this paper clearer, it would be better to compare the proposed method with a random search.\n\n- It is unclear what uniform rescale baseline is in Table 1 and 2. Please elaborate on it.\n\n- About the baseline results under GPU latency constraints in Table 1, how do you get these results?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper is motivated to optimize the neural network architecture encoded by categorical variables and / or integer variables. The technique used in this paper is based on the so-called natural evolution strategy. The goal is well stated, and the research topic is important. However, there is an non-negligible overlap to an existing work (Akimoto et al, ICML 2019) that has the same motivation and uses a quite similar approaches.\n\nDetailed comments below:\n\n\"However, optimizing these hyperparameters jointly is challenging for existing approaches. There is not yet an effective yet efficient solution.\"\n\nThere are existing works addressing the neural architecture search encoded by categorical and ordinal (integer) variables. For example, Akimoto et al (ICML 2019) proposes a one-shot neural architecture search framework based on the natural gradient. The architecture parameters are encoded by the mixture of categorical variables (representing types of operations, etc) and integer variables (representing the number of filters and their sizes, etc). \n\n\"As the loss function is non- differentiable with respect to the architecture hyperparameters, we use natural evolutionary strategy (NES) to approximate the gradient of the architecture hyperparameters. \"\n\nActually, the above references follows the same idea as the natural evolution strategy. More precisely, the algorithm is derived from the information geometric optimization (IGO) framework (Ollivier, JMLR 2017) that generalizes NES. \n\n\"Our contributions are three-folds: 1) We propose a novel gradient based approach to jointly opti- mize the architecture hyperparameters in a unified manner. 2) We adopt natural evolution strategy to approximate the gradient of the non-differentiable architecture hyperparameters. 3) We verify the effectiveness and efficiency of proposed method with state-of-the-art results in various network structures.\"\n\nFrom above perspective, the first two contributions are not novel in this paper. There are differences in details from the above reference, but there are huge overlap. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "1. Summarization \n\nThis paper considers the hyperparameters engineering problem in network architecture design. Unlike existing works optimizing these parameters based on human knowledge or grid search, the work proposes to search for them automatically in a gradient-based manner. More specifically, it focuses on three dimensions in network hyperparameter, i.e., network depth, number of channels at each layer and image resolution, and uses natural evolutionary strategy (NES) to jointly search for optimal values in all three dimensions. This work is among the first to introduce NES into network hyperparameter optimization, and demonstrates its effectiveness by applying it to multiple backbone networks. However, the experimental results are limited. \n\n2. Strength\n\nThis paper is well-motivated, as most of the works consider only one aspect of the network hyperparameters at one single time. Jointly exploring hyperparameters for network architecture design is a promising direction to explore. The proposed gradient-based search algorithm (based on NES) is neat and easy to follow. The experiments show that the method is effective at saving computation cost and boosting performance, though itâ€™s limited. \n\n3. Weakness\n\n1) Lack of details in methodology. For example, how to jointly deal with depth and number of channels per layer. As depth varies, the number of hyperparameters used to represent channels changes. So how you define the variables to jointly parameterize both depth and number of channels per layer.  \n\n2) There are some existing works discuss the relation between depth, channels and image resolution, for example EfficientNet [r1]. How this work is related and superior to such works is not presented. \n\n3) Lack of baselines. Multiple works explore towards the same direction, such as FbNet[r2], Mnasnet[r3], and [r4], there is no direct comparison to these methods and discussions. \n\n4) In experiments, most of the cases, the proposed method is only marginally better than baselines. How the searched hyperparameters is different from or similar to baselines are not present and analyzed. \n\n5) On MobileNet V1 and under GPU latency constraints, is the case the proposed methods substantially surpass the baseline. However, it is not well-explained, which makes the results suspicious. \n\n[r1] Tan, Mingxing, and Quoc V. Le. \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\" arXiv preprint arXiv:1905.11946 (2019).\n[r2]  Wu, Bichen, et al. \"Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n[r3] Tan, Mingxing, et al. \"Mnasnet: Platform-aware neural architecture search for mobile.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n[r4] https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html\n"
        }
    ]
}