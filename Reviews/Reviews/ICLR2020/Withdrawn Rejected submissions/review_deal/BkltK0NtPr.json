{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper discusses the effects of over-parameterization in deep neural networks. The main claim is that for any continuous activation function the loss function does not have so-called “setwise strict“ local minima, i.e., one can find a path to the global minimum without increasing the energy along the path. The main assumption is that if the number of datapoints is N, the activation function has derivatives up to N^th order and there exists at least one layer in the network with at least N neurons. The paper seems to build upon the theory of weakly global functions which are functions for which all setwise strict local minima are setwise global minima.\n\nThis paper should be rejected for the following reasons:\n\n1. The main claim of the paper is not true. It contradicts previously published results, e.g., https://arxiv.org/abs/1611.01540\n2. The development in this paper is not at all rigorous. I do not see a proof of the claim. The main narrative consists of a host of special cases and elementary examples. The appendix is complicated with more special cases, e.g., Appendix D, proposition 3 is for a 1-dimensional input and one hidden layer. I do not follow the proofs for the claims in Appendix E.\n\nI would encourage the authors to make their main narrative self-contained."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper theoretically studies the benefit of overparameterization for the loss landscape in fully connected deep neural networks. Here the definition of overparameterization is the number of training samples larger than the number of neurons in a hidden layer. This paper shows that if the last hidden layer is overparameterized (with some more mild assumptions) than loss function is weakly global. This implies there are no bad strict local minima. Authors also show by example that overparameterization networks could have bad non-strict local minima could exist.\n\nDue to last minute request, I have not been able to digest the proof so I won't be able to say anything regarding the correctness of the paper. \n\nWhile there are interesting observations one could find on overparamerized network's loss landscape,  with current form the paper's presentation is unclear and I wan not convinced on its relevance to realistic settings.  \n\nPros:\nThe paper is tackling an important question regarding the benefits of overparameterization. As authors note isolating benefits coming from overparameterization is important study.\n\nAssumptions used to prove the theorems are general enough except for the overparameterized assumption.\n\nCons:\nThere are few problems I encountered reviewing.\n\nI found paper unclear to read and understand. Maybe due to last minute submission, the paper does not appear to be polished and needs more work making the presentation clear.  Beyond various typos, broken reference (unnecessary Yu et al. infront of Yu & Chen (1995)), I did not find section 2 from 1-dimensional case especially helpful for understanding impact of the paper. Also since various previous works have similar claims to this paper, it would be clear to distinguish how current results are distinguished in the contribution section. For example as authors say Nguyen et al (2018)/ Venturi et al (2018) also show that non-existence of bad strict local minima. The contribution section may become clear if authors could describe specific contributions beyond what already have been described in literature. Is the relaxation to continuous activation function that is significant for example?\n\nI have concerns about the applicability of the results. For example, in realistic data would A3 or B1 hold? For example with 50k MNIST, one would need 50k hidden layer and the benefit of overparameterization in practice appears for much smaller networks.  Also for Theorem 2 assumption B2 is quite strict which doesn’t include ReLU/TanH/Sigmoid activation functions. \n\nFor applicability, I wonder if there would be a practical guidance based on findings in the paper which would make the results more impactful. \n\nWhile it is important to understand what type of minima overparameterization network brings in, wouldn’t more relevant true phenomenology of deep networks would be captured through statistical arguments in terms of how likely our initialization and optimization algorithm would fall into a certain type of minima. I was not convinced some of the particular examples in low dimensional settings reflects what realistic over/under parameterized neural networks are showing. \n\nQuestions and Comments:\nHow is counter example in section 6.1 an example of overparameterized case? I only see one hidden unit with 1 data point which is not overparameterized by paper’s definition. \n\nPlease remove unnecessary “Yu et al.”\n\nTypo p1 last paragraph “has -> have”"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This work studies how over-parametrization smoothens the loss landscape in neural networks. The work also shows interesting negative results, notably that over-parametrized networks can have bad local minima. It clarified the notion of \"bad local minima\" and distinguishes between weakly global functions and non-weakly-global functions. These negative results are nice and interesting as they go again the widely accepted claim that over-parametrization makes the landscape always \"nicer\".\n\nA major drawback of this work is that it completely ignores the generalization properties of the corresponding neural network. It is not at all surprising that over-parametrization makes optimization simpler. The real point to explain about over-parametrization is why at the same time it does not hurt generalization. The paper admits that it does not consider the generalization properties. But as such it is not relevant to understanding learning properties of deep neural networks.\n\nThe absence of 'bad' local minima seems particularly irrelevant in the view of recent line of work on overparametrized neural networks, shoving that best generalization properties are achieved in the so-called interpolating regime where there exist many global minima of the loss landscape, all with zero training error, but only some of them leading to good generalization. The real question is to explain why the algorithm dynamics find those that generalize well and not the other one. The present work does not shed any light on this question. \n\nSo while I do appreciate some of the negative results I think overall this work does not constitute an important contribution to the current questions and I hence lean towards rejection. \n\n\nMinor comments: \n\nI am not really appreciating the analogy between illnesses of the landscape and the human body. This is maybe useful in a wide audience newspaper article, but I do not see its use here where readers are well aware of the underlying questions and concepts. Talking of “pharmacodynamics\" and related does not really seem of interest. \n\nThe article is full of misprints: last paragraph on page 1: has -> have, inima - minima \n\nI am also noting that the paper has 10 pages and hence according to the paper call higher standards should be applied in the review process. \n"
        }
    ]
}