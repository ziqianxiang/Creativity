{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper considers the robustness against adversarial attacks in learning of deep neural networks. Other than the usual Lp-distortions, the authors introduce novel adversarial distortions, JPEG, Fog, Gabor and Snow. Then, the authors propose the summary metric, unforeseen attack robustness (UAR), to measure the robustness against a given distortion. In experiments, the authors report UAR scores for existing and novel attacks and conclude that the existing defense against Lp attacks does not generalize to unforeseen attacks and the five attacks, L_\\infty, L_1, Elastic, Fog and Snow, offer greater diversity. The joint adversarial training over these five attacks is also investigated in experiments.\n\nThe four novel distortions defined in this paper are interesting and promising in practice. However, there are little theoretical discussions in this paper and most arguments look somewhat ad-hoc.\n\nAlthough the experiments are in fact extensive to some extent, since the results are only for specific datasets and models, it is difficult to tell how general the obtained observations are.\n\nSection 5 discusses the joint adversarial training by experiments. Are there any theoretical results on joint adversarial training related to the discussion here?\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "The paper investigates empirically to what extent adversarial training with respect to a certain set of adversaries (e.g. Projected Gradient Descent (PGD) with a maximum l_inf perturbation of epsilon) also provides robustness against types of adversaries (e.g. PGD with different l_p norm constraints). While this problem has been studied before in terms of robustness transferability between different l_p constrained adversaries, this paper proposes a few novel attacks (\"JPEG\", \"Fog\", \"Garbor\", \"Snow\") to be included in such analysis. It shows that, while robustness from/against l_p constrained adversaries transfers relatively well, this is not the case for some of the newly proposed adversaries. It therefore proposes a framework, including a new canon of adversaries (including in particular the \"Fog\" attack),  for a more broad assessment of empirical robustness against a variety of adversaries.\n\nThe paper contains a lot of material that should be interesting to browse through for any researcher working the area of adversarial machine learning. The number of experiments carried out and reported on is impressive. The purpose of broadly and thoroughly assessing empirical robustness against a variety of adversarial attacks is important. While the paper makes a convincing case for the importance, some of the concrete steps that it's trying to promote still seem preliminary to me. For instance, how complete is the set of adversaries in the proposed UAR framework? In the sense: isn't it to be expected that the set of sources considered in Figure 6 (b) will have to be extended, e.g. once a new adversaries has been discovered that has low correlation with any of the known adversaries? In that sense, testing robustness against unforeseen/unknown adversaries still appears to be an open problem, unless the space of adversaries is constrained and it is established that any unknown adversary is necessarily correlated with the known ones.\n\nOn the methodological side,  the calibration of distortion sizes (p.4) was not clear to me. In 1.: what exactly means \"comparable\"? Does 2.a (\"images which confuse humans\") involve surveys / user studies? And what does it mean in 2.b: \"reduce accuracy of adversarially trained models to below 25\"?\n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Summary:\nThis paper proposes to examine robustness against more unforeseen attacks (than Lp attacks) such as Elastic, Fog and Snow etc. The Unforeseen Attack Robustness (UAT) evaluation metric is further proposed to produce a consistent score over a range of test attacks and perturbations sizes. They also demonstrate the overfitting issue of joint adversarial training on 2 different norms of attacks.\n\n--------------\nMy concerns:\n1. It is hard to tell which type of robustness is evaluating in this paper: adversarial or common corruptions? Adversarial robustness often refer to robustness to small imperceptible adversarial perturbations, however, many of the perturbations illustrated in this paper are neither small nor imperceptible. I would expect none of existing models can survive any of those large perturbations. On the other hand, the new attacks proposed in this paper are basically common corruptions, which has been comprehensively studied in [1] on 15 types of corruptions including the proposed Elastic, fog and snow. I don't think adversarially perturb those common corruptions can contribute more for robustness evaluation. There are also other corruptions such as translations and rotations [2].\n\n2. The real contribution of this paper is not clear to me. What is the focus of this paper: the robustness of different DNNs, training strategies or defense techniques? Why only one type of defense (eg. adversarial training) was considered (why not a few latest works in adversarial training: [3][4])? My concern comes from this \"one-defense\" setting. Since all models are trained by one fixed defense, it is hard to draw any concrete conclusions on other defenses, or defense in general.  In the abstract, the authors claimed that \"These results underscore the need to evaluate and study robustness against unforeseen distortions\", which seems quite obvious. It is always beneficial to include more corruptions, but why the suggested Elastic, Fog, Snow in particular? \nSo far, defense techniques still suffer from low robustness to even small perturbations (the strongest defense, adversarial training, only has 43% - 65% robustness against PGD perturbation L_{\\infty} < 8/255). Then the question is, why evaluating against so many attacks is necessary and important, considering they even fail the easiest Lp corruptions?\nBy the way, in Table 1, why the ATA4 is 66.9, which seems too high, even higher than state-of-the-art robustness?\n\n3. The proposed \\epsilon_{min} and \\epsilon_{max} calibration is a bit confusing. \"The minimum distortion size \\epsilon_{min} is the largest \\epsilon for which the adversarial validation accuracy against an adversarially trained model is comparable to that of a model trained and evaluated on unattacked data.\" -- what this means, exactly? The proposed UAR score is also not clear: 1) what is the different between Acc and ATA? 2) \"ATA is the best adversarial accuracy on the validation set that can be achieved by adversarially training a specific architecture\" -- how this best accuracy was obtained?? 3) the validation set is also confusing, is it part of the training data held out for validation?\n\"To make UAR roughly comparable between distortions, we evaluate at \\epsilon increasing geometrically from \\epsilon_{min} to \\epsilon_{max} by factors of 2 and take the subset of \\epsilon whose ATA values have minimum `1-distance to the ATA values of the L1 attack at geometrically increasing \" -- I have no idea what the authors are referring to here.\n\n4. The presentation can be improved. For example, shorten the ticks used in those matrix plots (Figure 6, 7, etc). It can also benefit from a summary table somewhere about all those distortion types and sizes. It also helps if the authors can explain the \\epsilon_{min}, \\epsilon_{max} and UAR score with a more concrete example.\n\n[1] Benchmarking Neural Network Robustness to Common Corruptions and Surface Variations\n[2] The limitations of adversarial training and the blind-spot attack. ICLR 2019\n[3] Theoretically principled trade-off between robustness and accuracy. ICML, 2019.\n[4] On the Convergence and Robustness of Adversarial Training. ICML, 2019."
        }
    ]
}