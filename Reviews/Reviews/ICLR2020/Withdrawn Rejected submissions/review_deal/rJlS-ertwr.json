{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a method called InfluenceNet to deal with partial observations in reinforcement learning. The idea is to keep track of only a small amount of available information using a recurrent mechanism and attention - based on the previous work of Oliehoek et al (2012). The model is composed a RNN (tracking the past) and a FNN (current only) which jointly influence the value and policy functions. The model is tested on a simple traffic control task and four Atari video games.\n\nIt is hard to distill novelty out of the paper and it is unclear how the idea of \"influence-based abstraction\" helps in the evaluated cases, where the performance of the InfluenceNet is highly correlated with the FNN component. At best the proposed is a simple combination of FNN and RNN, and the RNN hardly does any job. The experiments are very inadequate to empirically confirm any hypotheses.\n\nThe reference list seems to ignore much of recent works in solving partial observations, e.g., memory-based methods.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper considers the RL problem of learning an agent's policy in a high dimensional and only partially observable environment that can be modelled as a partially observable Markov decision process (POMDP). The authors propose InfluenceNet, a model-free deep RL architecture. The main intuition is that in a POMDP the Markov property is not satisfied, but on the basis of influence-based abstraction the non-Markovian dependencies can be accounted for by conditioning on the history of a subset of the locally observable variables. InfluenceNet incorporates this idea by processing the current local observation separately with (1) a FNN and (2) a weighted linear combination of vectors representing particular regions in the input image with an RNN. The weighting of the linear combination in (2) is determined by an attention mechanism and the results of (1) and (2) are combined to determine value and action probabilities. Experimental results on a simple traffic control task and different Atari games support the effectiveness of the approach in determining the regions which contain relevant information. Further, the results indicate that (while improving the runtime performance) InfluenceNets performance is on par or even better in comparison to two simple baseline setups for problems in which the information can be reduced to a small subset of the input.\n\nOverall, the paper is marginally below the acceptance threshold, but I am willing to increase my score if some more clarification are provided and the results of the experiments are revisited and extended (see comments below). \n\nThe paper provides a methodological contribution by an extension of existing approaches from the field of POMDP methods and the application of an attention mechanism to address the problem of high dimensional input which I believe can be quite useful for the RL community. The submission is well written and easy to follow, the problem is well motivated and the concept of separating the Markovian and non-Markovian dependencies in the proposed architecture is explained in a reasonable way. Suitable experimental results support the architectual decisions and illustrate the benefits of the proposed method, but should be revisited with respect to the given remarks below. The paper includes reasonably complete information that will help to reproduce them and I think that the reproducibility of the results is good, even though I did not explicitly validate it. The authors provide the source code whose documentation could be improved (README, empty folder etc.).\n\n\nQuestions / clarifications / remarks:\n\nThe experiments should consider an architecture based on an LSTM in addition to the RNN as baseline for a recurrent network. Hausknecht & Stone (2015) and Sorokin et al. (2015), which are both cited in the paper, use LSTMs in their architectures and can be seen as related approaches and the current state of research for the problem at hand. The influence of the long-term memory component of an LSTM would be particularly interesting when considering the memory capability of an agent.\n\nThe results for Pong and flickering Pong with the RNN look suspiciously poor as if the agent would behave totally at random. The results should be double checked and if they are correct, please comment on them. Again, an approach based on a LSTM should be considered here, as it is known to work better than random on the problem of Pong. Check for example https://www.endtoend.ai/envs/gym/atari/pong/ and \"A3C LSTM\".\n\nFurthermore, it should be worked out where the increase in performance of InfluenceNet comes from compared to completely recurrent models. How much can be attributed to the separate FNN added to the architecture and how much to the attention mechanism?\n\nTable 1 shows the runtime performance of the individual methods. It is not clear how the d-patches were selected for InfluenceNet. Since it can be assumed that there are differences in runtime performance between a manual and automatic selection of d-patches, both variants should be listed.\n\n\nMinor comments:\n\nPage 2:     - typo: R(s_t, a_t) is defines the ... >> R(s_t, a_t) defines the ...\nPage 8:     - typo: InlfuenceNet >> InfluenceNet\nFigure 2:   - The shape and color of action a_C is not explained and does not correspond to the shape of action a_A; missing arrow from x_2 to the other state factors for t=3\nFigure 4:   - blank space in the second last sentence\nTable 1:    - One result for the Atari games is incorrectly highlighted bold \nTable 2:    - Multiple results are highlighted as being the best \n\n\nFinal remark:\n\nThe paper shows a very high agreement with the following (workshop) paper and should be considered as plagiarism if the authors do not match:\nMiguel Suau de Castro, Elena Congeduti, Rolf A.N. Starre, Aleksander Czechowski, and Frans A. Oliehoek. Influence-Based Abstraction in Deep Reinforcement Learning. In Proceedings of the AAMAS Workshop on Adaptive Learning Agents (ALA), May 2019."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes applying an attention mechanism to the input of a recurrent neural network such that the resulting architecture can learn to store only parts of the input which are actually relevant to predicting the future. This is motivated by the concept of a d-separating set, which serves as a sufficient set of observed variables to predict the influence of hidden variables. The idea is implemented as a soft spatial attention mechanism applied to the output of a CNN before it is input to an RNN. The architecture also includes a feedforward component to which the attention mechanism is not applied. The benefits of this architecture are explored in a simple but illustrative traffic domain where the light at an intersection is controlled using only local information. The architecture is also explored on several Atari games as well as a variant of pong which randomly hides certain observations to increase partial observability.\n\nThe motivation of learning d-separating sets so that the agent can focus on only the variables that can have a meaningful impact on the future is interesting and perhaps warrants further exploration. However, I find the method presented in this paper to be lacking in significant novelty or new insight and recommend rejection primarily for that reason. The method itself is only very loosely connected to the idea of d-separation sets as ultimately the whole thing is just trained by standard back-prop and there is no direct exploitation or learning of a graph structure. The method seems to me to be essentially a rebranding of spatial attention, the benefits of which are fairly well explored, but applied only to the recurrent part of the architecture which I don't consider enough of a contribution to publish at this venue.\n\nIn addition to this, the experiments provide limited insight or evidence of the effectiveness of the architecture. While the traffic example is reasonable, the Atari games tested (aside from perhaps flickering pong) have only fairly trivial partial observability (e.g. the direction an object is travelling cannot be determined from a single frame). This makes them a fairly poor domain for testing the proposed approach and does not seem in line with the proposed motivation of the paper.\n\nAlso, according to the appendix default hyper-parameters were used for everything except the additional parameters of InfluenceNet, which were hand-tuned, further limiting the significance of the experiments. In particular, I have trouble believing the RNN baseline couldn't be made to do well on flickering pong given a more favourable hyper-parameter setting. Also, was the recurrent architecture tested a regular RNN or a more sophisticated architecture like a GRU or LSTM? Given I would think one of that latter would be a more relevant baseline, as well as a more relevant setting to apply the proposed method, given ordinary RNNs are rarely used in practice.\n\nWhile I don't feel the paper presented here is ready for publication, the idea of studying of how we can isolate sufficient variables for representing the history is worthwhile and I hope the authors continue expanding on this direction."
        }
    ]
}