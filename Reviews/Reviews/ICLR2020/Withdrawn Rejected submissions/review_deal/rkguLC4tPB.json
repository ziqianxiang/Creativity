{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes the unknown-aware deep neural network (UDN), which can discover out-of-distribution samples for CNN classifiers. Experiments show that the proposed method has an improved rejection accuracy while maintaining a good classification accuracy on the test set. Three reviewers have split reviews. Reviewer #2 provides positive review for this work, while indicating that he is not an expert in image classification. Reviewer #1 agrees that the topic is interesting, yet the experiment is not so convincing, especially with limited and simple databases. Reviewer #3 shared the similar concern that the experiments are not sufficient. Further, R3 felt that the main idea is not well explained. The ACs concur these major concerns and agree that the paper can not be accepted at its current state.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper is about a novel method to detect unknown samples which are of a different class than the trained ones. The idea is to use an output subnet which use a fully connected layer and a binary tree which encode the product relationship instead of the sum currently used in state of the art method (particularly the softmax with low confidence). The binary tree is made of split nodes which are responsible to produce a probability distribution from the root to each leaf. The max path i.e. the path with the largest probabilities determines the class of the input and can be used to measure how confident the classifier is about the classification decision. Combine multiple subnets and the tool obtained is able to do complex predictions and maintain a good generalization performance. The method also uses an information theory based regularization which decrease the probability of having subnets with uniform probability distribution i.e. a large entropy. Experiments on CIFAR-10 and MNIST against CIFAR-100, SVHN show that the method has an improved rejection accuracy while maintaining a good classification accuracy on the test set.\n\nThe topic of the paper is interesting and the approach seems to be solid, however the experiments are not so convincing. They are limited on two very easy datasets and do not show if the method is able to scale when more difficult and more realistic amount of classes are considered (like in CIFAR-100, SVHN or maybe ImageNet). Given that a simple dataset like these require 9 hour of training, it is also not clear how much the method is able to scale computationally and if it is applicable realistically. Moreover the presentation could be improved as figure 1 and its section 2 are complex and not easy to follow at several points. Hence, I’m leaning towards rejecting this paper.\n\nIn particular:\n- It would be interesting to see experiments where the number of classes is higher than the ones in MNIST and CIFAR-10. CIFAR-100 and SVHN would be a good testbed for such case.\n- How is the complexity of the method and how does it scale with the number of training classes?\n- It is stated that the method bring a 25% percentage points in the accuracy of unknown rejection detection, however table 1 shows a large improvement only in the case of SVHN. Hence the claims seems a bit off compared to the measured data. Moreover, using only CIFAR-10 is insufficient to back up the claim.\n- Why the related work section is at the end of the paper? It confuses the reader and would be more useful to be put after the introduction section.\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nThis paper proposes the unknown-aware deep neural network (UDN), which can be used to discover out-of-distribution samples for neural network classifiers. Its main idea is to introduce PR subnets to model the product relationship instead of the dot product of regular networks, then it can avoid over-fitting. Experimental results demonstrate that UDN can discover unknown samples more precisely than several baselines.\n\nThe problem of learning with out-of-distribution samples is important for real-world applications. The results provided in this paper seem positive. However, I think the main idea is not well explained, and the experiments provided are not sufficient. It’s not clear why introducing this PR subnet forest can help the model avoid overfitting, and why this structure is more beneficial than other simple ensemble structures. I think it’s more helpful to provide additional insights or theoretical analysis. As for the experiments, the contents of images in CIFAR, SVHN, and MNIST are completely different, so under the given setting, the unknown samples are easy to detect. It’s more practical and convincing to split categories in each dataset into two parts to simulate unknowns.\nBelow are some detailed comments:\n-      Can you compare the performance of UDN with Bayesian neural networks, since BNNs are also popular methods to model uncertainty?\n-      How does the hyper-parameters for PR subnets affect the results? \n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper proposes a neural network architecture for image classification, which can more accurately recognize the unknown class that is not presented in the training data than the prior work. The key idea is to organize the features into a binary tree and use the product of probabilities along the paths to the leaf node to predict whether the test image has all the relevant features that should present in known classes. This proposed method is compared to multiple baselines and demonstrates superior results in image classification, especially for correctly predicting the unknown class.\n\nI am not an expert in image classification. Thus I cannot judge based on the novelty of this paper. I vote for acceptance solely based on the clear writing, technical soundness, thorough evaluation and good results. The intuition behind the method that missing a key feature of a particular class should significantly reduce the probability of assigning the input to this class is well captured by the product of probabilities along a path in a binary tree structure. In other words, the proposed method makes sense intuitively and is validated on several datasets. One small concern that I have is that a binary tree explicitly partitions the feature into separated groups. For example, if for a known class, f11, f12 and f16 are the three important features to be present. According to Figure 1, there is no path that connects all these three nodes. Will different partitioning of the features affect the accuracy of the classification, including the unknown class?\n\nTo improve the paper, it would be great to add one more experiment to examine the effect of the ensemble, for example, by varying the number of product relationship module. \n\n-------------------------Update after rebuttal-------------------------\nThanks for your detailed response and the additional experiments. The response addressed my questions and concerns. Thus I will keep my original recommendation of acceptance.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}