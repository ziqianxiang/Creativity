{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The work proposed Super-AND, which extends the Anchor Neighborhood Discovery model (Huang et al. 2019) and adopts additional losses including unification entropy and augmentation trying to provide a comprehensive approach to unsupervised representation learning. Experiments on image benchmarks show promising results.\n\nFrom the ablation study shown in Table 4 and the main results in Table 1, it seems the data augmentation part solely contributes almost all the improvement in Super-AND (86.4 for Super-AND w/o Aug v.s. 86.3 for AND). Even the unification entropy loss and the Sobel need the augmentation in order to shine. On the other hand, this can be seen as some form of multi-task learning [1]. Then it is interesting to see how other pretext tasks collaborate with AND to justify the choice of augmentation.\n\nOne crucial aspect of unsupervised embedding learning is how it scales with the data. That is, how the model performs when there is abundant of data. The AND model did not excel in this since it did not produce favorable results in the large-scale ImageNet evaluation comparing to simpler models, despite it demonstrated solid improvements in the small-scale settings. Simple tasks may indeed have advantages in the large-scale setting as shown in [2] that rotation and context models work really well when proper architectures are used. Thus, it would be interesting to see how Super-AND works in the large scale setting. But the authors did not provide any evaluation in large scale data sets.\n\nOther comments:\n- In the AND work, the authors showed that the smallest neighborhood (k=1) is the best choice. Here, the augmentation loss is added to introduce more \"positive\" signals into the model. Does this help to include larger neighborhood to facilitate faster learning?\n- The final classification is performed using weighted k-NN classifier, but the authors did not mention how the \\tau value is determined. Is the same \\tau value used for all pretext tasks? If it is the case, this can not be considered as a fair comparison since the best performance of different pretext tasks may require different \\tau values. Also, how sensitive is the model to \\tau? \n\n[1] C. Doersch and A. Zisserman. Multi-task self-supervised visual learning. ICCV 2017. \n[2] A. Kolesnikov, X. Zhai, and L. Beyer. Revisiting self-supervised visual representation learning. CVPR 2019."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a method named SUPER-AND to learn visual feature representations. AND (Anchor Neighborhood Discovery) combines sample specificity method and clustering method. Sample specificity method takes each sample as an independent class and tries to discriminate them from each other. Consequently, class-related semantics are missing. Clustering method learns a clear discriminative boundary for each class. However, it is hard because of the complexity of data points and solution space. The combination is proved can mitigate both disadvantages. SUPER-AND improves AND with data augmentation method by incorporating augmentation loss into the objective function. Experiments show that SUPER-AND has better performances than baselines. The paper has two novelties: \n•\tThe authors propose a UE-loss function which strengthens the attraction among similar data points. \n•\tThey introduce neighborhood relationship vector which measures the similarity of a data point to other data points in the same batch.\nThe weakness of this paper is: \n•\tThe paper is mostly a combination of previous works: an improvement of AND with augmented data.\n•\tSome experiments are not totally convinced. (a). The t-SNE visualization in Figure 3 only provides 3 classes from CIFAR-10; (b). Figure 5 only provides retrieval results of 4 classes (10 classes in total); (c). The ablation study is not complete. Specifically, there is no result from the raw model with only UE-loss which, however, is one novelty in this paper.\n•\tThe paper needs more polish. The related work section does not include sample specificity method. There are also some notation problems which are listed in the following minor errors section.\nQuestions to authors:\n1.\tWhy not directly use learned feature vector v to compute augmentation loss? The proposed relationship vector r is just a linear transformation of vector v. What is the motivation to do this transformation and why does it benefit? Why not use r in other loss functions?\n2.\tThe abstract says SUPER-AND is feature invariant against augmentation. Is there any evidence in experiments?\nMinor errors:\n•\tEq (2) seems to be r_i^j = \\frac{v_i m_j}{||v_i|| ||m_j||}\n•\tEq (10): no notation for N^c\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposed a so-called super-AND algorithm to learn useful representations in an unsupervised manner, which hopefully reduces the demand for training deep networks with a large amount of labeled samples. The main idea follows the anchor neighborhood discovery approach proposed by Huang et al., by first finding neighborhood for each sample, performing neighborhood selection (curriculum learning) and finally optimizing a unification entropy loss (and data augmentation loss) that distinguished one neighborhood pairs from another. \n\nThe novelty of the paper looks very limited considering its similarity with Huang et al’s paper. Besides, the writing and organization of the paper have a large space of improvement. In its current version, the discussion of basic concept and ideas look fragmented and incoherent. It is not easy to read through the paper to clearly capture the main theme of the paper. Many notations are hard to follow. For example, in defining the problem, authors mentioned that the bold p_i in equation (1) is defined as the probability of image being in its own class- what is the meaning of this? Then why p_i is a vector and the $j$th entry corresponds to one memory m_j?  Does each memory slot correspond to one class? If so, how do you update the memory? And if not, why the probability vector p has a dimension that is the same as the number of memory slots? This would look very confusing to the readers. Another example is that there are many different versions of the loss functions listed in Section~3,  like unification entropy loss and augmentation loss; are they both optimized and can authors clearly state their global loss function?\n\nThe motivation of the paper is to separate samples from different classes far away from each other and local anchor neighbors should have similar labels; however, from the embedding visualization presented in figure~(3), I feel that it is very similar to (or even a bit worse than) the original AND algorithm in terms of class separability. \nI hope that the authors can spend more efforts clarifying their ideas and make their writing coherent so that readers can have a better experience reading it. Also avoid using vague terms like “learning representations that are visually meaningful” without clearly elaborating on its meaning. \n"
        }
    ]
}