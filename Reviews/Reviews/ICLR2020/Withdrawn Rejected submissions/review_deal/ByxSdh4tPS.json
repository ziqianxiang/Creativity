{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper presents \"EnsembleNet\" a multihead neural network architecture where the multiple heads have the same architecture and are all (roughly) trying to predict the same thing.  The diversity of this \"ensemble\" comes from the random weight initialization.  The paper investigates and compares the ensemble loss structure and the co-distillation loss structure.  They make the interesting observation that for the ensembling loss structure, performance is better when one puts a negative weight on the loss of the ensemble (the mean of the multiple heads) while putting larger weights on the performance of the individual head losses. They also show that for the case of square loss, if the ensembling model is a simple average, then the ensembling loss and the co-distillation loss are equivalent. Finally, they present a general strategy for converting a network to an EnsembleNet.\n\nThey hyperparameter tune with both losses and find that the co-distillation loss is marginally better than the ensemble loss structure.  Overall gains over baseline seem moderate.  \n\nI don't think this paper is an accept.  It has a couple interesting insights, but they seem rather subtle... perhaps this would be of interest to people who are specifically researching these types of ensembles.\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "The authors proposed a new approach for the end-to-end training of ensembles of neural networks. The method consists of two steps. The first one is the construction of a multi-head architecture from a base neural network. The second step is training this model using a co-distillation loss function, where different heads are treated as elements of an ensemble.\nThe idea behind EnsembleNet is similar to the multi-head architecture, but instead of just simply multiply upper blocks to obtain several heads, the authors also proposed to shrink these heads. This idea is not significantly different from multi-head model. Also, the authors noticed that the proposed approach almost does not benefit from more than two heads..\n\n\nExperimental results are quite promising and quite broad in terms of the number of problems to which this method was applied. Nevertheless, it would be beneficial to compare the proposed method with other memory efficient techniques such as: e.g. Shake-Shake regularization [1], deep mutual learning approach [2] in which according the table 2 the co-distillation of two models help to boost the performance of each of them and other similar approaches.\n[1]  Gastaldi, Xavier. \"Shake-shake regularization.\" arXiv preprint arXiv:1705.07485 (2017).\n[2]  Ying  Zhang,  Tao  Xiang,  Timothy  M  Hospedales,  and  Huchuan  Lu.   Deep  mutual  learning.   In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,  pp. 4320â€“4328, 2018\n\nOverall, the paper is interesting but the ideas on which it is based are quite simple and not quite new. The experimental part is promising but should include comparison with other similar approaches. Another concern is scalability of this approach to more than two heads.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nThe paper proposes a variant of ensemble learning for deep networks, more specifically a multi-headed network trained by co-distillation. The authors propose some new defitions for ensemble loss, that is an ensembling loss and a co-distillation loss. The authors show that the proposed losses result in better performing ensembles, while not requiring more computational power.\n\nI would vote for rejecting this paper. The paper suffers from an important experimental drawback, in that it does not compare the proposed methods with baselines from the state of the art (other than single models). The authors state themselves in the paper that the proposed losses are different from the losses in the co-distillation papers, which consist of a combination of the two losses. Without a proper comparison to the state of the art, it is impossible to say if the paper provides a significant contribution or not. \n\nFurthermore, the methodological contributions appear to be rather incremental. In order to be publishable, they would need be justified by significant experimental proof, which is currently lacking.\n\nSection 4, paragraph 1, what does \"about three runs\" mean? It's either 3 runs or it's not three runs. If results are computed on more or less than three runs, it should be explicited.\n\nIn short, I would say that the paper is well written and presented, but I think it is also rather incremental and lacks more baselines to warrant a publication at ICLR or a similar conference.\n"
        }
    ]
}