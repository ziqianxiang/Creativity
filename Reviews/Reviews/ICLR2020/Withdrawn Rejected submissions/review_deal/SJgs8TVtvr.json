{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposes a VAE with a mixture-of-experts decoder for clustering and generation of high-dimensional data. Overall, the reviewers found the paper well-written and structured , but in post rebuttal discussion questioned the overall importance and interest of the work to the community.  This is genuinely a borderline submission. However, the calibrated average score currently falls below the acceptance threshold, so Iâ€™m recommending rejection, but strongly encouraging the authors to continue the work, better motivating the importance of the work, and resubmitting.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "\nSummary:\n\nThe paper proposes to expand the VAE architecture with a\nmixture-of-experts latent representation, with a\nmixture-component-specific decoder that can specialize in a specific\ncluster.  Importantly, the method can take advantage of a similarity\nmatrix to help with the clustering.\n\nOverall, I recommend a weak accept.  The method seems reasonable, and\nthe paper is well-written, but the results are only marginally better\nthan other methods, and there are several weaknesses with the proposed\narchitecture and experimental setup.\n\nPositives:\n\n* The idea of a more expressive variational distribution seems good,\n  although it is not novel.\n\n* The ability to have multiple decoder networks seems reasonable.\n\n* The ability to incorporate domain knowledge (in the form of a\n  similarity matrix S) is a plus.\n\n* The experiments are thorough, although the method is generally only\n  slightly better than competing methods.\n\nNegatives:\n\n* It's not clear if the similarity matrix S is already solving the\n  clustering problem - in which case, why do we need the rest of the\n  model?  For example, in your experiments you often used UMAP to\n  cluster data.  How does using UMAP by itself work?  (Along these\n  lines, it was not clear if your GMM experiments clustered data in\n  the original space, or in the UMAP'd space - please clarify this).\n  A good ablation would be to somehow remove the S matrix, to see if\n  the model can accurately cluster samples.\n\n* There is little variance in the generated samples.  \n\n* There is not a one-to-one mapping of clusters to labels, so it is\n  hard to use this method to generate a specific type of data (for\n  example, it is hard to generate a specific digit).  This is a big\n  difference from, say, a conditional sampler as learned by a GAN.\n  This also arises in Fig. 3, where it is clear that latent cluster\n  assignments do not match human-interpretable cluster assignments.  I\n  suppose this is to be expected, but taken with the previous point\n  (little variance in generated samples) I think it seriously weakens\n  the paper's claim that this is an \"accurate an efficient data\n  generation method.\"\n\n* The method does not do well when the number of clusters is large.\n  Regular GMMs seem to outperform it.\n\n* I felt that this paper made excessive use of the appendix.  The\n  paper is not self-contained enough, effectively violating the length\n  restrictions.  Please make an effort to move key results back in to\n  the main body of the paper.\n\n\nExperiments to run:\n\nAn ablation regarding the similarity matrix S.\n\nClarification of whether GMM experiments are run in data-space, or\nUMAP'd space.\n\nMIXAE features prominently in your related works, but is not compared\nto in your experiments.  It sounds like a natural comparison.  Please\nrun this experiment, or explain why it is not a comparable method.\n\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors present an extension of variational autoencoders (VAEs), where Gaussian distribution of the latent variable is replaced by a mixture of Gaussians. The approach can be used for clustering and generation. The authors carry out experiments to evaluate the performance of the method in these tasks and compare it to competing methods.\n\nThe paper is well written and easy to read and understand. Specialized related work is discussed. I find the extension of VAEs to GMMs interesting for the ICLR community, although it is somewhat straight forward in terms of its technical difficulty. However, the technical novelty together with the fine empirical evaluation are just good enough for ICLR, in my opinion.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The proposed method of mixture-of-experts variational autoencoders\nis valuable and insightful.\nOn the other hand the work could be improved and clarified at some points:\n\n- in the abstract it is claimed that the method works for high-dimensional data.However, it should be better explained why this is the case. The method is largely based on density estimation with a mixture of Gaussians which is known to have limitations in higher dimensions (see e.g. classical textbooks like Bishop 1995)\n\n- the similarity matrix and the similarity values should be carefully defined. Is there also an underlying similarity function assumed?\n\n- a main shortcoming is that there is no discussion or experimental comparison with methods like spectral clustering and kernel spectral clustering. Given that the paper and the proposed method relates to similarity-based representations it would be important to know how it compares to such methods. Though e.g. in Table 1 the authors compare with about 10 other methods it would be more relevant that among some of these would have been spectral clustering and kernel spectral clustering, because of the similarity-based representations.\n\n- in section 4.1 the MNIST data are taken with k=10. Though it is nicely explained and illustrated on this data set, it is possibly somewhat misleading as an example. The reason is that this is a classification problem with 10 classes, therefore the choice k=10 is obvious. It would be more important to consider benchmark problems for clustering, instead of classification, for which the choice of k is also an important model selection issue and for which k is unknown (how should k be selected then?).\n\n- is each cluster always be assumed to be a Gaussian (which seems to be a strong assumption in general, and possibly not always realistic)? Could other components be used in the mixture?"
        }
    ]
}