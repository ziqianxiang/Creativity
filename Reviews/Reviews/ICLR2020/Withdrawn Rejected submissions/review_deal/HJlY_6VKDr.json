{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper formalizes the concept of buffer zones, and proposes a defense method based on a combination of deep neural networks and simple image transformations. The authors argue that the proposed method based on buffer zones is robust against state-of-the-art black box attacks methods.This paper, however, falls short of (1) unjustified claims (e.g., buffer zones are widened when the models are diverse); (2) incomplete literature survey and related work; (3) similar ideas are well-known in the literature, (4) unfair experimental evaluations and many others. Even after author response, it still does not gather support from the reviewers. Thus I recommend reject.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary: This paper proposes the concept of buffer zones and suggests to use unanimous voting as a way to induce such buffer zones. To widen the buffer zone, they further propose to diversity the model. They then proposed a new metric for measuring defense and demonstrated that their method is effective.\n\nDecision: Weak Reject. This paper is fairly intuitive, but I am not sure about the fairness of the comparisons in the paper, and the level of rigor of the experiments.\n\nI think the conjecture that buffer zones are widened when the models are diverse deserve to be empirically tested. It is not clear to me a prior how exactly the buffer zones widen (even though the belief that they widen is intuitively appealing). I think one way to potentially characterize the buffer zone is by actually performing white-box attack experiments comparing:\nWhite-box attack vulnerability of unanimous voting vanilla models\nWhite-box attack vulnerability of unanimous voting “diversified” models\nI would also encourage the authors to think creatively about other ways to back up the the buffer zone claims put forth in the paper.\n\nI am also a little puzzled with the authors’ choice of the diversification procedure. The procedure c(x) = Ax + b will only linearly transform each column of the image, but not each row. This design choice feels rather ad hoc. Why did the authors settle on Ax + b in particular? Why not xA + b? Or AxC + b?\n\nRegarding the fairness of the experiments, do the models that the authors compare against have the luxury of returning a “Undecided” label? If not, then the problem formulation is fundamentally different, and I do not think the comparisons are necessarily fair. Are there any papers out there that also allow for an “Undecided” label? If so, they should be the baselines that one compares against. I have a rather hard time believing that this is the first paper to try unanimous voting across an ensemble.\n\nI am generally inclined to switch to weak accept so long as the other reviewers are willing to accept that the experiments are sufficient and the comparisons are fair. I am not opposed to the new problem setting, since I think the setting makes sense. I just want to know that the paper is doing due diligence regarding related work in this setting."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "First of all, I think the authors do not do enough literature research on the topic of adversarial examples:\n\n1. In Sec 2., the authors only mention the FGSM in White-box Attacks. It is widely accepted that when evaluating the white-box attacks, you should at least test PGD attacks and/or C&W attacks.\n\n2. When you try to propose an adversarial defense, you should report the performance under white-box adaptive attacks. Only claiming effectiveness under black-box attacks is not informative or convincing.\n\nThe experiment results are also weird. For example, in Figure 8, why the Vanilla clean accuracy on CIFAR-10 is only 88.35%? Besides, the clean accuracy on CIFAR-10 of 2-Networks Buzz is 75%, 8-Networks Buzz is 60%. This clean performance is not acceptable, no matter how robust is the model."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a defense against black box adversarial attack. The authors train an ensemble of deep networks, and output a null label when the ensemble disagree. Success of adversarial attack is defined as the ensemble outputs an incorrect label that is not null. The authors experimentally show improved robustness to adversarial attack. \n\nThe idea is itself new, but very similar ideas are well known in the literature, and it is difficult to conclude that the proposed approach is superior. Several examples are:\n\nDefense by majority vote with ensembles has appeared several times in the literature (e.g. Pang et al 2019). The pro is that this paper proposes a novel way to create an ensemble by applying random linear transformations and rescaling to the input. But it is not clear this is superior compared to existing methods. \n\nRandomized smoothing (Cohen et al, 2019) guarantees smoothness of the classifier (and thus robustness to perturbation attack under certain norms). Note that randomized smoothing  provides certified guarantee against a stronger attack model (white box); it also guarantees the size of the margin (or buffer as the authors call it). The intuition of this paper is based on similar ideas so it seems necessary to at least compare with randomized smoothing. \n\nOutputting a null label is a major workhorse of adversarial defense. For example, previous work use a generative model to detect out of distribution samples; use a calibrated classifier to output null when low confidence. \n\nBecause the idea is only mildly interesting, good experimental support becomes crucial. However, I think there are several short-comings with the experiments:\n\nThe experiment contains only one (fairly old) attack method. Several recent alternatives such as SimBA (Guo et al 2019) can make the experiments more convincing. \n\nThe architecture is no longer the same for the target model (which is an ensemble with an additional random transformations) and surrogate model. It is unclear if the improvement is simply because of the difference in architecture. \n\nThe comparison to baselines seem unfair because it seems that the compared baselines do not have the option of outputting the null label. For example, a simple baseline of randomized smoothing + output the null label if the logit scores are below a threshold can make the story much stronger.\n\t\nMinor comments:\n\nSeveral suggestions on writing: the introduction contains much technical detail and even experimental results, and these are repeated again in later sections. The experimental section has many minor implementation details that could go into the appendix. \n"
        }
    ]
}