{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "\n\n======== Summary ======== \n \nThe paper proposes a method to decrease the computational complexity of training a student model in a knowledge distillation scenario. \nIt has been shown, in several works, that a student model, trained using knowledge distillation objective, can achieve a better generalization error than the same model trained in isolation and directly on the training data e.g. [Furlanello et al. 2018]. \nHowever, the standard process of knowledge distillation requires two sequential training of the teacher model(s) and then the student model using the teacher’s output(s).\nThe paper adopts a training process similar to SnapShot Distillation (SD) [Yang et al., 2018]. Additionally, this work replaces the *per-sample* teacher labels to a proposed “per-class” label. The per-class label of each class is obtained as the average of the per-sample teacher outputs of that class. \nThe paper tests the idea on several tasks, datasets, as well as architectures and demonstrates improvements on top of training the model with knowledge distillation. \n \n \n======== Strengths and Weaknesses ======== \n \n+ Knowledge distillation is of wide interest.\n+ The experimental effort is laudably comprehensive: For image classification, multiple recent architectures are used on the two datasets of CIFAR100 and ImageNet. For transfer learning, 4 different target datasets are used. The method is also tested on a NLP task.\n \n \nIn page 2, the main contributions of the paper are summarized as 1) being the first method to go beyond singular samples for knowledge distillation 2) developing a computationally-efficient distillation framework that brings significant improvements.\n \n- Regarding the first contribution, the following closely-related paper is not cited. It goes beyond knowledge distillation of isolated exemplars by considering multiple samples and distilling the mutual relations among them to the student network.\n[Park et al. “Relational Knowledge Distillation”, 2019]\nLess similar but still capturing the distribution of teacher outputs are the following works:\n[Xu et al. Training Student Networks for Acceleration with Conditional Adversarial Networks 2018]\n[Wang et al. “Adversarial Learning of Portable Student Networks”, 2018]\n \n- Regarding the second contribution, I believe SnapShot Distillation (SD)  [Yang et al., 2018] already proposes a fast and effective method. \n \n- that being said, the idea of averaging the teacher’s output is different from those works.\n \nPoints regarding experimental setup:\n- Since the performance of different methods are quite close (especially for SD vs DUPS+) , it would have been more conclusive to have multiple runs and report mean and std over those runs for all the methods (at least for CIFAR100 which is smaller and more manageable).\n \n- regarding the class-level teacher labels, both SD and LS are important immediate baselines and as such should be included in all tables. Table 3,4,5 miss SD results.\n \n- Moreover, the following baselines should also be included in the tables for a proper comparison: \n1) standard KD, since the proposed method is supposed to make the training stage of the standard KD faster while maintaining the performance \n2) [Pereyra et al, “Regularizing neural networks by penalizing confident output distributions” 2017] can be seen as a generalization of label smoothing and thereby is a relevant baseline.\n \n- How are the hyperparameters optimized for different algorithms? Was the same budget used for all baselines? How about the search space and intervals? On what metric and data were the hyperparameters optimized?\n \n- \\lambda is mentioned as the hyperparameter for DUPS only while label smoothing can benefit from an optimal weighting as well.\n \nAlso on a more general note:\n- The paper lacks a discussion regarding why removing the sample-level “dark knowledge” and only using the class similarity/confusion (as learned by a teacher) should work better than the standard knowledge distillation.\n \n \n======== Final Decision ======== \n \nKnowledge distillation is a very popular method both in the literature as well as in the real-world applications. Thus, improving its training time is an interesting research direction which can make the paper attractive. The “weak reject” rating is mainly based on the limited novelty due to the large overlap with prior works.\n \n \n======== Minor points ======== \n \nHere, I list some minor points which I hope the authors find useful when updating their manuscript:\n \n- Abstract: the paper’s proposal is stated as “we present a novel [...] in one generation”. This could benefit from a rephrasing such that an intuition or the high-level idea is better conveyed. \nP.3: section 3.1 has the f function mapping input samples to points in the label space. However, equation 1 assumes f’s output is a probability distribution, to be used in the cross-entropy and knowledge-distillation loss functions. In the end of p.4, f’s output is assumed to be the logits.\nP.4: definition of Y_{y_i} was already given on top of the page but is repeated in the middle.\nP.4: “we define f...] to be zero”: what does “zero” mean here? For instance, in the case of the categorical output distribution, shouldn’t y elements sum to one?\nP.5: while the text suggests that a random subset of samples for each class is used to determine the “teacher” label for that class, the algorithm in page 5 suggests all samples  are used. Which is the case?\nP.9: on which dataset, figure 1 and 2 are produced?\n \nfinally there are some typo including the following:\n\nAbstract: “average improvement of 1%-2% on various tasks” -> “average accuracy improvement of 1%-2% on various image classification tasks”\nP.1: “Student models, with the availability of softened output” -> For classification tasks, student models, with …\nP.1: “It is ad meaningful objective of”: doesn’t read correctly.\nP.2: “include ImageNet” -> including ImageNet\nP.2: “grammian matrix” -> Gramian matrix\nP.3: “image classification objects”: rephrase\nP.3: “In this paper We” -> , we\nP.3: “D ∈ XxY” -> D ⊂ XxY\nP.4: “random select” -> randomly\nP.4: “our method do not” -> does\nP.4: “the all samples” -> all samples\n \n======== Points of improvements ======== \nI think one way to improve the work is to first isolate the novelty of the paper with regards to all the related works and then accordingly divert the discussions in the paper to 1) conceptually motivate those novelties and more importantly 2) have the experiments highlight the empirical improvements brought in by these novelties.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes to address the extra computational cost of training with knowledge distillation, which is typically done in two stages (first training the teacher model, and then training the student model based on the teacher's predictions). More concretely, this paper builds on the recently proposed Snapshot Distillation (SD; Yang et al., 2018) technique of distilling the knowledge of the same model, albeit obtained from earlier stages of the training process, hence eliminating the need for a separately trained teacher model.\n\nThe key technical contribution is to utilise the predictions of the earlier model (which here functions as the teacher model) from multiple related training instances (i.e. peer samples), rather than only at a single data point. This takes the form of the average probability distribution (i.e. the whole softmax output) assigned by the earlier model to the peer samples that share the same label as the current training instance, and use this as the target teacher probability distribution. Experiments on image classification, out-of-domain image classification, and language modelling demonstrates improvements over the baselines.\n\nOverall, while the proposed approach is an interesting one, I have some serious concerns regarding the presentation and experiments, as listed below. I am therefore recommending a \"Weak Reject\" ahead of the authors' response.\n\nConcerns:\n1. I am not convinced about the strength of the baseline models, especially in the language modelling experiments. The reported perplexities in Table 5 (the best one was 85.7 perplexity) are generally much worse than various published work, such as the AWD-LSTM (Merity et al., 2018) and a well-tuned LSTM (Melis et al., 2018), both of which achieved <60 perplexity on Penn Treebank test set. This makes it unclear whether the improvements come from the DUPS approach, or simply from the use of weaker (e.g. under-regularised) baselines. Rather than just reporting the authors' own implementation, I would suggest adding the DUPS technique on top of the current state of the art models to see whether DUPS offers complementary gains on top of strong baselines.\n\n2. While I understand how DUPS works from the equations, the paper offers very little explanation or intuition about why DUPS works better than snapshot distillation. Why does DUPS result in a better teacher model than using snapshot distillation? Section 3.4 mentions that one of the weaknesses of snapshot distillation is that it relies on a cyclic learning rate, but it is not explained how using DUPS can address this problem. More confusingly, the paper also ends up adding the same cyclic learning rate to the DUPS model (called \"DUPS+\" on Table 1), even though the cyclic learning rate was claimed to be problematic in the first place. In short, it is not very clear: (i) what problem this paper is trying to solve, and (ii) why the proposed DUPS solution is a good idea to solve this problem.\n\n3. Related to point 2 above, the paper is not very self-contained. Some of the ideas are hard to follow for readers who are not very familiar with snapshot distillation (Yang et al., 2018).\n\n4. The analysis presented at the Discussions section (4.4) also fails to adequately explain why DUPS is beneficial compared to the baselines.\n\nMore minor comments:\n1. The use of the term \"logits\" is very confusing here. Typically \"logits\" refers to the unscaled tensor before the softmax activation is applied. It seems that what the paper means here is not really the \"logits\", but rather the output of the softmax function, which forms a valid probability distribution (distillation minimises the cross-entropy between the teacher and the student's probability distributions; both of which are obtained after applying a softmax activation to the logits).\n\n2. In Section 3.1, it is mentioned that \"[The KD loss] is typically computed by cross entropy ... or Kullback Leibler divergence ... between the logits produced by the teacher model and the student model\". This dichotomy is quite misleading, since minimising the cross entropy is equivalent to minimising the KL divergence (up to a constant). \n\n3. In section 4.1 (Tasks and Datasets), the task \"Natural Language Processing\" should be referred to as \"Language Modelling\", as NLP is much broader than just language modelling.\n\n4. This paper can benefit from carefuly copy-editing to improve clarity. Some examples:\n\na. In Section 3.3, \"we random select n peer samples ...\" should be \"we randomly select\".\n\nb. In Section 3.4, \"are a random subset of the all samples ...\" should be \"are a random subset of all the samples\".\n\nc. In Section 4.1, \"... the popular subset ILSVRC2012 which containing ...\" should be \"which contains\".\n\nd. etc.\n\nReferences:\nGabor Melis, Chris Dyer, and Phil Blunsom. On the State of the Art of Evaluation in Neural Language Models. In Proc. of ICLR 2018.\n\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM Language Models. In Proc. of ICLR 2018.\n\nChenglin Yang, Lingxi Xie, Chi Su, and Alan L. Yuille. Snapshot Distillation: Teacher-Student Optimization in One Generation. In Proc. of CVPR 2019.\n\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This article mainly proposes a scheme for improving the effectiveness of knowledge distillation, namely, Distillation by Utilizing Peer Samples (DUPS) in one generation, which improves the effective distillation of knowledge while ensuring that the additional cost of training is not significantly increased. And combined with three parts of the experiment to verify the credibility of the proposed solution. I think the main contribution points are as follows:1) soft the label in the knowledge distillation with the help of the teacher model, thereby improving the effectiveness of knowledge transfer. 2) use the dataset in the process of knowledge distillation instead of a single sample.\n\nOn the whole, I give the opinion of rejection, the main reasons are as follows:\n1) Although the method proposed in this paper (DUPS) has improved the effectiveness of knowledge distillation, such as one to two percentage points in image classification tasks, the extent of this improvement has not reached a satisfactory level.\n2) In the part of the label propagation in the paper, I think that the work essentially uses the teacher model to soften the label. However, it seems that the author has explained this part as one of the focuses of his contribution. In fact, this part of the innovation does not very strong.\n3) In the classification task, the method proposed in the paper is not satisfactory on the DenseNet100 model, and there is no effect of some previous methods. I have not seen the authors in this paper to be very convincing explanation about this phenomenon. \n4) I think the authors did three parts of the experiment mainly to verify that his method is very generalized effective, however, I have not seen a more in-depth analysis of its own method through more detailed comparison experiments to compare that the part of the label propagation process and the use of peer examples which are more obvious to the model performance improvement.\n"
        }
    ]
}