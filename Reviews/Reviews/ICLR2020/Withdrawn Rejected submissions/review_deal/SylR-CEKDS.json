{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors explore different ways to generate questions about the current state of a “Battleship” game. Overall the reviewers feel that the problem setting is interesting, and the program generation part is also interesting. However, the proposed approach is evaluated in tangential tasks rather than learning to generate question to achieve the goal. Improving this part is essential to improve the quality of the work. \n\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "[Summary] \nThis paper studies a very interesting problem - if machines can learn to ask the right questions to address or gain crucial information about tasks. I believe this is a very important yet under-explored problem. Specifically, this paper considers a setting where a neural network model is trained to ask questions by predicting a formal program. The network consists of a CNN encoder which encodes a partially observable state and a Transformer decoder that generates a program as a sequence of tokens. The experiments on a Battleship task show some promising results. \n\nSignificance: are the results significant? 3/5\nNovelty: are the problems or approaches novel? 3/5\nEvaluation: are claims well-supported by theoretical analysis or experimental results? 4/5\nClarity: is the paper well-organized and clearly written? 4.5/5\n\n[Strengths]\n\n*motivation*\nThe motivation for investigating machines' ability to generating questions to gain important information is convincing.\n\n*novelty*\nThe idea of utilizing learning models for generating questions and modeling human-generated questions intuitive and convincing. This paper presents an effective way to implement this idea.\n\n*technical contribution*\nLeveraging a context-free grammar (CFG) and reinforcement learning (RL) for question generation seems effective especially when it comes to generating unique and novel questions.\n\n*clarity*\nThe overall writing is clear and the authors utilize figures well to illustrate the ideas. Figure 1 illustrates the Battleship task and Figure 2 presents a clear overview of the proposed framework.\n\n*ablation study*\nAblation studies are comprehensive. The proposed framework consists of multiple components. The provided ablation studies (Table 2 and Table 4) help analyze the effectiveness of each of them.\n\n*experimental results*\n- The presentations of the results are clear. The conclusions are fairly convincing: \n- Experiment 1: learning to generate programmatic questions encourages learning rules for composition\n- Experiment 2: it is possible to capture the human-generated questions distribution as a conditioned language model.\n- Experiment 3: CFG+RL can produce more diverse and novel questions compared to supervised learning or RL.\n\n[Weaknesses]\n\n*novelty & contribution*\nOverall, I do not find enough novelty from any aspects while the overall effort of this paper is appreciated. \n- The problem is not entirely novel as [1-3] have explored asking questions with neural networks learning to generate programmatic questions.\n- The proposed framework leverages a CFG and learns with RL, while the former (program synthesis with a CFG) has been studied in [4-5] and the latter (program synthesis using RL) has been discussed in [4, 6-7].\n- Many setups considered in this paper have been explored in several neural program synthesis papers [8-12], which are neglected from this paper.\n- The conclusions presented in the experiment section are more or less expected (i.e. other works have presented similar results on slightly different problems).\n\n*oversell the motivation*\nI believe the authors oversell the key motivation (i.e. to enable learning agents to ask questions) a little bit. To be more specific, it would be more interesting if an agent is allowed to ask a question, take actions based on the answer, and then ask the next question. However, this paper investigates the case where a significant amount of human-generated questions are given, which could limit the learning in my opinion. Also, it is not clear to me how the setup evaluated in this paper can be extended to a setting that allows an agent to iteratively ask questions.\n\n*experiment setup*\nDo the models learning using RL get the reward based on just the ground truth questions (a sequence of tokens) or the execution results? \n\n*format*\nThe title & reference format looks wrong\n\n*reference*\n[1] Question Asking as Program Generation in NIPS 2017\n[2] Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding in NeurIPS 2018\n[3] The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision in ICLR 2019\n[4] Leveraging grammar and reinforcement learning for neural program synthesis in ICLR 2018\n[5] Learning a Meta-Solver for Syntax-Guided Program Synthesis in ICLR 2019\n[6] Neural Scene De-rendering in CVPR 2017\n[7] Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning\n[8] RobustFill: Neural Program Learning under Noisy I/O in ICML 2017\n[9] Execution-Guided Neural Program Synthesis in ICLR 2019\n[10] Neural Program Synthesis from Diverse Demonstration Videos  in ICML 2018\n[11] Learning to Describe Scenes with Programs in ICLR 2019\n[12] Learning to Infer and Execute 3D Shape Programs in ICLR 2019"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "The paper uses a deep neural network architecture (CNN + Transformer) to model logical translations of questions in the form of programs. The experimental setup uses the \"battleship\" game scenario, which is an interesting domain for questions because of the inherent partial observability present in the game. The paper is clearly written and well-presented.\n\nThe length of the submission is 9 pages in content. The call for papers states that \"Reviewers will be instructed to apply a higher standard to papers in excess of 8 pages.\" Given the major reservation I have regarding the current version (see below), and the need to apply a higher standard, it is hard for me to recommend acceptance in its current form.\n\nThe major reservation I have is that the paper currently sets up the reader to expect the data to contain real, human-generated questions. The abstract mentions \"human question asking\", talks about predicting \"which questions humans are likely to ask in unconstrained settings\"; on p.2 the paper states that \"the model can be trained from human demonstrations of good questions\"; and so on. Hence it came as a surprise to learn on p.6 that collecting human questions and translating them into programs is too laborious, so an automatic system was used to generate the questions instead. This isn't necessarily a killer for the paper, but it does require more justification, and more sign-posting early in the paper.\n\nOn p.3 there is some attempt at justifying the decision to use a simulator, which is that an existing paper has shown that it captures the full range of questions that people ask. I feel that this justification needs expanding.\n\nAnother issue with the paper is that it heavily builds on the existing work by Rothe et al., in fact so much so that it's difficult to fully appreciate the current paper without reading that existing work (which I haven't done, just for full disclosure). In fact, a summary of the current paper would be that it takes the existing work of Rothe et al., which is a rule-based question-generation system, and it \"neuralizes\" it by replacing the rules with a neural architecture.\n\nThat said, I think there is still a lot to commend in this work: the setting is an interesting one for question generation, the motivation for using programs is well-made, and the work appears technically sound.\n\nSome more minor comments\n--\n\nI wonder about the log-likelihood numbers on the synthetic questions. Doesn't this just show that a neural network can effectively reverse-engineer a synthetic grammar? I'm not sure how interesting that is. I also wonder about the decision to filter questions that score poorly according to the generative model used for training the network - doesn't this just add additional bias in favour of the model being evaluated?\n\nA similar comment applies to the decision to exclude ungrammatical questions from the uniqueness metric. I assume \"ungrammatical\" here means ungrammatical according to the pre-defined grammar. But that just makes it easier for the grammar-based system to perform well on this particular metric, no?\n\nI don't think EIG is defined anywhere in the paper.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors explore different ways to generate questions about the current state of a “Battleship” game. To do this, they introduce a neural network architecture with a convolutional encoder and a Transformer-based decoder and consider both supervised and reinforcement-learned training approaches. They evaluate the introduced methods in three different ways that demonstrate basic classification and generation abilities of the model.\n\nWhile the paper is generally well written and clear, I noticed quite a few grammar mistakes and typos. I expect these can be fixed in the final version.\n\n\nIn my opinion this work has several limitations and I don’t think it is a strong enough contribution to be interesting to the ICLR audience. Thus I am leaning towards rejection.\n\n\nWhile the problem of question generation is quite interesting, this work is limited in several ways. Firstly the authors limit the domain of question answering severely by just considering questions about “Battleship” that can be expressed in a lisp-like language. They do not attempt to generate questions that are oriented towards a more complex goal (like winning the game), but instead use three tasks that are quite limited:\n\n- First they show the model works well on very simple toy tasks; in particular the tasks are 1. Identifying the colour with the least visible tiles, 2. Identifying the ship with a missing tile, and 3. Identifying the colour of the ship with both a missing tile and the least number of visible tiles. These are extremely simple classification problems and no real question has to be generated. While this is a good sanity check for the model, it is not an interesting result.\n- In a second task they try to show that the model can capture the distribution of human authored questions. However, they do not use human-authored questions but instead use importance sampling using a heuristic method from Rothe et al. My skepticism of this experiment comes from the fact that  the “decoderless model” that does not take into account the board state performs similar to models that do (and even outperforms the model with LSTM decoder). I am not sure how meaningful this task is.\n- Lastly they show that using REINFORCE with a tweaked reward (a version of the energy function from Rothe et al.) produces new kinds of questions that have not seen before in the training data. This is not surprising and also not very meaningful without either an application or a thorough qualitative analysis of the generated questions.\n\nSo while extending the work of Rothe et al. with neural architectures seems like an interesting endeavour, the tasks the authors use in the paper are severely limited and do not demonstrate interesting behaviour.\n\n\nI believe this is not in scope for changes to this paper, but in general it could be interesting to evaluate the question generation in an end-to-end setup, similar to the reformulation framework used by Nogueira and Cho in “Task-Oriented Query Reformulation with Reinforcement Learning” and Buck et al. in “Ask the Right Questions: Active Question Reformulation with Reinforcement Learning”.\n\nNit: The colours chosen for the diagrams could be improved. In particular purple is quite hard to distinguish from the dark gray.\n"
        }
    ]
}