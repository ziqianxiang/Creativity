{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary: \nThis paper points out three major issues for the current differentiable neural architecture search (DARTS) algorithms: 1) the used bi-level optimization relies on a 2nd order approximation and thus is difficult to find the optimal solution; 2) the multicollinearity of correlated operations may not represent their real importance during architecture search; 3) the discrepancy between the complexity of the search stage architecture and the finally selected architecture. To address such three issues, the paper suggests one-level optimization with strategies of grouping similar operations in the early optimization, using a progressive pruning approach as well as a gradient confusion. \n\nStrength:\nIt is interesting that the paper studies some major drawbacks of the existing DARTS algorithms.\nThe techniques of progressive pruning approach, grouping similar operations as well as a gradient confusion are promising to overcome the bottlenecks. \nThe proposed method achieves promising performances on CIFAR-10, CIFAR-100 and ImageNet. \n\nWeakness:\nThe motivation of going for one-level optimization is not new. The original DARTS paper has ever studied the possibility of the one-level optimization while it may have some failures. Besides, there exist one work [a] sharing the same motivation. So I am concerned that the paper seems to miss this important related work. \n[a] Stamoulis et al., Single-path NAS: Designing hardware-efficient convnets in less than 4 hours. 2019.\n\nThe used strategies are straightforward and heuristic. While they bring some good results, I cannot see significant technical and theoretical contributions. On the other side, such techniques can be used for two-level optimization. So it would be better if the paper can show some evaluations on the original two-level optimization using these strategies. \n\nThe grouping strategy looks very greedy for the early search. It starts from the group selection based on representative instances like 3*3 separable convolutions, 3*3 max pooling and 3*3 dilated convolution. If the 3*3 separable convolution is finally selected, it does not mean this kind of operations is optimal, because some other scaled operations like the 5*5 dilated convolution might be better than 3*3 dilated convolution.  \n\nIn section 3.2, the paper presents the correlation of the learned \\alpha and the stand-alone model performance and I see a huge improvement of your method w.r.t. the origin DARTS (0.2 in DARTS and 0.91 in this paper). However, the evaluation method seems to be unclear to me and what do you mean by ‘replace the selected operation in the first edge of the first cell for the final architecture with all the other candidate operations in the first stage of StacNAS’’. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper provides a solution for stable and consistent optimization of a differentiable neural architecture search method (i.e. DARTS). Specifically, this paper introduces a grouped variable pruning algorithm based on one-level optimization and optimization complexity matching. The experimental results show that the proposed method leads to state-of-the-art performance on CIFAR-10/100 and ImageNet datasets.\n\n- This paper mentions that when we randomly select operations at stage 1, the final result is almost the same as that of the proposed method. How about the grouping of operations? When we randomly group the operations, how would the results change? It would be better to provide such results to clarify which parts of the proposed method contribute to the final performance.\n\n- If possible, it would be better to compare the proposed method with recent state-of-the-art methods such as XNAS, FairNAS, PCDARTS, and ScarletNAS. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This work outlined 3 problems of DARTS a) bi-level optimization, b) multicollinearity of correlated operations, c) the fundamental\nchallenges of matching neural network optimization complexity in NAS and in the final training. The authors proposed algorithm to fix them with a) one-level optimization b) group variable pruning and c) complexity matching by gradient confusion.\n\nThe author provided detailed experiments to support their claim of the problems of DARTS and validated their proposals with extensive benchmarks. The structure of the paper is well organized and the wording is in very good style.\n\nI have two following minor comments:\n1. Did the author do an ablation study on each of the component they proposed? For example using DARTS with only group variable pruning to highlight the contribution of it. And similarly, for one level optimization.\n2. The part how to select depth using gradient confusion is not very clear to me."
        }
    ]
}