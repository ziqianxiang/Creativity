{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "[Overview]\n\nIn this paper, the authors proposed a dynamic graph message passing network which learns to dynamically capture the structural context information in the images for various vision tasks. In the dynamic graph message passing network, the authors proposed to learn to adjust the positions of neighbor nodes in the graph for a target node, and then learn a node-specific filters for performing message passing. By applying the proposed dynamic graph message passing network, the authors showed superior results on various vision tasks, such as detection, semantic and instance segmentations. A further detailed ablation analysis help to understand the different components in the model.\n\n[Pros]\n\n1. The paper is well-written and the model is clearly motivated and explained. I enjoyed reading the paper and the presentation of the experiments are also very comprehensive and impressive.\n\n2. The authors proposed to learn position-dependent random walk to construct the graph structure, and also learn node-specific filters and affinities for the message passing across the graph. Both of these two components are intuitive.\n\n3. The authors evaluated the proposed model on various vision tasks under different backbone architectures. It is shown that the proposed model can achieve better performance over various previous models, such as deformable conv, non-local networks, etc. \n\n4. Finally, the authors also performed a number of ablation studies to show the effectiveness of the proposed model on various tasks, with different ablated settings. It shows that all of the separate components contribute the final performance on semantic segmentation task. \n\n[Cons]\n\n1. Though the model is well-motivated. The proposed model have limited novelty. The reasons are three-folds: \n\na) the authors proposed dynamic walk method to learn to find the graph nodes, which is similar to the strategy used in deformable conv. Both of them are trying to learn the offset for the grid location based on some learnable parameters. Looking at Figure 4, the learned walks for nodes looks similar to the learned patterns in deformable conv.\n\nb) in dynamic filter, the authors attempted to learn different filters for different nodes in the graph, which has naturally happened in the deformable conv module. The difference is that in this model, the filter is learned based on the input node feature, while in deformable conv, the filter is learned globally across different locations.\n\nc) when looking close to Eq(5) and Eq(6), learning affinities is also not new. We have seen such strategy have been introduced in:\n\n[1] Graph Attention Networks. Petar et al. ICLR 2018\n[2] Graph R-CNN for Scene Graph Generation. Yang et al. ECCV 2018\n\nOverall, I think the proposed dynamic message passing strategy is not a brand-new techniques, which is more like a mixture of various components from other work.\n\n2. The overhead brought by the proposed model is still pretty high. Though the authors claimed that the proposed model has reduced much the FLOPs compared with non-local network, it is still much higher than other models like deformable conv. Also, the authors did not compare themselves with GCNet and CCNet on the computational complexity. \n\n3. Based on the above analysis, it is still not clear whether the improvements over other methods are due to the model design or the computational and parameter overhead introduced by the model. When we look close the Eq(5) and (6), and substitute Eq(6) into Eq(5), we can find the overall computation in Eq(5) become multi-hop computations over the node features. In this sense, I am curious whether increasing the number of  DGMN layers without using dynamic filters and affinities but different parameters at different layers can still achieve the same performance.\n\n4. Based on the above analysis, the authors should add at least one more experiments by decreasing the overhead introduced by the DGMN and compare it with the previous work. Also, if possible, make increase the parameters and computational complexity of baseline network so that it is comparable to the DGMN would be a strong experiment to show.\n\n[Summary]\n\nThis paper present a dynamic message passing networks for various vision tasks. the authors proposed to learn to positions for the grid nodes and the filters and affinities as well. The paper is well-written and clearly presented. The experimental results demonstrate the effectiveness of the proposed model on object detection, instance segmentation and semantic segmentation. However, as pointed above, I still think the novelty of the proposed model is limited and it is not very clear to me whether the improvement is due to the proposed model or the high computational complexity. Overall, I still think the paper is a good paper. I would like to give a weak accept above borderline. However, since it is not an option, I choose weak reject as the rating. I will be open to change my rating if the authors can address my above questions.\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This work explores methods that capture long-range dependencies for computer vision tasks. It uses a graph-based formulation to view the problem, and proposes to perform input-dependent sampling and learn input-dependent weights and affinities for message passing. \n\nPros:\n1. The proposed method is clearly introduced\n2. The differences between the proposed method and related work are clearly explained and demonstrated through ablation study.\n3. The visualization in Figure 4 backups the authors' claims.\n\nCons:\n1. It is not clear why the graph-based formulation is better, making it hard to compare the proposed method with (Jia et al., 2016; Wu et al., 2019). More details should be provided.\n2. The results of Non-local (Wang et al., 2018) in Table 2 is not consistent with the original paper.\n3. In all experiments, is the training setting the same for all the models involved?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nThis work proposes a dynamic graph message passing network to efficiently capture long distance relationships in computer vision tasks. The proposed methods in this paper are closely related to deformable and non-local papers.\n\nThe paper does not clearly show the difference with deformable and non-local and why the proposed methods works better than them. This paper is more than 8 pages, which requires higher standard when reviewing. Here are some other comments.\n\n1. The number of parameters for learning position-specific random walks can be very large as it is P ×(K×C) x h x w. This can also be observed from Table 1(a) by comparing DGMN w/ DA+DW with DGMN w/ DA+DW+DS. It means the application of proposed method is limited.\n\n2. The comparison details with deformable paper and non-local paper is not clear. This paper actually is closely related to these two papers. There is little details about these two models in experimental studies. Notably, DGMN w/ DA+DW even performs worse than non-local. By adding more parameters to learn walked nodes, DGMN w/ DA+DW+DS outperforms non-local. Although non-local use 2.88M parameters, the authors can remove some of linear transformations to make it clear. Since the proposed method works very similar to non-local and deformable, I need more details to verify the effectiveness of the method.\n\n3. Is the proposed method really related to graph? The key part of GCNs is the node connectivity. Here, the method samples a fixed number of nodes that are connected to a center node. Why the authors want to use graph concepts to explain the story. The connection to graph is not convincing to me.\n\nSuggestions:\n\n1. Better explanation of the novelty especially compared with deformable and non-local.\n\n2. Fair experimental studies or detail explanation about the fairness.\n"
        }
    ]
}