{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Thank you for your submission.\n\nThe paper studies representation learning with vs. without supervision. It claims unsupervised learning is not enough for learning strong representations for downstream tasks.\n\nThe paper refers to previous work that supports the claim that tasks are important for representation learning in biological systems.\n\nThe paper's results provide evidence that supervised training with a specific task training set yields a richer representation than unsupervised training. The performance is measured on the same set of tasks, but with held-out realizations/configurations.\n\nI vote for rejection of this paper.\n\nThe paper is making a case for the fact that supervised training signals are richer for representation learning that unsupervised ones. This is widely accepted.\n\nThe paper makes a valid point that optimizing unsupervised learning criteria (such as log-likelihood of reconstructed inputs) is not in itself the solution to learn good representations. The point of unsupervised learning is leveraging data that is often more abundant than data for supervised learning. In my view the way to reconcile these two points is to identify/develop adequate unsupervised losses and, when possible, compare their representation learning on supervised tasks of interest. In contrast, replacing the unsupervised learning with supervised learning does not reconcile the two points, but only bypasses the issues of adequate unsupervised representation learning.\n\nThe paper's arguments in favor of task-mediated representation learning are an interesting motivation to support multitask learning.\n\nA few technical remarks: \n* The bandit problem is not really a bandit problem, because there's no decision-making involved. It's a binary prediction task with sequences as inputs.\n* I think it would be useful to introduce the problem being addressed and mention the contributions before going into the details of the dataset and network architectures. A quick few-paragraph overview of the paper (problem being addressed; what the results look like) in the introduction will help the reader navigate the paper better.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper is driven by the argument that neural encoding in biological systems is shaped by behavioral objectives and, hence, proposes to make use of an abstract surrogate objective. This is said to result in a model that learns representations that are implicitly shaped by the goal of correct inference.\n\nIndeed, closing the gap between neural encodings in biological and artificial systems is an important goal. Unfortunately, the paper is not well presented. There are a number grammar flows. Or consider e.g. Fig. 1. Here, the variable Y is not explained. It is also not easy to understand \"the arm 13\" can be found. Or consider the introduction of the model. It follows very basic differentiable programming units, while the abstract claims that \"rather than labeling the observations ... we propose a model that learns representations that are implicitly shaped by the goals of correct inference\". Ok, now the inference is the label. Fine. But what is the benefit of doing so? Unfortunately, this is not discussed and also the experimental evaluation does not show this. A comparison to VAE on a single dataset is not enough to convince the reader that this generally holds. Moreover, it is not shown that the representation now helps better in some applications than existing approaches. It would be much more convincing the new models beats VAE in a task VAE are good at. In any case, evaluations on more tasks/datasets are definitely missing to show that the method generally works fine.\n\nTo summarize, interesting direction but the paper is not ready for publication at ICLR yet. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary: The paper proposes a model of \"inference-driven representation learning\" that is inspired by \"neural encoding in the biological systems\". This boils down to training a model to learn representations that are suited for a given task (and not general-purpose representations as an auto-encoder would do).\n\nThe paper is sparse on a lot of details - about the datasets, about the proposed approach, and the experimental results. There is no quantitative comparison with any baselines and many results are described in vague terms without explicit quantitative results (for example, \"Performance of the model did not change significantly\"). In the current form, it is extremely difficult to decide if the paper makes any meaningful contributions.\n\n=============\n\nFollowing are the main concerns:\n\n* In absence of any baseline, the empirical results are meaningless. For example, the overall performance of 98% does not mean much aa we do not have any estimate of how hard the tasks are. Moreover many results are described vaguely (pointed out above).\n\n* The dataset should be described in detail. Even information about the dataset size is missing. Given that the dataset is procedurally generated, it would be useful to consider some meaningful ablations which could be hard to perform with real datasets. For example, If the reward distribution is changed during training (for the 2 arm bandit), how quickly can the model adapt to the new distribution?\n\n* The model description is lacking a lot of details. For example, how is the problem statement (P) \"encoded\"? How is the problem represented before feeding to the model?\n\n* Ingesting the dataset sequentially imposes a dependence on the order in which the data is presented. Now it is no longer just a representation learning problem, the model has to learn long-term dependencies as well (when propagating gradients). How is this handled properly (in the context of issues like vanishing and exploding gradients)?\n\n* In the first paragraph of the introduction, the statement \"propose one possible way...unlabelled data\" is somewhat misleading. It suggests that the representations can be learned without needing any task-specific labels which is not the case. The authors here use \"labels\" to refer to the underlying regularity in the data. It is quite common for the learning algorithms to not assume access to this kind of information.\n\n\n=============\n\n\nFollowing are the things that could be improved in the future version of the paper but did not affect the score:\n\n* Figure 1 does not seem to be cited anywhere.\n\n* On page 5, line 2, it seems that Figure 2 is cited by mistake.\n\n* Words are missing at certain places (eg \"representations that implicitly shaped by\").\n\n"
        }
    ]
}