{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a method to address the covariate shift and label shift problems simultaneously. \n\nThe paper is an interesting attempt towards an important problem. However, Reviewers and AC commonly believe that the current version is not acceptable due to several major misconceptions and misleading presentations. In particular:\n- The novelty of the paper is not very significant.\n- The main concern of this work is that its shift assumption is not well justified.\n- The proposed method may be problematic by using the minimax entropy and self-training with resampling.\n- The presentation has many errors that require a full rewrite.\n\nHence I recommend rejection.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "In this work, the authors proposed a method to address the covariate shift and label shift problems simultaneously. In detail, the prototype-based conditional alignment and self-training based label distribution estimation is utilized. Empirically evaluation is conducted on three datasets to show the superiority of the proposed method. However, the work suffers the following weaknesses:\n \n1). The main concern of this work is its shift assumption. In the language of dataset shift, the joint distribution p(x, y) can decompose into two different manners, which are p(y|x)p(x) and p(x|y)p(y). Covariate shift is defined as p(x) not equals to q(x), while the conditional output distribution is invariant p(x|y) = q(x|y), where p(.) and q(.) are distribution for source and target domains. Label shift is defined as p(y) not equals to q(y), while the conditional input distribution p(y|x) = q(y|x). The work assumes that p(x|y) not equals to q(x|y) meanwhile p(x) not equals to q(y). It means to minimize the joint distribution p(x, y), which is well motivated. However, it does not solve the two abovementioned shifts simultaneously here. Instead, it aims to minimize the marginal distribution and conditional distribution in the anticausal direction. See more definitions in the papers “When training and test sets are different: characterizing learning transfer” and “On causal and anticausal learning.”\n \n2). The novelty of the paper is limited. While the authors claim that it is the first time to approach it in the proposed manner, the problem of both p(y) and p(x|y) change is not new. For instance, in the paper “Domain adaptation under the target and conditional shift,” the case of distribution shift correction also does not assume the same conditional distribution and marginal distribution for the source domain and the target domain. Also, the fulfill of conditional alignment is used the formulation and architecture of the work “Semi-supervised Domain Adaptation via Minimax Entropy,” except that there is no labeled target data in the target domain (see Eq.(1)). Besides, the notation f in Fig. 2 is missing the description of Section 3.           \n \n3).  Although the prototype-based method does help in minimizing the problem of p(x|y) not equals to q(x|y), using the minimax entropy domain adaptation in an unsupervised setting is problematic. Without a few labeled target data points, it is challenging to learn the discriminative features for the target domain. If positives and negatives (suppose it is a binary classification) are severely overlapped in the target domain,  the learned prototypes could be not consistent with those in the source domain. In other words, the prototypes might not indicate the same classes for source and target. Another issue is that the proposed model cannot solve the problem given in the assumption. In detail, the assumption is p(x, y) not equals to q(x, y), using the shared feature function F(.) and classifier C(.) for the source and the target cannot obtain domain-invariant feature and adaptive target predictor at the same time.\n \n4). There is an issue in the label distribution by self-training. As the authors claimed, balanced sampling could diminish the effect of the label shift. However, there is no substantial theoretical evidence on this. Intuitively, the balanced sampling only ignores the original marginal distribution of the target domain. The authors should provide more explanation on it. Meanwhile, the sampling couldn’t work when there is a large number of categories. For self-training, it seems no mechanism to alleviate the label shift. Besides, the iterative learning manner heavily depends on the initialization of self-training, i.e., the top-k samples might not represent the marginal distribution.\n \n5). For the theoretical insights, first of all, Eq.(5) is given in “A theory of learning from different domains,” Shai Ben-David et al., Machine Learning, 2010. Second, there is a mistake in Eq.(6). The second term on the right of the inequality is not JS divergence of distributions over x. Instead, it is JS divergence of those after transformation of x, z (see the subsection An information-theoretic lower bound, Zhao et al. 2019b).  Also, the descriptions of the insights are not correct."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "\n\n\nThis paper proposes domain adverarial approaches modified to address covariate shift \neven in the case of shifting label distributions. The paper is interesting and the authors\nare looking at an important problem, but the paper suffers several major misconceptions\nand the exposition is full of errors.  \n\nThe paper proposes a method called “self training” to “estimate and align” the  target label distribution\nand a “prototype-based method” for conditional alignment.\n\nThe paper purports to introduce a new problem: “generalized domain adaptation”\nbut actually this is just covariate shift. \nThey confuse the term “label shift” and the colloquial “shifting label distribution”.\nMoreover when they describe the method formally they fail \nto even state the covariate shift assumption (that p(y|x) = q(y|x) ).\nAbsent this statement, the problem is unfdamentally underspecified.\nThey have stated only that p(y) \\neq q(y) and that p(x|y) \\neq q(x|y).\nThus the claimed contribution is to solve an impossible (more accurately, underspecified problem).\n\nThe proposed method leverages a “similarity classifier”\nThe thing that the authors call a similarity-based classifier\nisn’t well explained. It looks like an ad-hoc variation on a standard\nsoftmax prediction layer. \nHere, not that similarity means similarity between x and the label weights w,\nnot similarity between examples and each other.\nMoreover this classifier is trained as a classifier using a \nstandard cross-entropy objective on the target domain.\nThe paper lacks any formal justification for what this is offering\nthat we do not get from a standard classifier.\n\nThe next component of the model is \"Prototype-based Conditional Alignment by Minimax Entropy”\nand it is also confusingly explained.\n\nThe paper attempts to make some reference to the theory of Shai Ben David\nwhich has been badly misapplied in the deep domain papers\n(see discussion by Johannson et al 2019 and Wu et al 2019).\nStrangely, the authors misattribute the theory to Zhao 2019.\n\n\nThere are some nice ideas in the paper and the experimental results \n(flaws in domain adaptation benchmarks notwithstanding)\nappear to be promising.\n\nHowever the paper is written too confusingly, is outright wrong in many places\nand runs the risk of badly misleading readers over even the most basic of definitions.\nI encourage the authors to give the paper a gut rewrite\nand do not believe that it can be published while resembling its current form.\n\n“only aligns the covariate shift”\n>>> \tBe more formal, not clear what it means to “align the shift”\n\tMoreover, note that owing to lack of shared support , it’s not clear \n\twhat precisely the current methods (domain-adversarial) do\n\tor according to what principles they work.\n\n\n“the covariate shift needs to be minimized”\n>>> \tThis is not a coherent way of describing the problem.\n\tThe covariate shift is a property of the data.\n\tYou cannot “minimize the shift”\n\n“label shift (p(y) ̸= q(y))”\n>>> \tActually this is a “shfit in label distribution”. \n\tNote that you can have a shift in label distribution \n\teven under the covariate shift assumption.\n\tLabel shift is the reverse assumption that p(x|y) = q(x|y).\n\n\n“minimizing the label shift”\n>>>\tAgain, this doesn’t make sense. The practitioner doesn’t get to choose the data\n\tthat they will face at test time. The “shift” refers to the data. \n\n\n\"Specifically, we assume p(x|y) ̸= q(x|y) and p(y) ̸= q(y)”\n>>>\tThe problem described in this paper is called Generalized Domain Adaptation”\n\tbut actually it is just “covariate shift” this is not a new problem. \n\tThe use of new terminology for old problems and misapplication of old terminology,\n\te.g. “label shift” make this paper a potential danger to readers \n\twho then will be confused in their subsequent interactions with \n\tthe wider literature on distribution shift.\n\n“These methods have achieved state-of-the- art performance on several domain adaptation benchmarks“\n>>>\tIt’s worth pointing out that benchmark SOTA is a dubious way to assess performance out of sample.\n\tThe point is that in supervised learning you get to know that your target is the same as your source,\n\tso it’s ok to have a whole community smash the validation set and then see if we push the leaderboard on test data\n\tWith domain adaption, the relevant sample size is the number of shifts, no the number of images.\n\tHaving the community pound on 2-3 shifts tells us virtually nothing.\n\n\n\"our approach diminishes covariate and label shift”\n>>>\tAgain this is not the accurate way to describe what you do.\n\tYou attempt to salvage classifier performance under these shifts,\n\tyou cannot “diminish the shift”. \n\n“Recently, Azizzadenesheli et al. (2019b) propose a regularized algorithm to correct shifts in the label distribution by estimating the importance weights using labeled source data and unlabeled target data. Lipton et al. (2018b) introduce a test distribution estimator to detect and correct for label shift.”\n>>>\tThis is not exactly the right characterization of the related work.\n\tThe method proposed by Lipton 2018 is precisely what \n\tAzzizadeneshelli 2019 build upon (by adding a regularizer) \n\n“Conventional domain adaptation approaches … only align marginal feature distribution”\n>>>\tAgain, the authors speak about “aligning the marginal feature distribution”,\n\tbut this is confusing, Representations are aligned, not features.\n\t\n\n“This motivates us to align the conditional feature distribution, i.e. p(x|y) and q(x|y)”\n>>>\tAgain these are not things that can be “aligned”. They are properties of the data.\n\tThe entire paper needs to be re-written to be semantically coherent."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper deals with covariate and label shift in common\nevaluated on some standard benchmark data.\nThe paper is scientific sound and appears to be in good shape,\njust a few comments:\n- how is your label shift different to concept drift in supervised learning?\n  --> if this is basically the same I would expect that you can use,mention methods from there\n- the plots with t-SNE are obviously colorful but do not provide a lot of information - they should\n  be removed - I do not see a benefit\n- it is very common that all these methods (like yours) are provided on some kind of image\n  data ... is there a particular reason / limitation?\n- I would like to see additional experiments on similar text data -> reuters\n- self-training is a concept from semi-supervised learning and not particular well supported\n  in the community - how do you make sure that the result remain valid\n- how do you make sure that the adaptation of the labels, the covariate shift and the classifier training\n  are not in facting cheating the result to an optimum within the optimized cost function?\n"
        }
    ]
}