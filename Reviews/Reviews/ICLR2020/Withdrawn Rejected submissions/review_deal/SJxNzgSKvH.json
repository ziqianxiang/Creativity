{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposes a method to speed up training of deep nets by re-weighting samples based on their distance to the decision boundary. However, they paper seems hastily written and the method is not backed by sufficient experimental evidence.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "A new approach is proposed to speed up training in deep models.\n\nThe idea is to select sample batches when back propagating the error based on the distance of the prediction foe the sample from the decision boundary. Specifically, we pick points closer to the boundary, i.e., ones that we are less confident about for backpropagation.\n\nExperiments are performed comparing the method with Hard negative sampling (HNM) , entropy-based sample selection as well as regular training. Experiments are performed on Cifar10 and Cifar100 datasets. \nWhy only two datasets, the method is general so there should be more datasets to verify its performance.\n\nThe results on Cifar100 in Fig 5 c seems to show that we cannot reach the training accuracy using the proposed method as compared to the other methods. What is the intuition here as to why it happens? In general though since the main goal is to speed up training I do not see very convincing evidence of this in the limited evaluation which seems to be the main weakness here."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper proposes a minimal margin score (MMS) criterion to speed up the training of the deep networks.\n\nI would vote for a clear rejection of this paper. This submission is a clearly unfinished one. The two biggest problems are as follows\n\n1. Lack of a comprehensive discussion on rules for sampling section, please see \"Automated Curriculum Learning for Neural Networks\". Why previous methods are worse than the proposed one is not clear.\n\n2. All experiments are only compared with baseline approaches. In some experiments, the improvements are really marginal (e.g., Figure 2). In these cases, the STD of these curves is not shown, it is not clear whether the improvements are significant or not."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "### Summary of contributions\n\nThis paper aims to accelerate the training of deep networks using a selective sampling. \nThey adapt ideas from active learning (which use some form of uncertainty estimation about the class of the label) to selectively choose samples on which to perform the backward pass. Specifically, they use the minimal margin score (MMS). \nTheir algorithm works by computing the forward pass over a batch of size B (which is much larger than the regular batch of size b), compute the uncertainty measure for each sample, and only perform the backward pass over the b samples with the highest uncertainty. The motivation is that the backward pass is more expensive than the forward pass, and that by only performing this pass on a subset of samples, computations are saved. \n\n\n### Recommendation\n\nReject. The central premise of the paper is unclear, the writing/presentation needs improvement, and the experiments are not convincing. \n\n\n### Detailed comments/improvements: \n\n\nThere is a central premise of the paper that I don't understand: that the forward pass is much cheaper than the backward pass. \nThis is claimed in the intro by referring to charts that hardware manufacturers publish (but there are no references included), but I don't see why this should be the case. \nFor a linear network with weights W, the forward pass is given by the matrix-matrix product (rows of X are minibatch samples):\nY = XW^T\n\nand the backward pass is given by the two matrix-matrix products:\ndL/dX = dL/dY*dY/dX = dL/dY*W\ndL/dW = dL/dY*dY/dW = dL/dY*X^T\n\nSimilarly the two operations in the backward pass for convolutional layers are given by a convolution of the output gradients with the transposed weigtht kernels and the input image respectively. \n\nPoint being, I don't see why the backward pass should be more than 3x more expensive than the forward pass. A simple experiment in PyTorch confirms this: the code snippet pasted at the bottom shows that the backward pass takes only around 2.6x longer than the forward pass.  \n\nfprop: 0.009286s\nbprop: 0.0240s\nbprop/fprop: 2.5893x\n\nIn algorithm 1, it is assumed that b << B. For this to be effective the forward pass would have to be *much* faster than the backward pass for this method to yield an improvement in computation. Can the authors comment on where this justification comes from?\n\nI am unclear on what the purpose of Section 4.1 is. This shows that the MMS of the proposed method is lower than the other two, but this should be completely expected since that is exactly the quantity being minimized. \nThere are also several unsubstantiated claims: \"Lower MMS scores resemble a better...batch of samples\", \"the batches selected by our method provide a higher value for the training procedure vs. the HNM samples.\", \"Evidently, the mean MMS provides a clearer perspective...and usefulness of the selected samples\". What does higher value, usefulness, clearer perspective mean?\n\nMore generally, it is unclear if there is really any improvement in the final performance from using the proposed method.\nIn Figure 2, all methods seem to have similar final performance. \nIn Figure 5, is there a reason why the curve for MMS is cut off? How does its final performance compare to that of the baseline method in red? It looks like the baseline might be better, but it's hard to tell from the figure. \n\nWhy are the experiments with the entropy measure in a seperate section? Please include them along with the other methods in the same plot, i.e merge Figure 2 and Figure 4. \n\nMy suggestions for improving the experimental section are as follows:\n- include all methods together in all the plots/tables\n- repeat experiments multiple times with different seeds to get error bars. Include these both in the learning curves and in the tables. \n- It's hard to see small differences in the learning curves, so including tables as well is important. Include best performance for all the methods in the tables. \n\nFinally, in 2019 CIFAR alone is not longer a sufficient dataset to report experiments on. Please report results on ImageNet as well. \n\nOne of the central premises of the paper is acceleration in terms of compute/time. To make this point, there should also be results in terms of walltime and floating-point operations. Please include these results in the paper.  \n    \n\n\n\n### Code snippet timing forward/backward passes\n\n\nimport torch, torch.nn as nn, time\n\nmodel =\tnn.Sequential(nn.Linear(784, 1000),\n                      nn.ReLU(),\n                      nn.Linear(1000, 1000),\n                      nn.ReLU(),\n                      nn.Linear(1000, 10),\n                      nn.LogSoftmax())\n\ndata = torch.randn(128, 784)\nlabels = torch.ones(128).long()\nt = time.time()\npred = model.forward(data)\nloss = nn.functional.nll_loss(pred, labels)\nfprop_time = time.time() - t\nt = time.time()\nloss.backward()\nbprop_time = time.time() - t\nprint('fprop: {:.4}s'.format(fprop_time))\nprint('bprop: {:.4f}s'.format(bprop_time))\nprint('bprop/fprop: {:.4f}x'.format(bprop_time / fprop_time))\n"
        }
    ]
}