{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary:\n\nThis paper proposes a method to generate hierarchical explanations for text classification by feature interaction detection. The method is model-agnostic and is demonstrated on a diverse set of text classification models: LSTM, CNN, and BERT. \n\nThe paper starts by proposing a method for computing the importance of a subset of features. This is done by subtracting the first and second top class probabilities at a subset of word inputs, where other words are replaced with <pad>. Next, the shapley interaction formulation from [1,2] is invoked to detect feature interactions. \n\nLastly, a hierarchical interpretation is generated top-down by minimizing interaction between sentence partitions, and repeatedly splitting generated partitions.\n\nInterpretations are evaluated via AOPC/log-odds scores for word importance, cohesion loss for phrase importance, as well as a coherence score in mechanical turk studies.\n\n\nRecommendation:\n\nReject, because 1) the paper does not compute shapley interactions with the proposed formulation of subset feature importance, and 2) the paper does not acknowledge related work on interpreting feature interactions in prediction models beyond SHAP interactions [3,4,5]\n\n\nSupporting arguments:\n\nThe interaction definition shown in Eq. 3 does not match the corresponding equation referenced in Lundberg et al. [1]. Based on [1], the $f$ in Eq. 3 should be the expected value of a model output conditioned on a subset of input features. The $f$ defined in Eq. 1 does not compute this expectation; therefore, the meaning of the Eq. 3 result is unclear - although it is extremely important to this paper. \n\n\nSuggestions:\n\nWithout justifying $f$, this work needs to justify that $\\phi$ is an estimate of interaction importance. One way to do this is by evaluating InterShapley against ground truth interactions. For example, an LSTM (or CNN, BERT) can be trained on synthetic data generated by functions with known interactions (e.g., x1*x2). Then, InterShapley can be validated by identifying these interactions. \n\nFurthermore, it is important to discuss how InterShapley is novel, especially when the idea of model-agnostic hierarchical explanations has been proposed already in [5].\n\n\n[1] Scott M Lundberg, Gabriel G Erion, and Su-In Lee. Consistent individualized feature attribution for tree ensembles. arXiv preprint arXiv:1802.03888, 2018.\n[2] Katsushige Fujimoto, Ivan Kojadinovic, and Jean-Luc Marichal. Axiomatic characterizations of probabilistic and cardinal-probabilistic interaction indices. Games and Economic Behavior, 55 (1):72–99, 2006.\n[3] Jerome H Friedman, Bogdan E Popescu, et al. Predictive learning via rule ensembles. The Annals of Applied Statistics, 2(3):916–954, 2008.\n[4] Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker. Accurate intelligible models with pairwise interactions. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 623–631. ACM, 2013.\n[5] Michael Tsang, Youbang Sun, Dongxu Ren, and Yan Liu. Can i trust you more? model agnostic hierarchical explanations. aXiv preprint arXiv:1812.04801, 2018."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Overview/Contribution:\n====================\nThe paper proposes an interpretation method that is based on feature interactions and feature importance score as compared to independent feature contributions. They evaluated their methods on three models for text classification task with two datasets. They evaluated the quality of the generated interpretations using metrics as well as human validation.\n\nOverall, I strongly don’t recommend accepting this paper as it is in current form. I back the decision with the following strengths and weaknesses. I suggest to the authors to address the weaknesses pointed out to make the paper more organized, motivating the need for such feature interaction based interpretation in a more clear manner. I would like to note that I have read the paper well and seen the supplemental appendix.\n\nStrength:\n========\n+ Human understandable interpretations are essential to describe decisions made by deep models to end users in sensitive applications such as healthcare and security. Interpretations that consider multiple features at a time have potential for generating more meaningful interpretations.\n\nWeakness:\n===========\n- The flow of the paper is not smooth to follow. Experiments comes very quickly without sufficiently detailing the proposed method. It is seriously flawed.\n- The method is neither described in detail nor motivated well enough. Authors are using the word explanation throughout while they are attempting a very simple feature/input importance score based interpretation. These two are used interchangeably used but they mean completely different things especially when talking about human level/understandable explanations [1]. \n- Nothing was described about the evaluated models. The names LSTM and CNN are very generic. No description what so ever about the datasets before giving the results on Table I.\n- Experiments and results are so intermingled that it is really hard to interpret and unravel the main points.\n- Feature importance score does not necessarily mean interpretations from the importance are good human understandable explanations. The proposed AOPC metric is evaluating feature importance by evaluating the probability of predicted label given the subset of input features, i.e. “Correlation does not necessarily mean causation”.\n- In general, the paper is not ready to be published at ICLR.\n\n1) Bekele, E., Lawson, W. E., Horne, Z., & Khemlani, S. (2018). Implementing a Robust Explanatory Bias in a Person Re-identification Network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops (pp. 2165-2172)."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nSummary: The paper describes a method for providing explanations for the predictions made by a text classifier. The method is novel in that it aims to explicitly model interactions between words so that e.g. the phrase \"not bad\" appears as an explanation itself, rather than as separate words \"not\" and \"bad\". They produce these explanations in a hierarchical way, by greedily dividing the input phrases into two consecutive subphrases until reaching the word level. The model is compared against and out-performs a variety of baselines on word level-importance scores, using two metrics from prior work (AOPC and LogOdds) which measure how much predictions change when high-importance spans are ablated. The authors also introduce a new metric (\"cohesion loss\") which is meant to evaluate span-level importance. This metric works by measuring how much predictions change when the words from important spans are broken up and scattered throughout the sentence (rather than appearing as one span). On this metric, they outperform one baseline which also produces span-level importance scores. Finally, they run a short human eval on MTurk in which they measure how well humans can anticipate model predictions given the explanations. Here they outperform one baseline (it is unclear why they can't compare against more baselines).\n\nEvaluation: The paper seems to present a nice idea. The qualitative results (example hierarchical explanations) are compelling. I have a few concerns related to overall clarity of the paper itself, which make me hesitant to recommend publishing it as is. My primary concern is that the most important evaluation (evaluation of phrase-level importance scores, Sec. 4.1) relies on a newly-proposed metric and thus we cannot be confident in the soundness of this metric. Similarly, the human eval seems rushed and only compares against one baseline, when in fact this should be one of the primary evaluations as the entire contribution of the paper depends on this method being superior at offering explanations. As a lesser concern, I also felt that the description of the method itself (Sec. 3.2) was quite terse and lacking in motivation/intuition, even though I don't think that the overall procedure is in fact very complex (and thus excessive notation is probably removing versus adding clarity). It was difficult to evaluate the novelty and reasonableness of the chosen scoring functions without consulting several other papers/resources. I think the authors need to rewrite these sections prior to publishing. Additional specific questions below.\n\nSpecific Questions for Authors:\n--> Section 3\n* Is the subscript under max in Eq. 1 correct? feels like it should be y' neq yhat, and y' in Y?\n* In Eq. 1 you say you replace with PADs. There is always some weirdness when getting scores on inputs that don't look like the training inputs--e.g. phrases surrounded by PADs are not the same as sentences. Can you comment on how this might effect the reliability of the results in terms of fidelity to what the model is actually doing?\n* You should give short a definition and explanation of Shapely value, and summarize how it applies to your approach (and the prior related work which also used it). I wasn't familiar with this before reading this paper so had to look it up, and I don't think its something that can be assumed to be well-known by ICLR audiences. It would help a lot if you spelled out what it is, what its original use was, and then highlighted the ways you differ from the out-of-the-box application (i.e. how/why you have to adapt it, and the intuition behind those adaptations).\n* You don't introduce what subscripted z^l notation is, its a bit obtuse\n* I am confused on the motivation behind Eq. 3 and what role it plays in the overall process. I thought the Shapely score (Eq. 2) is supposed to capture the feature interaction, but Eq. 2 depends on Eq. 3, which is also described as being \"feature interaction\". Can you explain more what these different scores are for, why both are needed, and which are novel vs. given by prior work in game theory and/or ML?\n* End of section 3: \"in generally\" -> \"in general\"\n* Note about complexity: You make the assumption that you can only get interactions between adjacent spans. While this is fine, it is not t necessarily ideal, right? E.g. in Fig 4, the ideal explanation i'd want to see is \"not bad at all\" regardless of the intervening noun (a journey), but this isn't accomplished if you can only measure interaction between consecutive spans. Longer phrases with richer parse structures could be harder to explain given this assumption. You should comment on/discuss this assumption and potential limitations associated with it.\n\n--> Section 4\n* Forgive me if this is a naive question, but why can't these AOPC and LogOdds metrics just be used directly for importance score? If we believe they are an accurate measure of what the model is using, why not just use them to generate the scores in the first place? \n* Cohesion Loss: This metric relies on interpreting the predictions the model makes when given malformed input that would be unlike what it sees in training (in this case, scrambled sentences). Again, I have some concern about getting/interpreting model predictions on malformed input, esp. for higher-capacity models like BERT which may be sensitive to the weirdness and behave worse as a result. Are there baselines for this metric? Or any way you could justify that its meaningful and reliable? Its suspect to have the paper's primary evaluation be based on a newly-defined metric, since its hard to know whether or not to trust it.\n* nit: Calling it \"cohesion loss\" implies lower is better, so \"cohesion score\" might be more appropriate\n* Why can't you run the human eval over many more baseline models, e.g. models that offer only word-level explanations (all the ones in Figure 2)? I'd like a more thorough human eval since the only reason we would want explanations is for human consumption, so that is the main point of this entire project.\n* Overall, the human eval section feels comically last-minute. The rate of typos in this section is 10 fold what it was in the rest of the paper. Make sure you give it a proofread. :) I am glad you included it, but feels rushed.\n"
        }
    ]
}