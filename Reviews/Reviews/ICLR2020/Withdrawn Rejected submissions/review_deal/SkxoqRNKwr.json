{
    "Decision": {
        "decision": "Reject",
        "comment": "While there was some support for the ideas presented, the majority of reviewers felt that this submission is not ready for publication at ICLR in its present form.\n\nThe most significant concerns raised were about the strength of the experiments, and choice of appropriate baselines.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "Overall, this paper provides valuable insight into the trade-off between privacy preservation and utility when training a representation to fight against attribute inference attack.\n\nDetail comments:\n\nStrength:\n+ The whole paper is well organized with logic. Notations are well defined and distinguished.\n+ The final results have a good intuitive explanation.\n+ The most impressive part of this paper is the analysis of the trade-off between privacy and utility from which the upper bound is quantified.\n\nWeakness:\n+ The minimax method looks trivial. The difficulty of using such an objective should be emphasized for practical implementation.\n+ The major weakness is the experiments. The experiments only on two datasets may not be convincing for me. And the repetition times, 5 or 3, for each dataset are pretty small. Considering the experiments are conducted with random noise, e.g., DP methods, such a small repetition time is not fair since there a large chance these results could be selected.\n+ Which DP Laplacian mechanism is used is not specified. Since there are already many improvements on the DP Laplacian mechanism, e.g., [A], it is necessary to make sure the baseline should be state-of-the-art for a fair comparison.\n+ The result is not intuitively surprising that the privacy loss and utility loss will be balanced toward an upper bound (Theorem 4.2). I am not sure how the Jensen-Shannon entropy between D^Y_0 and D^Y_1 can be calculated in practice since the true conditional distribution is not observed. For example, when the data distribution is heavily biased, then the conditional distribution might show less correlation between Y and A. Then, the privacy protection will be a pretty simple task with a very high upper bound. Based on this, it is worth to ask what the upper bound looks like in the dataset used in the experiments.\n+ How efficient this algorithm could be? In comparison to other baselines, does this method provide a more efficient solution?\n\n[A] Phan, N., Wu, X., Hu, H., & Dou, D. (2017). Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep Learning. 2017 IEEE International Conference on Data Mining (ICDM), 385–394. https://doi.org/10.1109/ICDM.2017.48"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a definition of privacy against attribute inference attack and formulates it as a minimax optimization problem that balances accuracy and privacy, it then provides an information-theoretic lower bound for the inference error. It also provides an analysis of the intrinsic privacy-utility tradeoff. Finally, it shows experimental evaluation in terms of inference error and accuracy of several representation-learning based approaches for protecting against attribute inference attack and compares with the lower bounded proved in the paper. By comparing utility with a differentially private algorithm, it shows that the other algorithm achieves higher utility under similar privacy.\n\nThe formulation of the minimax problem seems pretty interesting and the information-theoretic bound seems pretty useful in measuring inference risk based on the experimental results. \nAs for the comments and experiments with differential privacy, I’m not sure if I follow the argument correctly. It seems to me DP is stronger than the privacy guarantee defined in the paper, as it prevents an adversary from knowing whether record x or x’ is in the dataset, which I think directly implies that an adversary won’t be able to infer whether a record with attribute A or A’ (with the rest of the attributes kept) is in the dataset. Can you comment more on that side? Besides, there were two epsilon values used in the experiments for DP. As your goal is not to guarantee DP anyway, I think it makes sense to try more epsilon values, especially much larger epsilon values, as the analyzed epsilon is sometimes pretty loose upper bound."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary\nIn this paper, the author introduces a new privacy notation for the attribute attacks. Under the notation, the author has theoretically analyzed the trade-off between privacy preservation and model utility. Based on the theoretical finding,\nthey further propose an adversarial representation learning paradigm to achieve the best trade-off.  \n\nStrengths:\n1. This paper provides an interesting information-theoretic view to study the privacy-preserving machine learning algorithm.\nBased on this view, the author has presented a comprehensive analysis of the trade-off between privacy-preserve and model\nutility.\n2. The topic of studying the privacy guarantee against attribute attacks is important and implies a wide range of applications, such as preventing the model-inversion attack. \n3. The paper is well-written and provides an enjoyable reading experience. \n\nWeakness:\n1. In experiments, the DP based method should be an important comparison method. However, the DP method used in this paper\nseems to be a weak baseline, which injects the noise into the raw data. Many prior works have prooved that injecting noise\ninto the gradient leads to a better trade-off between the utility and the privacy budget [1]. Thus, the author should re-design their DP baseline and provides comparison results.\n2. There is a related work [2] of reducing the privacy leakage of the feature representation, which also takes a view from the information-theoretic view (i.e. a maximum entropy approach). Although this work focuses on the task of image representation, the author also conducts experiments on a non-image dataset, i.e, UCI dataset. The comparative experiments need to be conducted to show the effectiveness of the proposed method.\n3. The notation of the distribution is confusing. It seems that the author referred $\\mathcal{D}$  to both three joint distributions?\n\n[1]  Abadi et al. Deep Learning with Differential Privacy\n[2] Mitigating Information Leakage in Image Representations: A Maximum Entropy Approach"
        }
    ]
}