{
    "Decision": {
        "decision": "Reject",
        "comment": "This provides a new method, called DPAutoGAN, for the problem of differentially private synthetic generation. The method uses private auto-encoder to reduce the dimension of the data, and apply private GAN on the latent space. The reviewers think that there is not sufficient justification for why this is a good approach for synthetic generation. They also think that the presentation is not ready for publication.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed a new algorithm for synthetic data generation under differential privacy. The algorithmic architecture combines autoencoder and GAN in a way that it only needs to add the DP-SGD noise to the decoder of the autoencoder and the discriminator of the GAN. This seems to be a good idea to be explored further.\n\nThe authors claimed that the proposed new evaluation metrics are novel contributions of the paper but there is no discussion on why they are good metrics for evaluating the quality of synthetic datasets nor which metric should be used in what scenarios. \n\nIt is unclear how the experimental results (Figure 2, 3, 4 and Table 2, 3) are interpreted. The authors mentioned comparison with DP-GAN but it is not marked in the figures the performance of DP-GAN and how its results compared with DP-auto-GAN. Please clearly state what each figure means and why the results are significant.\n\nI wonder if it is possible to have a version of GAN that also predicts the labels of data so you can use classification task as evaluation metrics, which might be easier and more interpretable. \n\nIn Section 3.1, “not adding noise to the decoder” should be encoder.\nIn Section 3.3, “do not add noise to decoder” should be encoder.\n\nThe presentation of the paper needs to be improved. There are too many typos and grammar mistakes. The labels of figures in the experiment section are too small. The paper does not have a conclusion section.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper considers synthetic data generation using deep GANs and autoencoders that can be shared for model training.\nThe authors can generate mixed-type data and consider some additional metrics that allow the evaluation of the quality of synthetic data.\n\nWhile the problem raised in the paper is interesting, and there some insights on what kind of metrics one should use, the article now lacks conclusions and discussion of obtained results. \nIn particular, there is a significant number of misprints and inconsistencies here and there (see more on this below).\nMoreover, the experiments are irreproducible e.g. I could not found information about the value of reduced dimension q in the description of the experiments.\nAlso, there is no comparison with previous approaches (e.g. [1, 2]), only results about the proposed one are presented. \n\nThe paper will also benefit from additional rounds of proofreading:\n\n1. The algorithm $\\mathcal{M}$ is not defined. The range $Range(\\mathcal{M})$ is not defined.\n1. a mixture of Gaussian distribution -> a\nmixture of Gaussian distributions\n2. comepare -> compare, matrics -> metrics, deceases -> decreases\n4. should to minimize -> should minimize\n5. In the formula \"(true) loss function\" subscript \"i\" should be dropped, as we talk about $x \\sim Z$, not $x_i$ here\n6. Articles in many places can be improved (finding good autoencoder -> finding a good autoencoder)\n7. It is possible, that in the paragraph after the formula (2) \"encoder\" should be replaced by \"decoder\".\n8.  the total number of samples the real\ndata - > the total number of available real data samples \n9. The axis labels are too small for Figure 2\n10. No reference to Figure 3 in the text of the paper. For Figure 3 the most left plot has for some reason a smaller number of points. Why?\n11. The selection of classifiers is not discussed. I.e. why in some cases authors use random forests (5.2), but in other logistic regression (5.1)? Also in my opinion mixing of R2 and F1 scores in one plot can be confusing.\n12. No conclusion in the end\n\n\n[1.] Xu et al. Modeling tabular data using conditional GAN. NeurIPS 2019\n[2.] S.K.Lim et al. DOPING: Generative Data Augmentation for Unsupervised Anomaly Detection with GAN. IEEE ICDM 2019."
        }
    ]
}