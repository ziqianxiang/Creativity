{
    "Decision": {
        "decision": "Reject",
        "comment": "Reviewers agree that this paper contains interesting results and simple, but good ideas. However, a few severe concerns were raised by reviewers. Most prominent one was the experiment set up - authors use a pre trained ResNet101 (which has seen many classes of Imagenet) for testing which makes is unclear how well their proposed method would work for unlabeled pool of dataset that classifiers has never seen.\n While authors claim that their the dataset used for testing was disjoint from Imagenet, a reviewer pointed out that dogs dataset, bird datasets both state that they overlap with Imagenet. A few other concerns are raised (need more meaningful metric in Figure 4d, which wasn’t addressed in rebuttal). We look forward to seeing an improved version of the paper in your future submissions. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary: This paper considers the addition of self-supervised learning techniques in the few-shot learning setting. Extensive experiments are done to show that it can be helpful, including in cases where the labeled data is corrupted. The paper also considers the domain mismatch issue where unlabeled images come from a different domain.\n\nReview: This paper is thorough and clearly written. Applying self-supervised learning techniques to the few-shot learning regime is a simple idea, and this paper clearly shows that it can be beneficial. It includes extensive experiments on a wide variety of image datasets, including many additional studies in the appendix. The only possible criticisms of this paper are that it is limited to the image domain (as are most few-shot learning/self-supervised learning studies, so we can probably ignore this) and that it does not produce a huge delta in understanding compared to Gidaris et al. (2019). Gidaris et al. (2019) was posted to arxiv in June, I'm not sure if this work was also on arxiv around the same time, and even if it wasn't I'm not sure what to consider \"concurrent\". However, as the authors note they include additional experimental settings (like the domain-selection idea) that are not in Gidaris et al. (2019), so the works are somewhat complementary. The only other comment I have is that the paper is quite large in scope for a conference submission and as a result there are many details and experiments that are left for the appendix. I could also see the domain selection experiments constituting their own submission. For example the definition of the \"'distance' between a pair of domains\" is only introduced in passing in the midst of Section 4.2 covering domain shift experiments, and the method for training a domain classifier is similarly only mentioned in passing in 4.2.  Of course, it is not really valid to criticize a paper for being too exhaustive. Overall, I recommend acceptance.\n\nSpecific comments:\n- Truly a minor suggestion but I suggest moving Figure 1 to the top of page 2.\n- Snell et al. (2017) needs a \\citep\n- You ought to cite \"S4L: Self-Supervised Semi-Supervised Learning\", which is related to your discussion of connections between self- and semi-supervised learning (though not few-shot).\n- The paragraph beginning \"The focus of most prior work...\" in the Related Work section provides a nice framing of your work and so might make more sense in the introduction.\n- \"we consider self-supervised losses based on labeled data ... that can be derived from inputs x alone\" All labels (can be derived from in the inputs x alone, given an oracle (or human labeler). I think you mean \"that can be derived automatically without any human labeling\".\n- You state \"Our final loss function combines the two losses\". Is there no scalar multiplier on either loss term to trade-off the importance of each?\n- The gains from the self-supervised auxiliary tasks are over and above any gains from data augmentation alone. More experimental details are in Appendix A.5.\" I don't see any information to substantiate the claim that the self-supervised tasks result in a bigger improvement than data augmentation alone, can you provide those details?\n- \"However, does more unlabeled data always help for a task in hand?\" This question actually was addressed somewhat in \"Realistic Evaluation of Semi-Supervised Learning Algorithms\", see sections 4.4 and 4.5 therein."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary & Pros\n- This paper proposes a few-shot learning method that uses self-supervision as an auxiliary label and trains primary and auxiliary labels via multi-task learning.\n- This paper provides extensive experiments for analyzing the effect of self-supervision on various few-shot learning settings: (1) self-supervision can improve various few-shot learning algorithms, ProtoNet & MAML; (2) self-supervision with similar samples can provide more improvements.\n\nConcerns #1: Novelty of the proposed method\n- This paper uses a multi-task learning approach with self-supervision. But this approach is already used in various tasks, e.g., domain adaptation, semi-supervised learning, training GANs. Thus, the proposed method (in Section 3) using a multi-task learning objective with self-supervised losses seems to be incremental.\n\nConcerns #2: Somewhat unsurprising experimental results\n- This paper shows various experimental results, but some experiments seem to be trivial. For example, the performance gap is typically increased when learning harder tasks, e.g., when the number of training samples is decreasing, the performance gap between methods is typically increasing in a fully-supervised setting. Thus I think results in the paragraph \"Gains are larger for harder tasks\" might be predictable. Other examples are Figure 4a and 4b in Section 4.2 because one can easily expect that using training more in-domain samples can provide more performance gain.\n- In the case of Figure 4d in Section 4.2, the authors claimed that the effectiveness of SSL decreases as the distance from the supervised domain increases. However, I think Figure 4d is not matched to the claim. For example, in the case of Dogs, better performance is achieved when using a more dissimilar domain for self-supervision except for D_s=D_ss. So I wonder how to draw the lines in Figure 4d.\n\nSome experimental results provide meaningful messages, e.g., single self-supervision can improve performance significantly while joint self-supervision does marginally. However, the contribution of the methodology is limited and some experimental results seem to incremental.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Summary:\nThere are three main contributions of the paper:\ni) The authors present an empirical study of different self-supervised learning (SSL) methods in the context of self-supervised learning.\nii) They point out how SSL helps more when the dataset is harder.\niii) They point out how domain matters while using SSL for training and present a method to choose samples from an unlabeled dataset.\n\nStrengths:\n1) They confirm the results of [7] and provide additional evidence of the benefit of the self-supervised learning in the few-shot setting.  They also showcase an interesting new result that self-supervised learning helps more in case of harder problems. \n2) The authors have a done a commendable job of coming up with a meaningful set of experiments by varying base-models, self-supervised methods, datasets, and few-shot learning methods in Section 4.1. This is quite a comprehensive study. \n3)  The paper is well-written and well-motivated.\n\nWeaknesses:\n1) The main weakness of the paper is Section 4.2's experimental setup. \n\ni) The definition of domain distance in not quite meaningful. Since the chosen datasets have very different classes (airplanes vs dogs etc), the average embeddings for different datasets/classes will be far from each other.  In Figure 4d, it is misleading to show a trendline that includes the same domain as that will always be 0. If that datapoint is removed the trend line is mostly flat. The authors want to present a quantifiable way to show how domain distribution affect performance on self-supervised learning methods. But this definition of domain distance is more meaningful in the domain adaptation setting (like Amazon-Office dataset used for domain adaptation (https://people.eecs.berkeley.edu/~jhoffman/domainadapt/)) as in that case the requirement is to get the embeddings close to each other for the same class but from different domains.\n\nii) The authors go on to create an \"unlabeled pool\" by combining images from many domains. Then they train a domain classifier by labeling in-domain images as positive and  labeling the images in pool as negative. Considering all images in the pool as negative is not correct as there can be images of same class in unlabeled pool. \n\niii) Then they choose the samples which the classifier predicts comes from the same domain. This probably succeeds as it is done on top of ResNet-101 features (that has seen all of ImageNet). This technique probably works possibly due to the ResNet being pre-trained with so many labeled classes. One baseline might have just been to choose the k-nearest neighbors (from unlabled pool)  to the average embedding of all the images of the chosen dataset. Since, it is quite easy for classifiers to detect from which dataset an image came from [6] it might be easy to choose an image from similar domain by using nearest neighbor in the embedding space. \n\nI am not sure how well their heuristic will work for an unlabeled pool that the classifier has never seen. Additionally, a portion of the dataset has been created by combining existing datasets. Since statistics of different datasets vary a lot artificially, the creation of an unlabeled pool by combining different datasets might work in the favor of the proposed heuristic of looking at \"domain distance\" to choose the samples.\n\n2) Effect of domain shift in SSL (Figure 4b) is studying an extreme case where the domain shift results in almost no common classes in the SSL training phase. It would make more sense to include datasets where at least some of the classes are shared so that self-supervised learning methods get to see some relevant classes. In practice, a self-supervised learning method would be applied on a large unlabeled pool of images. Hopefully with increasing diversity and number of images, there might be some images on which ding SSL helps the downstream few-shot task. Hence comparison with such a dataset like ImageNet/iNatrualist is important here.\n\nDecision:\nThe paper presents an well thought-out empirical study on self-supervised learning for few-shot learning. But there are  major concerns with the empirical setup and methods presented in the section where they propose a method to choose samples for SSL from an unlabeled pool of images. \n\n\nMinor Comments:\n1)  “In contrast, we humans can quickly learn new concepts from limited training data” - Remove we. \n2) “Despite recent advances, these techniques have only been applied to a few domains (e.g., entry-level classes on internet imagery), and under the assumption that large amounts of unlabeled images are available.” This is not true. Self-supervised learning methods have been used for continuous control in reinforcement learning[1, 2], cross-modal learning[3], navigation [5], action recognition[4] etc. Later on in related work the authors list many papers that use self-supervised learning in different contexts. This line should be modified to reflect how common self-supervised learning methods are in other fields as well and with less data.\n3) “making the rotation task too hard or too trivial to benefit main task” - add respectively.\n4) \"With random sampling, the extra unlabeled data often hurts the performance, while those sampled using the “domain weighs” improves performance on most datasets.\" replace weighs with weights\n5) [8] points out how self-supervised learning on a large dataset but with a domain shift (YFCC100M) is not as effective for pre-training as it is doing self-supervised learning on the downstream task's dataset (ImageNet). While it is a different setting it is inline with one of the main conclusions of the paper.\n\nReferences:\n[1] “PVEs: Position-Velocity Encoders for Unsupervised Learning of Structured State Representations” Rico Jonschkowski, Roland Hafner, Jonathan Scholz, and Martin Riedmiller.\n[2] “Learning Actionable Representations from Visual Observations” Debidatta Dwibedi, Jonathan Tompson, Corey Lynch, Pierre Sermanet\n[3] “Look, Listen and Learn” Relja Arandjelović, Andrew Zisserman\n[4] “Self-supervised Spatiotemporal Learning via Video Clip Order Prediction” Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, Yueting Zhuang.\n[5] “Scaling and Benchmarking Self-Supervised Visual Representation Learning” Priya Goyal, Dhruv Mahajan, Abhinav Gupta, Ishan Misra\n[6] \"Unbiased Look at Dataset Bias\" Antonio Torralba and Alexei A. Efros.\n[7] \"Boosting ´few-shot visual learning with self-supervision.\" Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick Perez, and Matthieu Cord. \n[8] \"Deep clustering for unsupervised learning of visual features\" Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze\n"
        }
    ]
}