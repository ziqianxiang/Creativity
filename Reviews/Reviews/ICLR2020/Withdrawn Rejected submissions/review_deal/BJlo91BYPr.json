{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The submission considers the irrationality in the inverse reinforcement learning. The authors claim that the irrationality can actually help to identify  the reward function, making the problem better defined. \n\nHowever, such a phenomenon is not really very surprising to me. The authors take the type of irrationality of the agent as something pre-specified in the model, which actually acts like some prior knowledge information. In other words, the author is rally posing more assumptions to the model, and to get a well-defined model. \n\nIt would be very interesting if the authors can actually identify the type of irrationality, or can deal with all the types of irrationality together in one model. At the moment, regarding the submission, I tend to reject the submission.  "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper studies reward inference from demonstrations of irrational experts. More specifically, a set of semantically meaningful experts' behaviors is considered which is derived from modifying the Bellman update. The quality of reward inference from these different experts is measured by two different scores and analyzed regarding properties of the demonstrator. The main finding is that irrationality can be helpful for inferring rewards.\n\nThe problem addressed by the paper is very interesting and relevant to a reasonable part of the community but I argue that the paper is not ready for publication in its current form. In particular, the experiments are too limited to thoroughly support the claims (this requires at least the consideration of more different environments; and to really make the paper impactful some parts of the \"future work\" section should be conducted) and the write-up should be improved from Section 3 onwards to provide more clarity.\n\nA few more detailed points:\n* I see the paper in its current form as a theoretical study on reward inference from irrational experts. To provide insights here, and as these only involves simulations, a rich set of different MDPs should be considered. In the current form it is unclear how general the results are (although I assume that certain findings hold more generally but there is no supporting evidence for that). Maybe even formal theoretical insights can be derived.\n* Regarding the Bellman update, on the RHS it should be $V_i(s')$.\n* Regarding the presentation of the irrational experts. Is there a simpler way of presenting the irrational experts through modified MDPs that the expert tries to solve optimally? Are all updates actually convergent, in particular the pessimistic one?\n* Please provide a formal specification of the reward model used in experiments.\n* Please provide a describe how you do the computation of the posteriors $\\theta$ in the main paper (or at least provide a forward reference to the appendix). Which prior on $\\theta$ are you using (put in main paper)?  \n* What is the precise nature of $\\xi$? I would assume it is a sequence of state-actions but that is not consistent with the definition of the log-loss which suggests it is only actions.\n* Probably more interesting than the log-loss and L^2 loss is the actual performance of an optimal policy using the inferred reward parameters. It would be good to report this numbers. How do irrational experts compare looking at this metric?\n* The discussion of related work should be extended. For instance, R. Shah et al.'s paper \"On the Feasibility of Learning, Rather than Assuming, Human Biases for Reward Inference\" should be discussed in more detail and similarities and differences clarified.\n\nMinor comments and suggestions for improving the paper:\n* The definition of Boltz in 2.2.2 can be made more clear. Maybe define the function using actions to connect to the above equation.\n* Correct \"update on the the trajectory $\\theta$\".\n* Please check the usage of $\\theta$ and $\\theta^*$ and make it consistent. I think it would also help to make the log-loss and L^2 distance not look like a function of $\\theta$.\n* Figure 4/5: Explain what we see. I guess the black square is the starting position?\n* Regarding figure 5: Comparing different $\\beta$ values seems more sensible if the norm of $\\theta$ is normalized. "
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "*First of all, TBH, I'm not very familiar with the field of reward inference / inverse reinforcement learning, my assessment to the paper is only based on educational guess.*\n\nThe paper proposes a surprising insight that reasoning about irrationality of human experts could help better reward inference, which is trying to get the reward function from human expert demonstrations. The authors start from Bellman update and try to add different modifications to the Bellman update to see how different types and levels of rationality affect the reward inference. The proposed modifications include adding Boltzmann sampling instead of taking a max, tuning different levels of optimism and pessimism, transforming the reward in specific way, change the discount into myopic discount or into hyperbolic discounting, and so on.\n\nThe authors conduct an experiment on a small 5x5 gridworld and find that most changes to introduce irrationality into the Bellman update have led to lower log loss than the rational expert, which demonstrates that the proposed irrationality methods are effective on reward inference. In Figure 4 and Figure 5, the authors show a few specific examples on how different types of irrationality and different levels of them could affect the policy's behavior. \n\nThe major limitation of the paper (the authors also mention it in Page 8) is that the proposed method is quite hard to generalize into many environments. The usefulness of the irrationality depends a lot on the environments and the authors may have to conduct more extensive experiments to make their claims solid."
        }
    ]
}