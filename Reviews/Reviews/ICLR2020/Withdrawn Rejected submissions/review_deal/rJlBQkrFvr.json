{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a new formulation for curiosity based on the expected reconstruction error of the next frame embedding, given the current frame + the next frame + the taken action. The experimental results are presented on 8 Atari games, including hard-exploration ones (Montezuma’s Revenge, Pitfall, and Private Eye). The baseline is chosen to be ICM.\n\nI am inclined to reject this paper. The key reason is that the experimental results are rather weak. From Table 1, ICM is better than the proposed method in 4 games, and the propose method is better than ICM in 4 games. It seems those methods perform on-par. Figure 2 also shows all methods perform rather similarly.\n\nDetailed arguments:\n[major concerns]\n* The experimental results currently do not convey the value of the method.\n* The choice of the baselines is not fully justified. For example, the \"Exploration by Random Network Distillation\" (RND) paper (Burda et al) is mentioned but not compared to. This is confusing because the point of RND was that it's not hurt by stochasticity -- which also seems to be one of the problems addressed in this paper. Another relevant baseline would be \"Count-Based Exploration with Neural Density Models\" (Ostrovski et al) -- it is known to show strong results on Atari and it also addresses exploration. Finally, \"Episodic Curiosity through Reachability\" (Savinov et al) also deals with environment stochasticity in a different manner -- by using episodic memory -- and shows good results in stochastic environments with added randomized TV.\n* Why are only 4 out of 8 datasets shown in Table 2?\n* The issue of the stochasticity is left rather un-analysed. What if the source of stochasticity is not sticky actions but rather some visual distractor? There are many such distractors explored in the prior work: random noise in the ICM paper, randomized TV in RND and Episodic Curiosity papers. The latter even considers different rules for switching the image on the TV: random image for any action or random image for one dedicated \"switch\" action.\n[minor concerns]\n* It is not fully clear what \"Perception-driven approach to curiosity\" section means.\n* It seems rather strange that both the embedding of the current and the next frames are given as the input to the encoder. Why would the method profit from the current frame embedding, why not give only the next frame as the input if in the end the comparison is done to the embedding of the next frame?\n\nPaper improvement suggestions:\n1) Stronger experimental results would help to convey the value of the proposed method.\n2) Adding stronger baselines would help to evaluate the impact."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The authors present a novel intrinsic reward/curiosity metric based on what they call Bayesian surprise. Specifically, the goal is that this approach should provide robust performance in sparse reward environments (as curiosity/exploration-based approaches often do) and in environments with stochasticity (which curiosity/exploration-based approaches can struggle with). The paper presents this perception-driven curiosity model, which is based on the difference between the predicted next state and the current state, and presents a series of experiments. The experiments compare the performance of agents trained on the intrinsic reward and some variations across games with both dense and sparse extrinsic reward. \n\nI like the approach and the paper is well-written. However, I find that the results of the experiments do not fully support the claims made in the paper. The authors point out themselves that River Raid, Pitfall, and Montezuma’s Revenge are all games with sparse extrinsic reward, where one would expect that the author’s intrinsic reward to have a clear benefit. However, only one game (Montezuma’s Revenge) sees any improvement, and that is a fairly minor one. Further, it is well understood from prior work that Montezuma’s Revenge benefits from exploration/intrinsic reward/curiosity. I would have expected at least some discussion of why these results did not match the expectations the authors had raised earlier in the paper. But ideally I’d like to see better results than this. Thus I lean towards a weak reject. \n\nBeyond discussing the incongruous results, the paper could also stand to contextualize or walk back some of the statements. For example in section 5.2 “We showed that our model explores more effectively than our baseline in cases where extrinsic rewards are sparse”. As I discussed above, this does not seem to be the case. \n\nThe paper would also benefit from additional baselines to better contextualize the results, how exactly this particular notion of visual novelty/perceptual-driven curiosity and bayesian surprise compares to others. \n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper presents a method to reward agents via an internal reward signal that encourages curiosity. The curiosity module uses a conditional variational autoencoder which enables the computation of Bayesian surprise. The Bayesian surprise is then used as an internal reward signal that is added to the usual external reward signal. This signal is supposed to help the agent learn the task at hand.  The authors provide experiments in high-dimensional settings comparing the proposed approach with  a competing approach (ICM).\n\n\nI vote for rejecting this paper mainly for the weak experimental validation that is unable to answer if the proposed method is more performant than the baseline ICM. \n\nSupporting arguments:\n\nThe main contribution of the paper as stated by the authors is to use a CVAE and Bayesian surprise. This approach differs from the baseline ICM since in ICM the intrinsic reward is the prediction error. So, the objective of the authors should be to show that, indeed, their idea of combining CVAE and using Bayesian surprise proposes some benefits compared to the baseline. Since this should be the main goal, strong experimental validations should be performed to support their proposed method.\n\nThe experiments have been performed only using 3 seeds. This would be enough if the results of the proposed method would be clearly better in terms of performance when comparing to ICM. However, the two methods seem to have extremely similar performances. For example in table 1 most, if not all, standard errors are overlapping for all methods. This highlights that the results might be the consequence of stochasticity in training/evaluation. See also for example, Figure 2, Riverraid, Breakout, Gravitar, BeamRider, Pitfall, and SpaceInvaders, all learning curves are extremely noisy and unstable. \n\nDue to this poor evaluation one cannot conclude if the proposed approach is beneficial or not. \n\nImprovements:\n\nIn order to improve the paper the authors should be able to at least experimentally validate (in a significant manner, e.g. more seeds or few seeds but highly performant when compared to the baseline) that the proposed approach has some benefits. Those benefits could be in terms of performance or any other characteristic that they might find relevant for the reinforcement learning problem.\n\n"
        }
    ]
}