{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper examines whether a pure language agent may interact within the 3D worlds (ViZDoom) by solely relying on text descriptions. The authors compare their approach with other input modalities such as RGB pixels, semantic segmentation, hand-crafted features. They observe that language representation is sometimes more suitable in some of the studied tasks. Besides, they also observe that the language models are robust to distractors.\n\nFirst of all, I like the initial motivation of the paper, and I think it is an excellent research direction: How language can be used as RL state representation? and more generally, how RL and language can be interleaved. The authors develop a meaningful language oracle that allows analyzing their research hypothesis. The paper contains some good complementary experiments, such as adding some tasks nuisance to assess language robustness or altering the spatial granularity of their textual representation. Finally, the authors provide the training hyperparameters and some interesting statistics such as the average number of words to describe the agent's state.\n\nUnfortunately, the paper has numerous imprecisions that drastically undermine my confidence in it. \n\nOn a mathematical side:\nThe reinforcement learning notations and equations are imprecise, and the surrounding explanations are not rigorous. \n     * PPO description is incomplete as the authors only wrote the policy gradient equation\n     * The notation is imprecise: \\gamma and  s' are not defined in the notation, the reward function is incorrect r(s) -> r(s,a)\n     * DQN implementation is lacking key details. What is the replay buffer? Do they use a target network? n-step return? Double-DQN? \n     * The implementation of PPO sounds strange, the authors mention two networks, while classic PPO is based on A2C with two heads (which is different). \n     * What is \\tilde{S} in Figure 2 right. In the paper, the author mention that the state is solely \\tilde{S}, while the figure assumes that \\tilde{S} is appended to S.\nIn other words, the reinforcement learning description and methodology looks a bit leaky. I would have appreciated more details and more rigorous descriptions. \n\n\nOn the experimental side:\n     * I am missing the logic while displaying Figure 5. Some experiments have nuisance, and other experiments have no nuisance, the ordering is also misleading. In the end, It gives the feeling that the figures are cherry-picked. If it is not the case, please add the missing experiments in the Appendix + explain your choice in the paper.\n     * The authors mention that the PPO algorithms are in the Appendix, but there are only three cherry-picked experiments, which may look suspicious. Again, please add all the experiments or remove PPO.\n\n\nOn the model side:\n      * The authors implement their models over language and then computes the baseline with similar capacity to be fair. While it may first sound like a good idea, it is unfair as the baselines are not correctly tuned. RGB may require more capacities than convolution layers over word embedding.\n     * The author only mentions that they use pretrained word embedding n the Appendix while it is a crucial point. For instance, RGB models have to learn the representation from scratch while language models already have a good prior. It would be fair also to learn the word embedding from scratch\n     * Another fair baseline would be to use the parser input as a feature vector. Thus, you can assess whether it is the language model (i.e. word embedding) or the new hand-crafted features that explain the relative improvement\n\nIn the end, I find that proposed baselines not strong enough (even if some of them have scores in line with the literature.\n\n\n - On the writing side\n   *  The abstract is imprecise. While I agree the representation is a recurrent issue in DRL, it is weird to say that RGB is sub-optimal as it is raw information. Language is a nice complement to structure our knowledge and to interpret our perception (cf work on Embodiment or Grounded Cognition), but it is risky to solely rely on language too (cf Grounding symbol problem [3]). Besides, the paper is not using natural language but synthetic language (l5 abstract). Finally, the conclusion is a bit strong. For instance, the language models are not always better (cf health gathering)\n   * The introduction is a bit hard to follow, it alternates between discussions with representation learning, reinforcement learning, and language. When speaking about representation in different contexts, some citations are missing. There is a significant discussion on distributional embedding. Although it is an exciting question in NLP, word embedding is now well-spread, so there is not need to describe them in several lines. I would highly recommend the authors to re-work this section.\n   * The section choices are a bit surprising. While RL is a super short section, there is half a page to describe word embedding. It is also weird to spend half a page on image segmentation + adding a figure (1) when it is not the topic of the paper. On the other side, the VizDoom environment, its scenarios are well-explained, and the language oracle is well-explained. I am not sure to understand the interest of Figure 4 right. I would have preferred a sketch of the neural models.\n\n\nOn the discussion side:\n - I am missing a discussion on individual game results. For instance, textual representation seems helpful or Defend the Center but not for Health Gathering. Any intuition or observation?\n \nOn the related work side:\n - I am not sure whether the first paragraph is relevant for the current paper\n - In the second paragraph, I would mention instruction following and the recent survey on RL + language [4]\n\n\nRemarks:\n - I am not a big fan of the initial citation. Yet it is a personal opinion, and it has no impact on my analysis\n - When the average number of words is above 200 with heavy nuisance,  do you increase the size of the convnet\n\n\nConclusion:\nAlbeit some exciting research motivation, I cannot recommend paper acceptance for the following reasons:\n - There are too many missing experiments, and I need concrete proof that the figures were not cherry-picked\n - I think that the baselines are not totally fairs\n - The writing can be improved (key sections are too short, less relevant sections are too long)\n - The RL components are not rigorous enough\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary:\n\nThis work proposes to study the effectiveness of natural language for representing state in reinforcement learning, rather than using vision-based or featured-based representations. The intuition here is natural: typical means of representing state often include lots of redundant information in order to be a sufficient statistic for next state and reward predictions, whereas language can be far more concise (and often flexible). The primary focus of the study is an empirical evaluation of the learning performance of a benchmark algorithm (DQN) using four different kinds of representations in the VizDoom environment: image-based, feature-based, semantic-segmentation-based, and language-based. The natural language state representation is generated by a semantic parser iterating over patches of the frames of the game and extracting relevant information. Experiments involve training a DQN to play several different levels of Doom given each state representation, with different levels of \"nuisance\" added to determine the effect of confounding information in the state input. Results indicate that in several cases, the language-based representation leads to improved (higher value policy in fewer samples) performance. Additionally, a study is conducted to investigate the impact of the number of patches used in generating the language-state, which reveals that patch-discretization has essentially no impact on learning performance in the level tested (\"super scenario\").\n\nVerdict: The paper brings a fresh perspective to the problem of representation learning in RL---the empirical study is on the thinner side, but I find the initial insights and results to be of sufficient interest to the community to lean toward acceptance.\n\nMore Detail:\n\nI enjoyed reading this paper. The empirical study is interesting, but I suspect the paper would be stronger if at least some of the PPO results (beyond Fig. 7) from the Appendix could make their way into the main paper. Additionally, I see that: 1) the feature-based method was not tested with PPO, and 2) a smaller number of levels are reported for PPO than DQN. Why was this the case? I suspect a plot showing the average performance across all levels (normalized over max value/reward) for both PPO and DQN would be indicative, and would fit given space constraints. I have a few other questions about the experiments, which I leave below under \"Questions\". Evaluation on some environment other than Doom would also strengthen the claims of the work. In particular, I am wondering whether environments where the generated language cannot easily approximate a sufficient statistic of state, and instead induces partial observability due to ambiguity or lack of expressivity. VizDoom is a case where, with the semantic parser used, the language is effectively a full characterization of state, but we can't expect this to always be the case. Perhaps even an environment like breakout would be useful for exploring this phenomena; in breakout, describing the precise configuration of remaining blocks is unnatural. Since ambiguity and implicature are necessarily features of natural language, I am wondering what the impact of these kinds of properties might be on RL in general.\n\n\nQuestions:\n\tQ1: As alluded to above, in an MDP, a state by definition contains all information necessary to predict $R(s,a)$ and $T( \\cdot \\mid s,a),\\ \\forall_a$. Language, on the other hand, is notoriously ambiguous (semantically, phonetically, sometimes syntactically), and often contains more or less information depending on the context, the speaker, or the listener. I am curious: is language appropriate for representing state on its own, rather than informing state estimation? That is, I think of the setup presented as defining a POMDP with language as the observation. VizDoom seems like a case where natural language can very nearly exhaustively describe the state, but other environments might not allow for such a neat characterization of state. I am wondering what the authors think about this point.\n\n\tQ2: In Figure 5, are the shaded regions depicting 95% confidence intervals, one standard deviation, or something else?\n\n\tQ3: The natural language representation seems to struggle in the health gathering case: any idea why?\n\n\tQ4: It is surprising how poorly the feature vector variation performs across the board. Any thoughts as to why?\n\n\tQ5: Figure 7 (and the corresponding experiment) is very illuminating! I did not expect those results, but they were helpful to see. Do we anticipate this trend across all of the levels tested, or is it something unique to the \"super scenario\"?\n\nComments:\n\n\tC1: I would be curious to see how combinations of the features might perform.\n\n\tC2: I would suggest aligning the y-axes in Figures 6 and 8 (making them all have the same scale). It would be easier to compare the three approaches. On that note, it would also be helpful to use subcaptions to denote which is the Language/Semantic/Image based representation, as the text title is quite small. Alternatively making the text larger in the plots would help.\n\nTypos/Writing Suggestions:\n\tAbstract:\n\t- \"we observe, is through\"::\"we observe is through\"\n\n\tSec 2. (Preliminaries)\n\t- \"approach, to a deep learning\"::\"approach to a deep learning\"\n\t- \"a forth method\"::\"a fourth method\"\n\n\tSec. 4 (Semantic...)\n\t- \"environment weâ€™ve\"::\"environment we have\""
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This work advocates for using natural language instead of visual features to represent state in reinforcement learning. Using VizDoom as a testbed, the authors extract semantic states (e.g. how many entities of which type are where)  from the game. These semantic states are then edited using a grammar into language descriptions for the learning agent. The authors compare models that rely on these language representations to those that rely on raw pixels and semantic maps and find that for most cases, the language representations outperform visual representations.\n\nWhile the hypothesis that natural language representations is an effective way to parametrize the state space is interesting, the experiments in this paper do not test this hypothesis. This is due to the following reasons:\n\n1. The authors do not use \"natural\" language, since the language here is generated from a simple template.\n2. Because the language is not natural, they correspond to precise, unambiguous, state representations (e.g. there is no difference from the model's perspective whether the utterance is \"you have high health\" vs. \"feat['high health'] == 1\"), which fundamentally simplifies the learning process --- the vision-based models have to learn how to extract precise state representations, whereas the \"language\" models are given these precise state representations.\n\nThe second point is particular important. For many learning problems (including this VizDoom experiment in the paper), language descriptions do not occur naturally. There are some works that assume that the model can produce language descriptions, which it then uses to facilitate learning (see https://arxiv.org/abs/1806.02724). However, in this case, the language descriptions are being provided by humans. If I understand correctly, the authors are comparing models that learn from raw pixels (e.g. baselines) to models that are fed precise, hand-extracted states (e.g. proposed). The results then are not surprising, nor do they test the hypothesis. The takeaway for me from this paper is that the authors engineered a clever feature extractor for VizDoom, and show that the state composed from the extracted features are more sample efficient to learn from compared to raw pixel values.\n\nI also have some feedback regarding the writing:\n- The introduction is too verbose. The paragraphs are disjoint, with independent overviews on deep learning, semantics, and NLP. Out of the 5 paragraphs, only the last one talks about the content of the paper. The introduction does not go into any detail about what experiments were actually run, and what the results are."
        }
    ]
}