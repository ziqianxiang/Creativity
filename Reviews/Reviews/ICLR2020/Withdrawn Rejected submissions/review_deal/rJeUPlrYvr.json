{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #5",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper studies the Neural Network Pruning problem. It proposes an FNNP method, which incorporates an Adaptive Batch Normalization (ABN) based evaluation module to connect the accuracy of pruning candidates and their final converged accuracy. The experiments show that the ABN-based sub-net evaluation module facilitates fast and accurate neural network pruning.\n \nMy major concern of this paper is that it fails to state the motivation to adopt adaptive BN clearly. Based on the existing statements, ABN would be fast since it recalculates the statistics for BN based on a small part of training data. And It may be accurate since this evaluation module establishes a strong correlation between the sub-nets accuracies and their converged accuracy. However, the key reason why the global BN can cause week correlations and why adaptive BN can establish a strong connection is never pointed out. It is not convincing to simply experimentally evaluate the baselines achieve lower correlations to support the claim.  \n \nWith this fatal deficiency, although the results seem empirical, it is hard to evaluate the methodical contribution of this work. Other questions are listed as follows. \n \n1). The right panel of Figure 5 is ambiguous. Is the horizontal axis represents the number of Iters? Can you illustrate the distance results more clearly?\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This manuscript proposed a fast network pruning framework with an ABN-based evaluation module. The ABN module is capable of obtain the adapted mean and variance with retraining on a small sample of data. From the experiments, it is observed that the ABN module is able to catch strong correlation on test accuracy between the evaluated model and the fine-tuned model.\nFollowing are reviewer’s comments:\n1. The result is promising. The proposed evaluation method is fast and simple, achieving comparable performance on both efficiency and accuracy.\n2. What does the adapted mean and variance mean? I assume it is the domain gap (shift) between the vanilla network and the pruned network, and that is the main motivation of this work. But this point is not clarified in the manuscript.\n3. It is Figure 5 (left) rather than ‘Figure 4.2’ shows the correlation between iterations and accuracies.\n4. This manuscript is very poor written:\nOn page 4: duplicate ‘Figure 3’; duplicate ‘block’; ‘helps’ should be singular.\nOn page 5: duplicate ‘present’; ‘the smallest filters’ should be singular.\nOn page 6: ‘metirc’. ‘there’ but ‘their’, ‘beteween’, ‘each constraints’ and more."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper addresses to prune networks by evaluating candidate pruned networks using the adaptive batch normalization (ABN). While existing methods utilizes pretrained BN parameters for evaluating pruned candidates, ABN utilizes updated BN parameters by simply forwarding a subset of training data to update the BN statistics given some pruned structure. The authors observed that the evaluation metric based on ABN is more correlated with the fine-tuned accuracy compared to that of PBN. Finally, the authors present experiments for MobileNet and ResNet.\n\nOverall, I doubt the methodological contribution of ABN. I think that the main idea is quite similar to that of Liu et al., 2019b (the right figure of Figure 3). The proposed algorithm `fine-tunes’ only BN parameters while the algorithm in Liu et al., 2019b fine-tunes the whole parameters. Pruning entire one epoch as done in Liu et al., 2019b might be expensive, however, I believe that the training iteration could be adjusted given any computational budget. This is the main reason why my decision toward rejection.\n\nOther comments are listed below:\n-I am not clear about introducing a new correlation measure (5). Explicitly noting why the correlation based on accuracies as done in Figure 3 is not enough for the evaluation would be helpful.\n\n-For justifying (5), the authors mentioned that searching the top-1 model is NP-hard but the top-k model is not. I could not fully understand this statement due to the following reason: If the top-1 model refers the model generating the best test accuracy given pruned architecture, searching it is NP-hard but searching one of the top k best models would be also NP-hard. If the top-1 model refers to the top-1 accuracy (or class) of the fine-tuned model (i.e., Y[1]), then it would be easily computed (i.e., not NP-hard). More explanation on NP-hardness would be helpful.\n\n-In Section 3, there is a repeated argument `Figure 3 and Figure 3 respectively’.\n\n- In the second paragraph of Section 4.2, Figure 4.2 is referred but there is no Figure 4.2\n\n"
        }
    ]
}