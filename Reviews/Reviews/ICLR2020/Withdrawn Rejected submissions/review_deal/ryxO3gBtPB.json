{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes to distill the information from the original, larger dataset to a smaller dataset. The distilled dataset is computed by backpropagating through the gradient descent optimization procedure of a model using techniques of hyper-gradients. The author analyzes the lower bound of the size of the distilled dataset for linear regression when one gradient descent is used, which motivates the development of approaches using multiple gradient descent. In experiments, the authors compared to several baselines of distilling the dataset and showed that their approach has better performance. Additionally, the authors demonstrated the usefulness of dataset distillation in a continual learning setting.\n\nThe idea of distilling the information from a larger dataset to a smaller dataset can be potentially very useful in speeding up the training process of machine learning models and improving interpretability. This paper makes a nice first step on this direction. However, I cannot recommend accepting this paper because the proposed distilled dataset seems to strongly correlate with a specific model architecture and specific distribution of the weights of this architecture. This severely hurts the usability of distilled datasets. A dataset is model agnostic, and a distilled dataset should ideally also be model agnostic.\n\nThe significance of experiments (especially in section 4.1) is questionable for several reasons:\n\n- The experiments on fixed initialization are nothing but a sanity check. When the distilled dataset is computed from a model with a fixed initialization, the reported results are basically a training performance and have nothing to do with \"generalization\", and will be successful as long as the optimizer performs well. \n\n- The experiments on random initializations are limited. Unless I misunderstood, the random initializations all have the same distribution for models used in training and test. Since the training and test use the same distribution of random initial weights, it is naturally expected to see that models initialized with a fixed distribution of random weights perform well on the distilled dataset. For this reason, I don't think the experiments on adapting pre-trained models to another task with distilled dataset is meaningful. In practice, the distilled dataset will be used to adapt many models, and it is hard to believe that these models are all of the same architecture, and trained on the same dataset with different random weights. \n\n- It is possible that I miss the information, but the authors did not seem to mention how the 2000 different pre-trained models are obtained (even in the appendix). \n\n- The results in Table 2 are not that impressive. Simple baselines such as k-means++ can perform almost as good as the distilled dataset using hyper-gradients using randomly initialized models.\n\nSome suggestions on improving the paper:\n\n- Focus more on experiments that demonstrate the usability of distilled datasets, such as the continual learning experiment. It is more informative if all the comparison to baselines is done in the continual learning setting.\n\n- The analysis on linear regression is nice. To motivate the multiple gradient descent steps approach better I think it is worthwhile to also have some simple analysis on the lower bound when using multiple gradient descent steps for linear regression. The proof should be straightforward and comparing the lower bounds can be very informative.\n\n- In the title of 4.1.1, 'FRO' should be 'FOR'\n\n- The fourth line in the first paragraph of section 4.1.1 is ungrammatical. \n "
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Paper presents a sound method of compressing information about the dataset into a set of synthetic sample-target pairs, by the means of backpropagating through the optimisation process into the samples themselves (rather than to initial weights as MAML model would do). Due to optimisation similarities to MAML, it can suffer of similar issues - gradients vanishing as number of steps increases, paired with lack of representational power (in non-linear case) when the number is small. Despite these issues, authors present empirical evaluation on various tasks, both focusing on transfer and continual learning. It is an interesting \"discriminative replays\" alternative to \"generative replay\" models present in the literature (where \"discriminative\" denotes that the replay is not created to generate original data, but rather to achieve loss decrease directly).\n\nIf it was a journal submission, I would recommend \"minor revision\", thus I am voting for a weak accept of this paper.\n\n\nSome material/presentation concerns:\n\n# Overall\n\n- considering a \"fixed initialisation\" scenario is arguably not important, as if one could store the initial network weights, one could also just store \\theta* and have an empty \"synthetic dataset\" to train on. This setup makes perfect sense if authors were to consider multi-task learning, where from one initial theta, one needs to be able to reach minimum of various tasks. Given that this is not the focus of the experiments, I would suggest de-emphasis of these results, or even complete removal. The more practical setups described in the paper are of enough value.\n- please describe what shaded region in Figure 3 means.\n- please increase font in Table 2, it is very hard to read after printing.\n- please increase font in FIgure 2 (b), it is completely unreadable after printing.\n- Authors state \"We do not share the same learning rates across steps as later steps often require lower learning rates.\" but do not provide any details of how these are selected\n\n# Continual learning\n\n- Experiments lack strong continual learning baselines, such as 'Continual Learning with Deep Generative Replay\" (NeurIPS 2017), or subsequent \"Generative replay with feedback connections as a\ngeneral strategy for continual learning\" (arxiv), or \"Progress & Compress: A scalable framework for continual learning\" (ICML 2018). In general, the newest baseline comes from 2017.\n\n# Theoretical section (3.3)\n\n- The bound provided is to some extent trivial, thus I would suggest moving the derivation to the Appendix (and leaving a simple statement, as a Remark, for both existence of one-step-convergent dataset, and for the M >= D), as currently it is presented in a way that might suggest more fundamental result - doing so would improve readability of the paper and help authors reduce its length to recommended 8 pages. What authors are showing is essentially that in linear case, for arbitrary target theta*, one needs to be able to control each dimension, which is a direct consequence of assumed \"independence\" of features.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes to distill a dataset into a small set of artifical images, which can be used to then train a neural network. The proposed method of doing this is based on a meta-learning flavored formulation where a GD step is done on the synthetic images and then the loss computed  on the real images. The procedure appears to overfit to the initialization of the classification model and thus to resolve this the authors randomly resample the initialization throughout training. Experimental results compare the method to other coreset construction strategies and random sampling. Applications are also shown to continual learning.\n\nI think the general idea of this paper and the questions it poses are very interesting  and in fact a rather underexplored area however the experimental results are quite underwhelming. The original questions posed in the paper are not really answered (can we distill a dataset?). The experiments only demonstrate that an expensive procedure that trains a model can produce synthetic images that give higher performance than a randomly selected set of images. \n\nComments regarding experiments from Sec 4.1\n\nThe results indeed show the performance is greatly improved over using random images from the dataset, however obtaining 45% on CIFAR-10 would hardly be considered “distilling the dataset” as the full set of data yields performances of 90%+ on this task. I would be more interested to see how many samples are needed to distill the dataset such that it reaches reasonably performance. It is not completely clear why the authors didn’t do this kind of analysis but it might be due to the heavy computational cost of this method, in which case it would be good to discuss that. Similarly for MNIST the performance is quite low (effective use of regularization  and can yield similar performance with a 100 images), kmeans++ based selection gives quite close results to the synthetic procedure for MNIST. I think its also a bit misleading that the authors advertise the fixed initialization performance in the abstract, this performance in my mind is mainly useful for the purposes of analyzing the method.   \n\nI would also be interested to know if the coreset construction baselines can be improved. For example the kmeans++, why couldnt it be done in the feature space of a trained model (since synthetic method also trains a model). I am not an expert in the area of coresets but it would be good to know what is the state-of-the-art. A potentially interesting coreset consturction approach that also requires training a classifier can be found in this ICLR 2019 paper https://arxiv.org/pdf/1812.05159.pdf\n\n\nComments regarding experiments for the continual learning application:\nI posted a separate comment regarding this set of experiments, which I hope the authors can address. Currently it is not 100% clear what is done. For now I will go under the assumptions that the authors are ingesting the entire dataset for each new task, using this to get their distilled data. This seems like an unfair baseline to me for the GEM setup which sees each data point only once and a for a single iteration. The authors introduce an additional step which requires multiple iterations over the whole dataset to obtain the synthetic set of images. This may be more appropriate for comparison in another continual learning setting. \n\n\nA few other observations/suggestions\n- In my understanding the label of the synthetic data is currently fixed. It would be interesting if this can also be adapted or alternatively if a fixed soft label (e.g 0.3 one class 0.7)  can be assigned.\n- I would be interested to know if the target architecuture were different would the results change?\n\n---\nAddendum: After writing this review the authors have responded to my concerns regarding the CL experiments and have begun to recompute some of the experiments using a more rigorous comparison with promising results on permuted mnist. If the CL section is clarified in the revised version (I suggest besides more decriptions in the main text to perhaps add an explicit algorithm block in the appendix) and the more rigorous comparison (single epoch to signl epoch) extend to cifar100 I will consider increasing my score as the method will be competitive for CL.  I do still think the non-CL experiments (which make up the bulk of the paper are somewhat underwhelming as discussed in the review)"
        }
    ]
}