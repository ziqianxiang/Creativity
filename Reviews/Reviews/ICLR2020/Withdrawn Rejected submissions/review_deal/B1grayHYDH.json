{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes a regularizer on logit similarity to improve model robustness to adversarial examples. \n\nThe introduction needs a bit of work to separate the context of the present submission, the submission’s contributions, and the related work. There is also some overlap with content presented in Section 2. Some of the terminology could also be improved to remove ambiguity in the writing. For instance, adversarial examples are not constructed using “noise”. Because noise is used in Section 2 to refer to adversarial perturbations, it is not clear from the current write up whether later uses of the term “noise”, e.g., in Section 3, refer to an adversarial perturbation or random noise. \n\nRobustness claims are backed up by empirical validation, however the experimental setup falls short to provide sufficient evidence. To help the authors assess what evidence is sufficient, I would recommend consulting [https://arxiv.org/abs/1902.06705], which provides guidelines for the proper evaluation of robustness to adversarial examples. The report is more comprehensive, but some of the aspects that need to be improved include in particular:\n\n* The attack does not use sufficiently enough PGD iterations. \n* Transferability experiments are mounted with the FGSM only. \n* It is possible that the similarity between logits is a good regularizer against attacks that only target the final prediction of the model (i.e., the output of the softmax) but not adaptive attacks. One such possible attack would have the adversary generate an input that targets earlier layers of the model rather than the softmax. Considering such adaptive attacks would also contribute to strengthening the evaluation of robustness claims made here.\n\nAnother point of comparison that is missing from the evaluation are verifiable training procedures. For instance, PixelDP [https://arxiv.org/abs/1802.03471] and smoothing [https://arxiv.org/abs/1902.02918] would be good baselines to add to the experiments, at least for datasets where results are available (e.g., MNIST and CIFAR10). None of the approaches used in the evaluation are certifiable. \n\nNitpick on page 7: Gradient masking was introduced in [https://arxiv.org/abs/1602.02697] prior to Athalye et al."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper proposes a new training objective for increasing robustness against adversarial examples. The objective includes not only the standard cross entropy loss, but also the loss of the model over randomly perturbed inputs, and also includes a term that penalizes $\\ell_2$ distance between logits from perturbed images and their clean counterparts. The paper applies this technique to both increase robustness to $\\ell_\\infty$ adversaries and rotations/translations of the input image.\n\nThis paper has major evaluations that make it unfit for publication. In the paper’s comparison with previous baselines, evaluation produces incorrect results. This calls into question the strength (and implementation) of the attack, and therefore the evaluation of the entire proposed work.\n\nDetails:\n\nIssue with evaluation: The paper reports, with PGD steps=100 for CIFAR10 in the $\\ell_\\infty$ $\\epsilon = 8/255$ threat model, that the adversarial accuracies for ALP and the Madry et al defense range around ~94-95%. Neither of these are true, e.g. see [1] for ALP (this paper shows that the ALP defense is easily bypassed with just PGD) and [2] for Madry et al (this readme shows that 20 step PGD gives you ~48%).\n\n[1] https://arxiv.org/abs/1807.10272\n[2] https://github.com/MadryLab/cifar10_challenge#white-box-leaderboard\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper seeks to improve adversarial robustness by encouraging images that are close to each other to have similar representations. The authors do so by applying small random perturbations to images (they call such images perceptually similar) and adding a loss term to the representation layer. The authors then demonstrate that this method provides some improved robustness, but not as much as standard adversarial training. The authors are careful to do various evaluations to try to establish the validity of their results. \n\nI vote to reject the paper. The key reason is that prior work already uses the same idea that this paper uses, except the prior work has much better results. I am referring in particular to the work of Zhang et. al [1], first published in January 2019, which performs adversarial training on the representation layer. The method of Zhang et. al has far superior results to performing random noise-based training on the representation layer as this paper does.\n\nIf the goal is to enforce similarity of representations for all images within an Lp ball, I feel that the better way to do it is to enforce similarity of representations in the worst case (via adversarial training, as in Zhang et. al) as opposed to doing so via random noise augmentation. The intuition for this comes from the setting when we are only focused on the final logits and not the representation layer. In that setting, simply adding random noise to input images during training does not lead to good adversarial robustness, while adversarial training is much more effective at inducing adversarial robustness.\n\nI would also caution the authors against using the term “perceptual prior” when the authors are simply proposing that the representation layer should be similar when the input images are close to each other in some Lp-norm. The term “perceptual prior” hints at something much more general, while I would argue that this work is still investigating an Lp-ball smoothness prior.\n\nI do appreciate the thoroughness of the experiments that the authors performed, including the ablation tests, and I believe the paper was written fairly clearly.\n\nAdditional feedback:\n\n- One possible benefit of this method is that it is faster than adversarial training. The authors point out that scaling adversarial training to ImageNet is difficult - if this is the case, could you show that your method still works on ImageNet?\n- What do you mean by “over-fitting to adversarial samples” as a primary drawback of adversarial training in the introduction? This was unclear.\n- In equation (1), the superscript in f^j_clean is not explained.\n- Section 4.1 can perhaps be combined with section 3.1.\n- I also want to point out that ALP is a “broken” adversarial defense [2] because it is not robust to stronger attacks. Thus, this calls into question the results of Tables 2 and 3, where ALP still looks like an effective defense. Perhaps you can run PGD with more steps and more restarts (at least to the point where ALP no longer provides as much robustness) in your evaluations.\n- As a note, if you do PGD adversarial training on transformed samples (the way you do for random noise samples), you can also get robustness to transformation attacks [3].\n\n[1] https://arxiv.org/abs/1901.08573\n[2] https://arxiv.org/abs/1807.10272\n[3] https://arxiv.org/abs/1712.02779"
        }
    ]
}