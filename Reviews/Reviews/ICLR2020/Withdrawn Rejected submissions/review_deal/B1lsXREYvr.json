{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposed to use a compressive sensing approach for neural architecture search, similar to Harmonica for hyperparameter optimization. \n\nIn the discussion, the reviewers noted that the empirical evaluation is not comparing apples to apples; the authors could not provide a fair evaluation. Code availability is not mentioned. The proof of theorem 3.2 was missing in the original submission and was only provided during the rebuttal. All reviewers gave rejecting scores, and I also recommend rejection. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a new algorithm for one-shot neural architecture search (NAS) via compressive sensing. The authors propose a new search strategy, as well as a slightly different search space compared to DARTS [1], ProxylessNAS [2], etc. They use architecture samples from the one-shot model evaluated with the search parameters as a surrogate of the true objective in order to speed-up the search. Afterwards, these surrogate function evaluations are used to compute Fourier coefficients which are eventually used to optimize the vector of binary parameters encoding the architecture. \n\nOverall, I think the proposed algorithm is interesting and of practical usefulness. However, in terms of novelty, this work seems more to be an application of Harmonica [6] to the NAS problem (with small modifications in order to make it applicable). In page 10 you state some of the differences of your method with Harmonica. I agree that the number of function evaluations you use (coming from the one-shot model) is larger and computationally less expensive to obtain, however this does not guarantee that these are a good surrogate of the true objective that NAS aims to minimize, i.e. the validation/test accuracy of final (stand-alone) architectures . The empirical evaluations of their algorithm seem to outperform/be competitive compared to other NAS methods on all benchmarks used in the paper, however only DARTS is evaluated on their search space and the other results are taken from the corresponding papers. The paper is well-written and -structured with the caveat of being more than the recommended 8 pages.\n\nI will adjust my score depending on the authors responses concerning the following questions/issues:\n\n1. The correlation between the architectures evaluated using the one-shot weights and retrained from scratch, seems to be of crucial importance in your method, since you directly use the one-shot weights to collect the measurements, similarly to Random Search with weight sharing [3], ENAS [4] or Bender et al. [5]. What is the correlation of these measurements with the stand-alone architectures trained from scratch using the final evaluation settings? How did you tune the p in the Bernoulli distribution during the one-shot weight updates. According to Bender et al. [5] the ScheduledDropPath probability is an important hyperparameter affecting the aforementioned correlation.\n\n2. What is the main motivation for using 5 operations in the operation set and not 8 as in DARTS [1] for example? Does the main contribution in the competitive results come from the different search space or the search method?\n\n3. Is there any reference or proof for the correctness of Theorem 3.2?\n\n4. I think there are some parts that can be moved in the Supplementary, such as the pseudocode for the proposed algorithm or Figure 3, and some other parts that can be compressed, such as the Related Work section.\n\t\nReferences\n[1] Hanxiao Liu, Karen Simonyan, and Yiming Yang.  DARTS: Differentiable architecture search.  In ICLR, 2019.\n[2] Han Cai, Ligeng Zhu, Song Han. ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware. In ICLR, 2019.\n[3] LIAM LI, AMEET TALWALKAR. Random Search and Reproducibility for Neural Architecture Search.\n[4] Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, Jeff Dean. Efficient Neural Architecture Search via Parameter Sharing. In ICML, 2018\n[5] Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, Quoc Le. Understanding and Simplifying One-Shot Architecture Search. In ICML, 2018\n[6] Elad Hazan, Adam Klivans, Yang Yuan. Hyperparameter Optimization: A Spectral Approach\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Contributions:\nThis paper tackles the problem of One-shot Neural architecture search by proposing a new method. \nThe method consists mainly of new search strategy of the optimal architecture that is inspired by the recovery of boolean functions from their sparse Fourier expansions. As such, this work is an application of recent progress in the field of compressive sensing to One-shot neural architecture search. Given the problem formalism, the authors have also provides guarantee for the optimality of their method, i.e the method can recover the optimal sub-network of any given  a sufficient number of performance measurements.\n\nClarity\nOverall, the paper is well motivated and the technical content is good. That said the structure could be enormously  improved to ease the reading and the overall understanding. For example: better caption for Figure 2 explaining what is shown; presenting the pseudo-code directly in the method overview and spending the rest of the section explaining the method; showing the related work before the experiments; etc.\n\nNovelty\nThe main novelty in my opinion is the application of compressive sensing methods to One-shot NAS. This approach is significantly different from other One-shot NAS method that I am aware of mainly regarding the search strategy employed to find the best architecture. \nHowever, this work seems like an incremental improvement over Hazan et al 2018. To this regard, the only novelty that was the framing of One-shot NAS as a recovery of boolean functions from their sparse Fourier expansions is not new either.  That is said, I am open to be proven wrong on this point!\n\nResults\nThe experiment section is not self-content, the readers is refered a couple of times to other papers to get details that are  critical to reproducibility and understanding. \nOverall, the search strategy of CoNAS seems  parameter efficient, fast and competitive. Also, small ablation studies showing the effect of the different parameters of CoNAS were very informative and well-appreciated.\nHowever,  the search space is different between CoNAS and the others methods for some experiments, making it difficult to decide if the search strategy of CoNAS is definitely competitive compared to other methods or not.\n\nPoints of improvement:\n1 - Structure of the paper\n2 - Clarify novelty compared to HARMONICA ( not the application domain please)\n3 - demonstrate that with the same search space your method is competitive.\n4 - Does m=1000 in your experiments satisfies theorem 3.2? What is the value of d in your experiments? Can you provide supporting experiments that answer those questions?\n\nPreliminary decision:\nFor now, I will say *weak reject* \n\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1050",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "In this paper, the authors study Neural Architecture Search, which aims to automate design of neural network models. Their approach consists in using a two stage algorithm:\n-\tA first Neural Network $f$ is trained for predicting the performances of sub-architectures. Then binary sub-graphs coding the sub-architectures are uniformly sampled (Bernouilli(0.5)), and their performances y are evaluated thanks to the first Neural Network.\n-\tThe graph sampling matrix A, which is indexed by the m sampled architectures and the Fourier basis of size $O(n^d)$, is built. Then the optimization problem $x^*= \\arg\\min_x ||y â€“ Ax||$ is solved using Lasso. The largest Fourier coefficients are chosen to build an estimate g of f. Finally, computing minimum of g, the architecture is generated. \n\nSince $m << n^d$, the optimization problem is ill-posed. Theorem 3.2 shows that if A satisfies the restricted isometry of order s, then the sparse coefficients x can be recovered.\nThe algorithm is evaluated and compared to the state-of-the-art on various image classification tasks and on RNN.\n\n\nMajor concern:\n\n1/ I did not find the proof of Theorem 3.2 in the main paper and in the appendix, so I do not buy it.\n\n2/ The authors claim that their algorithm performs better than the state-of-the-art, but according to tables 1,2,3, I did not find significant differences of performances in term of test errors.\n\n3/ The key idea of the algorithm is not well explained. A one-shot NAS f is pre-trained. f is assumed to be well-trained. Then it is approximated with a Fourier-sparse Boolean function. Why using an approximation if f is perfect? Do you expect to reduce the needed number of sampled architectures?\n\n\nMinor concerns:\n \nEquation 3.1 is not clear. $X_S (\\alpha_l)$ does not depend on k. So all rows seem to be the same.\n\n"
        }
    ]
}