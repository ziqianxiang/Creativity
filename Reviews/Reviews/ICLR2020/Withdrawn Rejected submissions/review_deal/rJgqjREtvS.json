{
    "Decision": {
        "decision": "Reject",
        "comment": "All three reviewers agreed that the paper should not be accepted. No rebuttal was offered, thus the paper is rejected.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper presents a single image super-resolution method. The discussion refers to sparse coding as motivation and how such coding can be achieved within a neural network using activation functions, and then it slides into iterative soft thresholding and ends up with the observation that ReLU and the soft nonnegative thresholding operator are equal (paper should have pointed that they are equal with slight difference around the bias term). This argument is not new, this conclusion has already been known and analyzed extensively in (Papyan et al., 2017a).\n\nThe proposed network is not much different from what several other SR methods used before (see NTIRE 2019). It is basically composed of a collection of single layer residual units that share the same parameters. As expected, there is also a skip connection from the input to the last layer. The only slight variation is that the activation function ReLU for the residual layers are arranged before the convolutional layers. \n\nTraining details are missing. \n\nThe paper does not provide any comparisons with the top-ranking methods in the NTIRE 2019 single image super-resolution challenge leaderboard. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This work investigates the problem of super-resolution using\nconvolutional sparse coding (CSC). The work is motivated\nby a reduction in optimization time stating that the\nstate-of-the-art CSC solvers are slow and require to\nput the entire image in memory. This is not true,\nthere exist online approaches such as https://arxiv.org/abs/1706.09563\nor distributed methods https://arxiv.org/abs/1901.09235.\n\nThe use of an unfolding algorithm similar to LISTA is\nnot clearly motivated and explained, especially concerning\nthe training procedure when working in the SR context.\n\nFinally the latex formatting suffers from many issues and typos.\nBe careful with difference between \\citet and \\citep for citations.\n\nFonts in figure 4 are too small\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This work exploits the natural connection between CSC and Convolutional Neural Networks (CNN) to address CSC based image SR. Specifically, Convolutional Iterative Soft Thresholding Algorithm (CISTA) is introduced to solve CSC problem and state-of-the-art performance is achieved on popular benchmarks.\n\n[Strengths]\n- This paper is well-written and easy to follow.\n\n[Weaknesses]\n- My main concern on this work is its novelty seems to be limited. In Introduction, the authors raised 4 kinds of issues, including framework/optimization/memory/multi-scale. However, optimization and memory issues are mitigated by the CNN architectures; multi-scale issue is addressed with the help of work from Kim et al.. It seems the proposed questions are mostly addressed by the previous methods.\n\n- It seems CISTA may be the main novelty of this work. However, no comparasions with previous ISTA methods (e.g., [R1-3]) are conducted and discussed. \n\n- From Tabs. 2&3, the improvement is very limited. Besides, time complexity needs to be compared with the competitors, including flops or inference times.\n\n[R1] A Fast Proximal Method for Convolutional Sparse Coding, IJCNN 2013.\n[R2] Convolutional Neural Networks Analyzed via Convolutional Sparse Coding, JMLR 2017.\n[R3] On Multi-Layer Basis Pursuit, Efficient Algorithms and Convolutional Neural Networks, TPAMI 2018."
        }
    ]
}