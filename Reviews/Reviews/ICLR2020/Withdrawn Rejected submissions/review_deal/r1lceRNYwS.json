{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents how we can learn video embeddings with unsupervised learning, proposing Video Instance Embedding (VIE). With experiments on public benchmark set and retrieval demo, the authors argue that the proposed method is effective.\n\n1. The authors claim that 'inherently different' videos pushed far apart, while similar videos are aggregated. How does this 'inherently different' or 'similar' is defined? It seems based on the visual signals only, as it relies on unsupervised learning. Some related videos may be visually different, but still may be quite relevant to each other, and vice versa. It may be nicer to discuss about what 'similarity' this paper is trying to learn, as basically this model is for video metric learning.\n\n2. It may be interesting to compare against other self-supervised learning approaches. For instance,\nObjects that Sound (Relja Arandjelović, Andrew Zisserman) https://arxiv.org/abs/1712.06651\nIn this paper, the authors propose a model capturing audio-visual correspondence.\n\n3. It will be also interesting to see experiments on larger dataset such as YouTube 8M or Sports 1M. These dataset do not provide access to the pixels though; so I am not sure how hard it is to extract data from YouTube and process them. The action is an area that this kind of model can take advantage mostly, but many other videos may not like that. So, evaluating on general video dataset like YouTube 8M may be useful.\n\n4. The embedding size is not shown. It'd be nicer to see comparison with different embedding sizes.\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Contributions: This submission proposes a video instant embedding (VIE) framework for self-supervised video representation learning. It applies the training objectives from (Wu et al 2018b) and (Zhuang et al 2019), and studies the impact of different CNN architectures and sampling strategies, which are specific to videos. When using the same CNN backbone, the proposed method achieves higher performance than previous approaches, such as rotation.\n\nAssessment:\n- The proposed method achieves state-of-the-art performance on self-supervised video classification benchmarks (UCF and HMDB), and compares different video CNN backbones and data augmentation strategies, the empirical evaluations are very useful for researchers working on this topic.\n- The proposed method is an application of previous published methods (Wu et al 2018b; Zhuang et al 2019), using instance retrieval and local aggregation objectives (the latter shows little empirical gain) on videos. The implementation details, such as the use of memory bank, and different data augmentation strategies are useful, but has very limited novelty.\n- Despite the state-of-the-art performance, the empirical evaluations can be improved. For example, the impact of different training objectives could be studied further, and apple-to-apple comparison with many recently closely related work, such as CMC, CPC, DeepInfoMax, could be provided (the only such comparison is between ST-puzzle and VIE, when using 3DResNet, so it is hard to understand where the performance gain comes from).\n\nOverall, the submission proposes an interesting and state-of-the-art method for self-supervised video representation learning, but its technical contribution and empirical evaluation are limited. I therefore recommend weak reject."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary\nThe paper applies an existing self-supervised method (Local Aggregation) on videos that was initially developed with images. The paper mainly studies how effective the video representations are for 2 downstream tasks (action recognition and image recognition) with different video architectures and frame sampling strategies.\n\nStrengths\n1) Their method achieves a significant improvement in performance over baseline self-supervised video representation learning methods on 2 datasets: UCF101 and HMDB51\n2) The paper presents though experiments with different architectures and training settings in order to be fair to the baselines.  \n\nWeakness\n1) Comparison to other self-supervised methods trained on Kinetics data but tested on ImageNet are missing. The problem is that the authors state that the features learnt with VIE are competitive for ImageNet classification but there are no other baselines to compare their approach with. Most self-supervised methods on video do not provide results on ImageNet classification. So it is understandable why they don't have these metrics but without them it is difficult to place the importance of their method for the ImageNet classification task.\n2) There are some claims in the paper that remain unsubstantiated. The only form of validation for these claims is the final performance of the downstream task which does not necessarily validate the claims in the paper. For example:\n\ni) \"Benefit from long-range temporal structure\" In this section the authors train the model with temporally clipped videos by dividing the videos into N bins. They compare it with models trained on full videos and conclude that because performance with entire video on downstream tasks is higher, their training mechanism is capturing long range temporal structure present in videos. It is unfair to compare these two models as they both are trained with different amounts of data. The simplest explanation seems that the model that sees full videos sees more data. It is not clear from this experiment if the long-range temporal structure is benefiting the task or more data is benefiting it. \n\nIt is difficult to isolate the source of the performance improvement in this manner. But it might be more easier to test performance on a downstream task that requires some temporal reasoning[3, 4] and check if the embeddings are indeed capturing the structure.\n\nii) \"However, the relatively high performance of the static and two-stream models shows that VIE can achieve useful generalization, even when train and test datasets are as widely divergent as Kinetics and ImageNet.\" This claim is based by comparing the supervised learning and self-supervised learning on Kinetics and then testing on ImageNet. The last layers are usually learning more task-specific features which is why transfer learning performance is worse for Conv5 (Table 5 in Appendix). But Conv4 features are still generalizing well (41.13 is best for VIE-TRN) for the supervised learning models. These are still better than all the ImageNet numbers of VIE-TRN. I see that VIE-TwoStream numbers are much better but he comparison with supervised learning counterparts is not fair as VIE-TwoStream is 2 different models clubbed together and all the Supervised Learning baselines are single model.\n\nDecision\nTheir method showcases the power of the Local Aggregation training algorithm for unsupervised representation learning. Their approach scales well to videos and their paper has interesting experiments. But in the present version there are some unsubstantiated claims in the paper.  \n\nMinor comments:\n\n1) \"has remained a significant artificial intelligence challenge\" - Too informal a description for an academic paper.\n2) \", showing substantially improving on the state of the art \" - improvement over\n3) \"early modern” - please rephrase.\n4) Use of the term TwoStream is confusing as most literature in action recognition[1,2] refers to using RGB and flow as two streams. Two-stream models in this paper refer to using 2 base networks for extracting embeddings. As the two-stream terminology is already used in many papers in action recognition, please rephrase this to something less ambiguous.\n\nReferences\n[1] \"Two-stream convolutional networks for action recognition in videos\" Karen Simonyan, Andrew Zisserman.\n[2] \"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset\" Joao Carreira, Andrew Zisserman.\n[3] \"The \"Something Something\" Video Database for Learning and Evaluating Visual Common Sense.\"1\nRaghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, Florian Hoppe, Christian Thurau, Ingo Bax, Roland Memisevic.\n[4] \"Scaling Egocentric Vision: The EPIC-KITCHENS Dataset\" Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, Michael Wray.\n"
        }
    ]
}