{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a method for reinforcement learning with unseen actions.  More precisely, the problem setting considers a partitioned action space.  The actions available during training (known actions) are a subset of all the actions available during evaluation (known and unknown actions).  The method can choose unknown actions during evaluation through an embedding space over the actions, which defines a distance between actions. The action embedding is trained by a hierarchical variational autoencoder. The proposed method and algorithmic variants are applied to several domains in the experiments section.\n\nThe reviewers discussed both strengths and weaknesses of the paper.  The strengths described by the reviewers include the use of the hierarchical VAE and the explanatory videos.  The primary weakness is the absence of sufficient detail when describing the solution.  The solution description is not sufficiently clear to understand the details of the regularization metrics.  The details of regularization are essential when some actions are never seen in training.  The reviewers also mentioned that the experiment analysis would benefit from more care.\n\nThis paper is not ready for publication, as the solution methods and experiments are not presented with sufficient detail.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper deals with the problem of how to enable the generalization of discrete action policies to solve the task using unseen sets of actions. The authors develop a general understanding of unseen actions from their characteristic information and train a policy to solve the tasks using the general understanding. The challenge is to extract the action's characteristics from a dataset. This paper presents the HVAE to extract these characteristics and formulates the generalization for policy as the risk minimization.\n\nStrengths:\n1. This paper shows us how to represent the characteristics of the action using the a hierarchical VAE.\n2. From the provided videos, we can directly observe the results of this model applied to different tasks.\n\nWeaknesses:\n1. In the paper, the authors mentioned that they proposed the regularization metrics. However, they didn't describe them in details. It is important to develop the proposed method in theoretical style.\n2. Analyzing the regularization metrics should be careful in the experiments.\n3. Since there are many previous works related to action representation, the experiments should contain the comparison with the other method to see how much improvement was obtained."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper studies the problem of generalization of reinforcement learning policies to unseen spaces of actions. To be specific, the proposed model first extracts actions’ representations from datasets of unstructured information like images and videos, and then the model trains a RL policy to optimize the objectives based on the learned action representations. Experiments demonstrate the effectiveness of the proposed model against state-of-the-art baselines in four challenging environments. This paper could be improved in the following aspects:\n1.\tThe novelty of the proposed model is somewhat incremental, which combines some existing methods, especially the unsupervised learning for action representation part that just combines methods such as VAE, temporal skip connections…\n2.\tSome components of the proposed methods are ad hoc, and are not explained why using this design, such as why Bi-LSTM for encoder and why LSTM for decoder.\n3.\tMore definitions about the model should be offered, such as “y^∗ is some optimal action distribution”, how to get the optimal action distribution?\n4.\tSome datasets are not sufficient enough for sake of statistical sufficiency, such as recommendation data with only 1000 action space.\n5.\tThe contributions of action regularizations are not validated on experiment section.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper addresses the very interesting problem of generalising to new actions after only training on a subset of all possible actions. Here, the task falls into different contexts, which are inferred from an associated dataset (like pixel data). Having identified the context, it is used in the policy which therefore has knowledge of which actions are available to it. \n\nIn general, this paper is written very well, with some minor suggested tweaks in the following paragraphs. The key strategies used here (HVAE to identify contexts, ERM as the objective, entropy regularisation, etc) all make sense, and are shown to work well in the experiments carried out. \n\nWhile the experiments are sufficiently varied, it worries me that only 3 or 2 seeds were used. In some cases, such as NN and VAE in the CREATE experiments show large variances in performance. Perfects a few more seeds would have been nice to see. This is the key reason why I chose a 'Weak Accept' instead of an 'Accept'.\n\nSome of the results (the latent spaces) shown in the appendix are very interesting too, particularly since they show how similar actions spaces cluster together in most cases. \n\nMinor issues: \n\n1) In Figure 3, I am not clear about what 'im' and 'gt' settings are.\n2) In Figure 3, it would have been nice to have consistent colors for the different settings.\n3) It would have been nice to see the pseudocode of the algorithm used.\n"
        }
    ]
}