{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper presents a new graph pooling method, called HaarPooling. Based on the hierarchical HaarPooling, the graph classification problems can be solved under the graph neural network framework.\n\nOne major concern of reviewers is the experiment design. Authors add a new real world dataset in revision. Another concern is computational performance. The main text did not give a comprehensive analysis and the rebuttal did not fully address these problems.\n\nOverall, this paper presents an interesting graph pooling approach for graph classification while the presentation needs further polish. Based on the reviewers’ comments, I choose to reject the paper.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper presents a new approach called HaarPooling in the context of Deep Graph Neural Networks. The approach solves dmiensional problems of applying the same model on graphs of different size and shows how to contribute to high performance in a set of graph classification tasks, while having low computational complexity.\n\nThe paper is very well written. The authors describe the concept of HaarPooling in a very detailed way and show the mathematical foundation of their approach. They further provide detailed mathematical explanations and proofs for the claimed advantages of their method, whilst using descriptive examples to ease understanding. The use of HaarPooling is then tested on five different datasets, showing a very good performance on each of them. The description of the machine learning experiments is very detailed, i.e., the methodology seems reproducable.\n\nMinor Comments:\n- In Figure 3 it seems counter-intuitive that the Graphs are arranged down-top (0-->2), whilst the equations above are arranged top-down.\n- The Related Work section could be placed earlier in the paper to get a better overview of the context and the problems that HaarPooling tries to solve.\n- All the experiments use exactly one HaarPool layer. Would it be possible to use multiple HaarPool layers, or is this not sensible or even possible (e.g. due to the dimensionality reduction). An explanation on this could be beneficial.\n- In Appendix A, weights are described with a capital 'W', which seems inconsistent with the lower case 'w' used in the rest of the paper. \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper presents a new graph pooling method, called HaarPooling. HaarPooling has a mathematically formalism derived from compressive Haar transforms. HaarPooling takes into account both graph the structure and features of the graph-structured input data to compute a coarsened representation. HaarPooling can be applied in conjunction with any type of graph convolution in GNNs. Experimental results verified the efficacy of the proposed method.\n\nThe writing, organization and presentation are satisfactory. \n\nMy comments regarding this paper are as below.\n1) More experiments on real tasks (e.g., multi-human parsing or pose estimation, etc.), both quantitatively and qualitatively, should be supplemented to further verify the superiority claimed in this paper.\n2) The main contributions of this paper are not clear to me, compared with other SOTAs."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper investigates the problem of graph classification using neural networks and suggests a hierarchical approach for constructing a feature vector describing a whole graph via the use of the compressed Haar transform. The general method utilizes a hierarchical chain of coarsened versions of the graph (group multiple nodes into a parent 'node')  where the coarsening is achieved via spectral clustering. After obtaining the graph chain, at each level the authors apply a GCN followed by a HaarPooling, namely applying a lossy Haar Transform compression to get a representation for each cluster. \n\nOverall graph classification and regression tasks are quite important and this work provides a new way of transitioning from node based learning methods to full graph representations via hierarchical compression. Nonetheless, I find that the organization of the paper needs additional work and the experimental investigation is not sufficient and compelling enough. So I do not think the current paper is ready for publication in a top-tier venue like ICLR yet.\n\nIn particular I find the discussion of the HaarPooling step's computational performance not particularly appealing since other parts of the hierarchical approach are computationally expensive for example, the spectral clustering or GCN steps are costly. It also seems that the discussion of the paper is more on the computation and viability of the compressive Haar transform than it is about using it as a compressor as part of a larger hierarchical system. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary\n\nThis paper proposed a new graph pooling method based on the Haar basis on graphs. The authors argued that existing graph pooling methods have drawbacks (ignored node features, ignored the hierarchical structure of a graph or, computationally expensive in terms of space or time) and that the proposed method overcame these drawbacks. The proposed method extracts the low-frequency components of signals in terms of Haar decomposition on a chain of graphs. By the property of the Haar basis, we can compute the pooling matrix with the complexity proportional to the graph size. The paper empirically compared the proposed method with state-of-the-art pooling methods and graph NNs using well-known datasets.\n\n\nDecision\n\nAlthough this paper gave a novel pooling method by incorporating the Wavelet theory, I still have questions for the effectiveness of the proposed method in the real datasets and whether the proposed method solved the problems of existing methods the authors mentioned in the introduction (see the Suggestion section). Besides, I think the authors can improve the organization of the paper to maximize the value of the paper. Therefore, I judge the paper as a border, tending to reject for now.\n\n\nSuggestions\n\n- The main part of the paper is 10 pages long. I think the authors can polish the organization of the paper to fit the recommended page size (i.e., 8 pages).\n- I want to know how many times the authors ran experiments for each configuration.  I think the variance of test accuracies are critical since the difference in performance between methods is not significant. Adding error bars to the experiment results are preferrable if authors ran experiments multiple times.\n- The authors claimed in the introduction that the drawback of existing methods is their time or space complexity and that the proposed method is computationally inexpensive. I think it is better to emphasize that HaarPooling does solve the questions. Although the authors demonstrated that the proposed algorithm is fast compared to the naive implementation, the comparison with other pooling methods are missing. For example, adding a comparison table in terms of time and space complexity of pooling methods is one idea. Another idea is to demonstrate it empirically by providing time and memory consumption to the experiment results.\n\n\nMinor Comments\n\n- Introduction\n\t- Haar Pooling is computed following a chain... → Haar Pooling is computed by following ...\n- page 1, section 1, paragraph 1\n\t- Graph classification and regression are a very different kind of task\n\t- → At first sight, I have thought that this sentence discusses the difference between classification and regression tasks. Could you reconsider the sentence?\n- page 3, section 2, paragraph 1\n\t- The authors used the term \"chain,\" whose definition is available in the later section (Sec. 3). I think this terminology is not common in the literature of graph NNs (at least I did not come up with the definition from this term). Could you consider to add the definition (or brief explanation) of the term when you use it for the first time?\n- page 5, section 3, Chain of graph by clustering\n\t- Write what $w$ is in the definition of a graph $\\mathcal{G}=(V, E, w)$.\n- page 5, section 3.1, Orthogonality\n\t- Define $l_2(\\mathcal{G}_j)$ (I can imagine its definition, though).\n- page 5, section 3.1, Haar basis\n\t- For two consective layers $j$, $j+1$ ...\n\t- → I had a little difficulty in understanding this sentence. Could you reconsider the wording?\n-  page 13, Appendix B, (10)\n\t-  I think \"1\" in this equation is an all-one vector. It is better to write differently the all-one vector and the scalar one.\n-  page 5, section 3.1 and page 7, (8)\n\t-  For the node $v$ in the $j$-th graph and $v'$ in the $(j+1)$-th graph, the authors use both of $v\\in v'$ (section 3.1) and $v'\\in Pa_{\\mathcal{G}}(v)$ (equation (8)) to denote the parent-child relationship. It is better to align the notation.\n- page 13, Appendix B\n\t- Could you write in the appendix how to construct the compressed Haar from the full counterpart (I suppose we should choose bases corresponding to $k=1$)?\n\n\nQuestions\n\n- In the paragraph starting with Locality on page 5, the paper says that the number of different values of the Haar basis is bounded by constant. I want the authors to make this sentence more precise. To be more specific, what \"the different values\" means (I imagine that it means the number of scalar values appearing as a component of some basis) and what \"constant\" means (with respect to which variable the \"constant\" is constant?)\n- I think that the complexity of the direct matrix product is $O(N^2C)$, where $N$ is the graph size and $C$ is the channel size (correct me if I am wrong). Therefore the result of Figure 5 was surprising to me because the direct matrix product takes approximately $O(N^3)$ time.  These look inconsistent to me unless the channel size $C$ is proportional to the node size $N$.\n- Equation (5) claimed that the compressive Haar basis approximately keeps the length of an input signal. I want to know how we can justify it (theoretically or empirically).  I think that it is empirically correct for the Fourier basis that signal carries \"information\" in the low-frequency domains and that high-frequency areas are noisy (e.g., NT and Maehara (2019); Ma et al., 2019). If this is the case for Haar transform, too, I understand that equation (5) holds and gives the justification that the proposed method can extract the information from lower frequencies."
        }
    ]
}