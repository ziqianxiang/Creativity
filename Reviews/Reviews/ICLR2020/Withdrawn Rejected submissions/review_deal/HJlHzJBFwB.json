{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes to speed up Bayesian deep learning at test time by training a student network to approximate the BNN's output distribution. The idea is certainly a reasonable thing to try, and the writing is mostly good (though as some reviewers point out, certain sections might not be necessary). The idea is fairly obvious, though, so the question is whether the experimental results are impressive enough by themselves to justify acceptance. The method is able to get close to the performance achieved by Monte Carlo estimators with much lower cost, although there is a nontrivial drop in accuracy. This is probably worth paying if it achieves 500x computation reduction as claimed in the paper, though the practical gains are probably much smaller since Monte Carlo methods are rarely used with 500 samples. Overall, this seems a bit below the bar for ICLR.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper studies the problem of avoiding Monte Carlo (MC) estimate for the predictive distribution during the test for Bayesian methods. MC estimate will incur multiple passes where the number of passes depends on the number of samples and therefore the cost can be huge. The authors propose One-Pass Uncertainty (OPU) methods to approximate the predictive distribution through distillation. Experiments on Bayesian neural networks are conducted to demonstrate the proposed method.\n\nQuality:\nThe proposed method appears to be technically sound. The view of approximating the predictive distribution over simplex is interesting and may inspire future studies under this formulation. Although the restriction of the student distribution to be tractable seems to limit the design of the student model significantly. And this restrictive distribution family may cause large amortization error, as suggested by Lemma 1 in the paper.\n\nThe experiments are well-conducted, and the proposed method is well-evaluated.\n\nSignificance:\nThis paper studies an important problem in Bayesian machine learning and the proposed method can be combined with many Bayesian methods to reduce the computational cost during the test. \n\nOriginality:\nAs far as I know, the method is novel. The related work is adequately cited. \n\nClarity:\nThis paper is well-written and easy to follow. \n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "Thank the authors for your detailed rebuttal. I agree with the authors that the proposed method acts as a useful tool for \"real-time evaluation of induced predictive uncertainty\", and the experiments also validate that the method indeed achieves comparable performance with smaller computations. But for now, I am inclined to not change my score.\n\n###################\n\n\nBayesian models maintain the posterior distribution for predictions, which might bring up big computational costs of multiple forwards or big memory costs of multiple particles. To resolve the computational and memory issues at predictions, this paper proposes to distill Bayesian models into an amortized prediction model, avoiding the original multiple forwards. Specifically, in classification, they distill the predictive probabilities into an amortized Dirichlet distribution. They evaluated different distillation metrics, including KL divergence, Earth moving distance, and Maximum mean discrepancy.  Empirically, they evaluate the proposed method over out-of-distribution detection. They demonstrate that their method achieves comparable performance with much speedup.\n\nStrengths, \n1, This paper is well-written and the ideas are well-presented. They evaluated the proposed method over different Bayesian models (MCDP & SGLD) as well different metrics (KL, EMD, MMD), and demonstrate the effectiveness of their method. Overall, this paper is very comprehensive.\n2, As evaluated and validated in the experiments, the proposed method vastly reduces the inference time at test phase. \n\nWeakness,\n1, The paper kind of lacks of novelty. Basically the proposed method distills a Bayesian models into an amortized Dirichlet distribution, which is straightforward. \n2, The baselines such as MCDP-KL, MCDP-EMD are strange, it is wired why you would distill the predictive distribution of a single point to a Dirichlet distribution. And I think it is probably unfair, as distilling the single-point distribution to the Dirichlet under KL, EMD, MMD might require large amount of particles, which they don't have.\n3, Related to (2), more baselines should be compared with to better demonstrate the method's effectiveness. 1) performance of the un-distilled MCDP and SGLD models. 2) BDK and DPN for the MCDP models. 3) MCDP and SGLD with fewer particles. The paper claims to achieve 500x speed up, while I reckon the performance of MCDP and SGLD won't deteriorate a lot if you use only fewer particles. \n4, It would be interesting to see experiments other than out-of-distribution detection, such as calibration. \n\nMinor Issues,\n1, The paper has several un-complied references, such as above eq(4) and appendix D.\n2, The \\Tau(x | \\theta) in Figure 1 is a typo.\n3, Assumption 1 should be put forward to the main articles for comprehensiveness of Lemma 1.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Overall I liked several results presented in this paper. The findings in Figure 2 gives clear illustration on how Bayesian classification models distinguish between in-distribution difficult-to-classify data and out-of-distribution data, namely uncertain predicted mean and large predicted variance. Though I believe this eventually depends on what kind of \"kernel\"s are used to correlate data points in the prior, throughout the paper I assume meaningful \"kernel\"s are used (for Bayesian NNs this is rooted in the inductive bias of neural networks). \n\nAnother result that I liked is in experiments we can clearly see the advantage of considering the bayesian predictive distribution over a single predictive mean. As demonstrated by BDK-SGLD vs. BDK-DIR-SGLD. \n\nThe proposed idea is a simple and meaningful improvement over previous works. Though the contribution is quite limited, the authors present it with great clarity, which I appreciated. However, I do find some discussions in the paper unnecessary and would expect for more technical contributions. For example, I didn't see the argument for the whole section discussing amortization gap. Everything seems straightforward given the hypothesis F is of enough capacity, which obviously does not hold in practice.\n\nMany other concerns are summarized below:\n* What if the teacher predictive distribution is far unlike a Dirichlet? How much is the discrepancy between teacher and student predictive distribution? Theoretical or empirical evidence is needed for this modeling choice.\n* One importance advantage of Bayesian classification models is that they can capture the covariance between predictions  of different data points. By amortization this advantage no longer exists.\n* In the paper the authors keep mentioning that the method can be applied to GPs but I don't see experiments or algorithms for it?\n* The concentration model is parameterized using an exponential activation, how does this activation affect the performance?\n* The distilling process is done on a held-out dataset. Which may not be wanted because an advantage of Bayesian classification models (eg. GPs) is that all hyperparameters can be automatically selected by marginal likelihoods and don't need a held-out validation set.\n* MMD/wasserstein distances are cool but they require also samples from the student, which adds more variance to the distillation process.\n* The experiment setup is extremely unclear to me. What is \"uncertainty measures\", are they used as metrics for detecting out-of-distribution data, how are AUROC/AUPR calculated using the uncertainty measures? I can guess the meaning but the paper should be more clear about this.\n* I found most numbers convincing except that sometimes BDK-SGLD outperforms BDK-DIR-SGLD, if I understand it right, the predicted mean of BDK-DIR-SGLD should be as good as BDK-SGLD?\n\nMinor:\n* On page 4, above Eq. (4) there is a broken figure link.\n* On Page 7, \"To save space, we only present the best performing uncertainty measure (E, P or C)\". What is \"C\" here?"
        }
    ]
}