{
    "Decision": {
        "decision": "Reject",
        "comment": "All reviewers agreed that this submission is still premature to be accepted to ICLR2020.\nWe hope the review comments are useful for improving your paper for potential future submission.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents a metric learning approach for the multi-label classification problem. It basically maps the input features and the output labels into the same space and then uses k-NN to find the closest labels for each inputs. During training, it minimizes the squared Euclidean distance between the input embedding and label embedding. In the experiments, some image datasets and text datasets are used to compare with several multi-label learning algorithms. \n\nThe proposed method is clearly presented in the paper. However, I have several concerns.\n\n1. The proposed method is straightforward and lacks of significant novelty.\n2. The datasets are very small scale in terms of both sample size and label size. \n3. The comparing methods are a little bit out-dated. I would also suggest to compare directly with the naive one-versus-all method using deep learning extraction models. \n4. There are a few typos. For example, on Page 1, \"because it wide application\", \"but cannon deal\", \"may lost key information\"; on Page 2, \"an bidirectional\", \"mapping of outputs to metric space are\"; on Page 3, \"a scalable models\", etc.. Besides, Eq (1) should be written in a more formal way. \n\nOverall, I vote for rejection and the main reason is the lack of novelty. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper aims to solve multi-label problems via learning a share embedding space for instances and its label sets. Specifically, the author considers deep neural networks F(x) as an encoder for the instance (either raw input or features) and a shallow MLPs G(y) as an encoder for the label outputs. In the training stage, the instance embedding and its label embedding are forced to be close. An additional constraint is instances with different labels should be far from each other. After training, the inference can be done in the embedding space by looking up the labels of the queryâ€™s kNN instances. \n\nMetric learning for multi-label problems is not new, many works have been proposed such as [1,2]. Using deep neural networks for multi-label problems is also not new, see [3,4]. Thus, the novelty of the proposed method is rather limited. The interesting part is the constraint of Eq(11), where kNN need to be updated whenever the instance encoder model F(X) is changing during the optimization, which is very expensive for large-scale application.\n \n[1] Sparse Local Embeddings for Extreme Multi-label Classification, NIPS 2015.\n[2] Learning Deep Latent Space for Multi-label Classification, AAAI 2017.\n[3] Deep Learning for Extreme Multi-label Text Classification, SIGIR 2017.\n[4] X-BERT: eXtreme Multi-label Text Classification with BERT, ArXiv 2019. \n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper addresses the problem of multi-label prediction.  It proposes a method that uses a co-embedding of instances and labels into a joint embedding space in a way that related instances and labels fall close by and unrelated ones fall far away.  For this purpose, embeddings from input space and label space to a common space are learned from training data. At the prediction time, KNN to the embedding of the test instance in the co-embedding space is used to predict relevant labels.\nFeaturized (attributed)  labels are potentially considered, which can facilitate incorporating label dependence and generalization over unseen labels.\n\nMain shortcomings:\n- The novelty of the work is limited. Ideas introduced in this work are present and being investigated in literature for a while now, leading to remaining limited contribution for the paper.  Related work on joint embedding, co-embedding, label-embedding, and zero shot learning seem to be neglected totally. For example, the paper lacks awareness of, citation to and comparison with related work such as [1-5].\n- Presentation of the paper can be highly improved. There are several grammatical and writing problems in the paper.\nFormulation can also benefit from  improved presentation. See for example Eq (1).\n- Technical arguments are not all well founded. For example, the scalability claim in the abstract  of the paper seems to refer to \"prediction\" time complexity being linear in the number of \"training\" examples, which is not actually fast.\n\nIn summary, based on the above reasons, I vote for the paper to be strongly rejected.\n\n[1] Akata, Zeynep, et al. \"Label-embedding for image classification.\" IEEE transactions on pattern analysis and machine intelligence 38.7 (2015): 1425-1438.\n[2] Weston, Jason, Samy Bengio, and Nicolas Usunier. \"Wsabie: Scaling up to large vocabulary image annotation.\" Twenty-Second International Joint Conference on Artificial Intelligence. 2011.\n[3] Li, Xin, and Yuhong Guo. \"Bi-directional representation learning for multi-label classification.\" Joint European conference on machine learning and knowledge discovery in databases. Springer, Berlin, Heidelberg, 2014.\n[4] Mirzazadeh, Farzaneh, et al. \"Scalable metric learning for co-embedding.\" Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, Cham, 2015.\n[5] Yeh, Chih-Kuan, et al. \"Learning deep latent space for multi-label classification.\" Thirty-First AAAI Conference on Artificial Intelligence. 2017."
        }
    ]
}