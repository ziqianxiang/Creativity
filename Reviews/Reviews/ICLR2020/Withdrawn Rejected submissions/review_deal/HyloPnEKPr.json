{
    "Decision": {
        "decision": "Reject",
        "comment": "Main content:\n\nBlind review #2 summarizes it well:\n\nThis paper extends the neural coreference resolution model in Lee et al. (2018) by 1) introducing an additional mention-level feature (grammatical numbers), and 2) letting the mention/pair scoring functions attend over multiple mention-level features. The proposed model achieves marginal improvement (0.2 avg. F1 points) over Lee et al., 2018, on the CoNLL 2012 English test set.\n\n--\n\nDiscussion:\n\nAll reviewers rejected.\n\n--\n\nRecommendation and justification:\n\nThe paper must be rejected due to its violation of blind submission (the authors reveal themselves in the Acknowledgments).\n\nFor information, blind review #2 also summarized well the following justifications for rejection:\n\nI recommend rejection for this paper due to the following reasons:\n- The technical contribution is very incremental (introducing one more features, and adding an attention layer over the feature vectors). \n- The experiment results aren't strong enough. And the experiments are done on only one dataset.\n- I am not convinced that adding the grammatical numbers features and the attention mechanism makes the model more context-aware.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper unfortunately violates the blind-review policy: its acknowledgement exposes the authors. I thus support desk rejection. \n                                                                                                                                                                                                                                                                                                                                                                                "
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper extends the neural coreference resolution model in Lee et al. (2018) by 1) introducing an additional mention-level feature (grammatical numbers), and 2) letting the mention/pair scoring functions attend over multiple mention-level features. The proposed model achieves marginal improvement (0.2 avg. F1 points) over Lee et al., 2018, on the CoNLL 2012 English test set.\n\nI recommend rejection for this paper due to the following reasons:\n- The technical contribution is very incremental (introducing one more features, and adding an attention layer over the feature vectors). \n- The experiment results aren't strong enough. And the experiments are done on only one dataset.\n- I am not convinced that adding the grammatical numbers features and the attention mechanism makes the model more context-aware.\n\nOther suggestions:\n- The citation format seems weird through out the paper. \n- Table 1 and 3 look like ablation results. It might be less confusing if it's presented as \"Full system: xx%; without pairwise FA: yy%; without grammatical numbers zz% ...\".\n- Equation 8 - 10 are quite confusing. What is f(x)? How large is V? What is u? etc.\n- Please define/explain the \"grammatical numbers\" feature when it's introduced in Section 2.2."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review": "This paper proposes to use an extra feature (grammatical number) for context-aware coreference resolution and an attention-based weighting mechanism. The approach proposed is built on top of a recent well performing model by Lee et al. The improvement is rather minor in my view: 72.64 to 72.84 in the test set. \n\nThere is not much in the paper to review. I don't think the one extra feature warrants a paper at a top conference. The weighting mechanism over the features is also unclear to me why it benefits from attention. Couldn't we just learn the weights using another layer? It could be context dependent if desired.\n\nIt is also incorrect to criticise Lee et al. (2018) that they would give the same representation to the same mention every time. Their model is context dependent as they use a BiLSTM over the sentence. Of course the same mentions are likely to get similar representations, but this is desirable."
        }
    ]
}