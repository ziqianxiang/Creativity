{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Summary: This paper presents a method for automatically tuning the L1 and L2 penalty terms of sparsity-inducing deep network training so as to maximize cross-validation performance.  The basic idea is to train a network with L1 and L2 regularization terms, and then use implicit differentiation to get the gradient of a cross-validation loss with respect to the corresponding lambda_1 and lambda_2 hyperparameters on these terms (since the L1 term is non-differentiable they use the standard optimization formulation to derive the gradients via implicit differentiation).  They show that the method \n\nComments: Overall I very much like the idea of using implicit differentiation of a cross-validation loss for automatic hyperparameter tuning, though this has been done in a very similar fashion by past work (e.g. in the cited Maclaurin et al., though see also e.g. paper).  The issue I have with this paper, though, is that neither the method nor results are sufficient to convince me that this approach provides either a theoretical or practical improvement in this case.  On the more mathematical/algorithmic side, the formulas for implicit differentiation only hold when W has reached an actual minima, which is almost certainly not the case in this setting.  Furthermore, the authors eventually just use an extremely crude approximation of the actual implicit derivative, replacing the Hessian with the identity, so it's unclear what their \"gradient\" term really even corresponds to here.\n\nOn the practical side, since the inner loop optimization needs to run a fair amount before the gradient can be computed, and since there are only two hyperparameters of interest (and it's not clear to me we even need the L2 term), it wasn't clear whether there was any real speed benefit to the proposed approach over simple grid search (and this speed benefit is really the only potential advantage of a gradient based approach over grid search).  No explicit timing results are presented in the paper, but the number of epochs used e.g. for CIFAR-100 are _well_ above what's needed for training a single model.\n\nFinally, in terms of the experiments, it's not at all clear to me the methods are outperforming the other approaches.  The authors bold their entries, but there is no metric in which they are better than all the others, and they could very well just indicate different points on the Pareto frontier.\n\nThere are also several typos in the paper, such as referring to the Hessian matrix as the \"Heisen\" and non-differentiability as \"non-derivability\".  "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper addresses the issue of deep network compression. Both L1 and L2 norms on a validation set are used for regularisation to achieve sparsity and avoid overfitting. Auxiliary variables are used to address the issue of L1 term not differentiable. The proposed algorithm (together with a pruning step) has been evaluated on some deep networks with promising results. \n\nI recommend weak reject because of the following reasons:\n- The key contribution is the treatment of the L1 term. The idea seems not new [1]. But it could that using it for the network compression problem is new. Yet, the original contribution is limited.\n[1] A Fast Dual Projected Newton Method for 1-Regularized Least Squares (IJCAI’11)\n- While the values of \\lambda_1 and \\lambda_2 can be automatically learned, the starting epoch for pruning and the thresholding are needed to set manually and tuned.\n\nSome specific comments:\n\nSection 3.3:\nProvide some references and also explanation on the trick behind the use of the auxiliary variables W^{l+} and W^{l-} to improve a bit readability.\n\nFor the experimental results, PP-1 gives the best result for ResNet-50. Any comments?\n\nThe paper needs to be further polished. The following lists just some of those I spotted and there are more.\nSection 3.3\n“w˜ is a vector which consisit of the elements of”\n-> \n“w˜ is a vector which *consists* of the elements of”\n\nSection 3.4\nHeisen matrix -> Hessian matrix\n\n“Becker & Lecun (1989) and LeCun et al. (1989) given the formula ..”\n->\n“Becker & Lecun (1989) and LeCun et al. (1989) *gave* the formula (equation?) ..”\n\n“and approximate it to a identity matrix”\n->\n“and approximate it to *an* identity matrix”"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper tackles the problem of automatically finding the hyper-parameters of sparsity-inducing regularizers for network pruning. In particular, the authors focus on L^1 and L^2 regularizers. The method then optimizes the hyper-parameters by making use of the idea of cross-validation gradient: Computing the gradient of a validation loss w.r.t. the hyper-parameters. For the L^1 case, this requires deriving a differentiable version of the regularizer.\n\nOriginality:\n- As discussed in Section 2.2, the idea of gradient-based optimization of hyper-parameters has already been studied in the literature, with several existing methods relying on a cross-validation-based loss, such as Barrat & Sharma, 2018, and Luketina et al., 2016. Here, it appears from Section 3.4 that the authors essentially make use of the approximation introduced by Luketina et al., 2016. There is therefore little novelty on this aspect.\n- I acknowledge that, to the best of my knowledge, hyper-parameter optimization has not been studied in the context of network pruning. However, I do not consider this in itself as a sufficient contribution for ICLR.\n- This nonetheless required defining a differentiable version of the L^1 regularizer, as shown in Eq. 10. I must say that I was not aware of this way to encode the L^1 loss, although the authors do not seem to claim this as novelty. In fact, looking in the literature, I found that this seems to originate from the sparse coding literature. I would appreciate it if the authors could discuss the relevant literature on this topic, so as to justify that Eq. 10 indeed is equivalent to Eq. 1. Now, as this seems to be a standard approach, the novelty here also remains limited.\n\nExperiments:\n- The experiments show the good behavior of the proposed algorithm. However, I would be interested in seeing how far the final hyper-parameter values are from the initial ones, and how sensitive the results are to their initialization. Ultimately, if one needs to carefully initialize the hyper-parameters, the resulting algorithm might not be very useful. I would therefore also appreciate it if the authors provided the results obtained by keeping the initial hyper-parameters fixed.\n- For the ResNet-56 experiments in Table 1, the PP-1 and PP2 methods of Singh et al., 2019 seems to give similar accuracy but much higher compression rates as the proposed method. Can the authors discuss this? Why are the results of this method not reported for the other experiments?\n- Why are the FLOPs and Reduced values for the proposed method not reported in Table 1?\n\nClarity:\n- The general methodology is reasonably clearly explained.\n- However, the paper is poorly written, with many typos and grammar mistakes (e.g., Heisen instead of Hessian, weighted delay instead of weight decay,...)\n- Some of the mathematical terms are also not clearly defined: \n* Eq. 4 uses \\tilde{y}, but below the equation the authors used \\hat{y}\n* The beginning of Section 3 uses both \\tilde{W}^{l+} and W^{l+}, but both seem to indicate the same thing\n\nSummary:\nMy main concern about this paper is its novelty, as the method essentially seems to apply existing techniques in the context of network pruning. I also would like to have a better understanding of the influence of hyper-parameter initialization on the results."
        }
    ]
}