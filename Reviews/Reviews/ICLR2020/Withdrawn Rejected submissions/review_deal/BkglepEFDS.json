{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper introduces a method to perform domain adaptation in the space of\nsoftware vulnerability detection using generative adversarial networks. A\ndual discriminator GAN projects labeled and unlabeled data to the same\nrepresentation space in order to assign it labels and address the domain\nadaptation problem.\n\nThere are two broad classes of concerns I have with this paper.\n\nFirst, the paper does not clearly describe why domain adaptation and GANs\nare especially important for this domain. What is different about this\ndomain that requires developing a completely new approach to domain\nadaptation instead of borrowing from prior work? Some comparisons to\nbaseline domain adaptation in the evaluation (see the second concern)\nwould help justify this decision.\n\nIt would help to justify why domain adaptation is even important to\nbegin with. What would happen if the classifier were trained on the\nprogram text of N-1 different programs and then evaluated on the Nth,\ndisregarding the fact that it's a different program?\n\nAfter showing that domain adaptation is a problem, it would then help\nto show why GANs are the right way to solve this. It is not obvious\nthat GANs---which often come with a significant increase in complexity\nand have problems of their own---are the right way to improve on the\ndomain adaption.\n\nSecond, the evaluation is not thorough. As the authors state, obtaining\na good dataset for evaluating vulnerability detection is exceptionally\ndifficult. Using the dataset from Li et al. is about as good as could\nbe asked for.\n\nHowever, given the limits of the dataset, it is important to be very\ncareful in performing the evaluation. The dataset consists of roughly\n20,000 labeled examples, of which fewer than 400 are in the positive\nclass. When splitting out only 20% as the test set, this gives under\n100 positive examples in the test set.\n\nAs a result, the statistical power of Table 1 is questionable. It would\nhelp to include margins of error over the five runs the authors performed\n(a good thing!) in order to show what results are meaningful and what\nresults are not statistically significant.\n\nFurther, such a small dataset has implications for overfitting: how\nmany parameters are in the model? Does the model completely overfit\nthe dataset within a few epochs?\n\nThe proposed approach has *many* technical details. This calls for an\nextensive evaluation to demonstrate which components of the proposed\napproach contribute most strongly to the accuracy of the classifier.\nHowever, this is not performed. Only two tables evaluate the proposed\nclassifier after developing it in the prior four pages.\n\nThus, many of the core assertions, such as the fact that covering more\nmodes makes the classifier more accurate, can not be experimentally\nvalidated to be true.\n\nLess important for a ML conference than the prior two concerns, I have\nsome serious reservations regarding the utility of such a classifier\n(or even ability of a classifier to work at all). Almost all C code is possibly\nvulnerable in isolation. It is only by knowing the bounds of arrays,\npreconditions on pointers, and other underlying assumptions that\nit is possible to say code is not vulnerable to some attack. (Worryingly,\nthis is the exact information that is *removed* in the preprocessing of\nthe data.) What is the intended use case for this line of work?\n\n\nMinor comments:\n- I have never heard of SVD used for software vulnerability detection---\n  it's always meant singular value decomposition to me. Looking online this\n  doesn't seem to be common practice.\n\n- \"We implement eight mentioned methods in Python using Tensorflow which\n  is an open-source software library for Machine Intelligence developed\n  by the Google Brain Team\" -> better to cite Abadi et al.\n\n\n(As this paper is ten pages long, and CFP asks for reviewers to \"apply a\nhigher standard to papers in excess of 8 pages\", I have done so.)\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose Dual-GD-DDAN as an extension of DDAN for the task of domain adaptation in software vulnerability detection (SVD), a problem where domain adaptation is important given the scarcity of labels. The authors report an experimental comparison across multiple datasets against several baselines and demonstrate clear performance improvements in terms of the F1-score. The paper is well-written and provides sufficient context to the readers.\n\nComments:\n\n- Result tables report the average predictive performance over 5 experiment runs, but they do not include a measure of variance of the results (+-std or an estimate of confidence intervals). This is relevant both for understanding the size of the effect coming from the proposed improvements, as well as the stability of the method - whether the improvements are consistent or occasional.\n\n- Similar to the above, despite the reader being reasonably convinced of the performance improvements in terms of average precision, the authors should consider using a formal statistical test if possible - if it is computationally prohibitive to run many repetitions for each domain adaptation direction, perhaps this can be derived jointly from the entire set of runs across all adaptation tasks?\n\n- The paper might benefit from the authors providing some examples of engineered features for capturing vulnerabilities. Even though the focus of the paper is on developing an automated approach, this particular task is not one that most readers will be familiar with. Providing additional domain-related information would be beneficial for the readers to understand what the model is ultimately supposed to be able to achieve - so far, this is only given in the appendix in the \"Motivating example\" section.\n\n- Can vulnerabilities be categorized? Are there multiple types of vulnerabilities? If so, what is the model performance (similarly, for the baselines) across different types of vulnerabilities rather than jointly - are there some that are much harder to get right via domain adaptation than others? Either way, something to touch on in the discussion at least briefly\n\n- The proposed approach seems to always achieve the highest F1-score, but not necessarily the highest precision or the highest recall. At the moment, the paper does not discuss the metrics in sufficient detail. Are precision and recall to be traded off equally? Should false negatives not be penalized significantly more than false positives? If so, would it not be beneficial to focus on a different measure? This is especially important considering Table 3, where SCDAN achieves a higher recall in 3 out of 4 experiments, though at the price of lower precision and a somewhat lower F1-score.\n\n- In line with the above, assuming that the classifier outputs a soft label (probability), would it be possible to select a different operating point, based on either the training performance or one particular domain adaptation (to inform the operating point for the others, presuming there is some generalization). In particular, if one was to aim for close to 100% recall, what would be the resulting precision? Would it be possible to derive a precision-recall curve and compute PR AUC in such a setting?\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper introduces adversarial domain adaptation to software vulnerability detection with separate feature extractors for each domain and two discriminators. The authors also claim to keep the manifold of both domain by adding a manifold regularization term. The experiments on real-world software projects show that the proposed method sets up a new state-of-the-art for the security community.\n\npros:\n+ It is pleasing to see deep learning can benefit the security community. However, it seems this is not a pioneer work. The data processing in 2.2 is not a contribution, but borrowed from [1]\n\ncons:\n- It seems the dual discriminators are not necessary. Given a D_S (source discriminator), it seems (1 - G_S) is an optimal target discriminator. There is no need for another D_T discriminator. However, the authors claim that the dual-component architecture solves the mode collapse problem, which is by no means reasonable. The proposed method just adds a redundant discriminator.\n\n- The loss for manifold regularization is not convincing, either. An trivial solution to minimize the Eq. 4 is to make all the representation vectors the same, leading to zero loss. However, this is not desired for manifold regularization.\n\n- Poor presentation: Figure 3 is not self-contained (cannot tell what the left plot represents from the caption or subtitle), not vector diagram. What's worse, the left figure even covers the text (\"problem\"). Only colors are distinguishable. The 0s and 1s in the same color are too small to distinguish.\n\n- It is too long a paper, with 10 pages of content. Please refer to the Submission Instructions of https://iclr.cc/Conferences/2020/CallForPapers :\"Reviewers will be instructed to apply a higher standard to papers in excess of 8 pages.\"\n\n[1] Deep Domain Adaptation for Vulnerable Code Function Identification, Nguyen et. al, IJCNN 2019."
        }
    ]
}