{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes a model architecture and training procedure for multiple nested label sets of varying granularities and shows improvements in efficiency over simple baselines in the number of fine-grained training labels needed to reach a given level of performance.\n\nReviewers did not raise any serious concerns about the method that was presented, but they were also not convinced that it represented a sufficiently novel or impactful contribution to an open problem. Without any reviewer advocating for the paper, even after discussion, I have no choice but to recommend rejection.\n\nI'm open to the possibility that there is substantial technical value here, but I think this work would be well served by more extensive comparisons and a potentially revamped motivation to try to make the case for it that value more directly.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Summary: The problem addressed is how to train a DNN to learn a hierarchical representation of the input that has heterogeneously annotated labels at various levels of granularity. After training the network’s goal is to emit sequentially finer grained labels with corresponding confidence. The authors use information theory to propose a network typology of information bottlenecks with skip connections to achieve this nested learning problem.  \n\nDecision: weak reject. \n\nReason: The proposal of learning a hierarchical representation is not new. Nor is the architecture. I think the interesting points (that are not really flesh out as much, but appear to be in the auxiliary material - section B.2) is the training regime. I would’ve liked to have seen more of what the role of the training regime is on the outcomes and how the network’s gradient’s behave in different regimes. But in general, the paper is well organized and argued, although there is a little belaboring of ideas of entropy and mutual information only to use it to buttress a point that is made and left hanging. The paper also defines too many concepts for every proposition it wants to support, making it cognitively costly to follow. \n\nFeedback:\nPleasure reading the paper. Few points of feedback:\n\n- Perhaps show the network gradients to help understand the dynamics of the learning\n- What do you think is the role of the training regime (section B.2) on the outcomes you are observing? Do you think it would be worth observing the effect of changing the learning regime (and the accompanying gradient) on the outcomes?\n- Generalization to any number of nested labels is not demonstrated\n- The empirical demonstration of contribution of skip connections is not too  \n- Corollary to above, what do you think would be the role of attention in the hierarchical representation learning?\n\nQuestions:\n1- Not clear why complementarity is a necessary condition of learning (section 3, before definition 1). Take for example the vehicle, wheels, truck example in figure 1. Learning F2 (wheels) features isn’t conditioned on correctly learning F1 (vehicle). In fact, could it be satisfactory, to first order approximation, to assume that learning finer-grained features first (roundness of wheels) and then combining lower level features (in what you refer to as Markov chain) may result in equal if not better accuracies? Is it possible to test this?\n\n2- Not clear why calibration of outputs (section “combination of nested outputs” in p 5) is an approximation of P(Y_i=q).\n\n3- Its not clear to me why the proposed rejection calibration method as a way of handling overconfidence is the right approach. Why this solution? Why not use, for example, regularization instead?\n\n4- Table 1 of results. As the distortions increases the relative improvements of the nested learner appears to increase in CIFAR-10 results. Can you demonstrate this is not an artifact of the distortion generation strategy and is indeed a stable observation?\n\n5- why is the marginal accuracy improvements so much larger for going from a budget of 1 to 2 than 2 to 5 in all cost functions? Does this not refute the claim that the nested model “gradually breaks”?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "I read the author response, thank you for responding to my questions.\n\nOriginal review:\n\nThis paper presents a hierarchical learning approach that trains neural network classifiers on a known hierarchy of labels.  The experiments show that the approach makes a network more robust to distortion compared to standard end-to-end learning.  The approach in this paper and its formalization of the task are interesting, but the significance of the techniques is somewhat unclear and the experiments could be more thorough in terms of the baselines and data sets considered.\n\nFrom the related work section, the relationship between this paper and the previous work is somewhat complicated -- it is hard for a reader not deeply familiar with these previous works to understand the unique contribution made in the submission, and assess why it is significant.  For example, in the last paragraph of that section, many of the distinctions drawn between the submitted work and previous methods seem minor (including a confidence measure for a certain prediction, for example) or seemingly subjective (about whether an operation with a previous method was “natural” or could be done “transparently”).  Making crisper, less ambiguous distinctions between this work and previous work would help.\n\nLikewise, the experimental results here do not compare against any of the hierarchical learning approaches discussed in the related work section.  The results show that the paper’s approach is more robust to distortion compared to standard end-to-end learning.  However, I was unclear on why it was not appropriate to compare against the other hierarchical learning methods from previous work.  Also, if the claim is improved robustness, I feel that evaluating against adversarial training (Madry et al., ICLR 2017) or similar approaches is necessary to understand the practical relevance of these improvements.\n\nFinally, experiments that consider larger hierarchies (here, the number of target classes tends to be small, CIFAR-10 and MNIST each have ten classes, meaning the hierarchies are not very rich) would help illustrate the potential power of the techniques.\n\nMinor\nI didn’t understand the following statement, and given that it’s a fairly bold claim I would rephrase it or explain it better in the paper body rather than referring the reader to the appendix:\n“A standard DNN unknowingly uses low quality data also to train higher layers, even if there is no high level information in the data.”\n\nI don’t understand what the right arrow operator on the top of page 5 means.  From earlier statements it seems that I(f_i(X), Y_i) -> I(X, Y_i) means that the left quantity is approximately equal to the right.  But I’m not sure why to use an arrow for that rather than an \\approx symbol.  The arrow would seem to imply that the left approaches the right in the limit, but if that is what you mean you should tell us in the limit of what.  Later the arrow is used to represent links in a Markov chain, adding further confusion for me.\n\nI think it would be helpful if before Equation 1, you mentioned this holds for strictly nested Y_i’s (since earlier in the paper, Y_i referred to more general things).\n \nI assume the ECE is computed over held-out validation data (i.e., not training data)?  The paper should say this.\n\nPage 7: lineal -> linear",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Nested learning for multi-granular tasks\n\n1. Summary\nThe paper considers a framework for classification with a hierarchy of labels [from coarse to fine]. The paper proposes a network architecture with multiple bottleneck layers, one for each label level, and skip connections. The objective function is the standard classification loss. The experiments show that coarse labels help learning and can improve label efficiency, i.e. don’t need all fine labels to get good classification performance.\n\n2. Opinion and rationales\n\nWhilst I think the execution of ideas is good and the motivation is very practical, I’m leaning towards “reject” for this paper due to the reasons below. I welcome the authors’ clarification and am willing to reconsider my view.\n\ni. The paper justifies the proposed architecture as successive information compression into label embeddings using some entropy based criteria (as in information bottleneck literature). This ensures the relationship of the entropies between the label layers. However, I’m not sure this justification is necessary (albeit being a valid one) given that the training/later sections do not come back to this justification.\n\nii. The novelty of the proposed architecture and training approach is low. The network is a nested structure of successive classifiers. The proposed calibration using rejection class and temperature scaling is not new.\n\niii. it would be better if the proposed architecture + method are validated on more real-world datasets with a more natural label grouping scheme, and compared to alternative architectures, e.g. multi-task learning with a shared network except the output layer.\n\n3. Minor details\n\nSome citations should be enclosed in brackets"
        }
    ]
}