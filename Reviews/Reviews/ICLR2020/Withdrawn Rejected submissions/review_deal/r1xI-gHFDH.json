{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposed a general framework to construct unsupervised models for representation learning of discrete structures. The reviewers feel that the approach is taken directly from graph kernels, and the novelty is not high enough. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "title": "Official Blind Review #3",
            "review": "The paper presents an unsupervised method for graph embedding. \n\nDespite having good experimental results, the paper is not of the quality to be accepted to the conference yet. The approach is rather a mix of previous works and hence not novel. \n\nIn particular, the algorithm for WL decomposition is almost fully taken from the original paper with a slight modification. Advantage of using it for unlabeled data is poorly motivated as unlabeled graphs can easily take statistics such as degree as the node labels, which was shown well in practice. \n\nModified PV-DBOW is in fact the same algorithm as the original CBOW model but applied to different context. It has been used in many papers, including Deep GK, graph2vec, anonymous walks. \n\nAlso, the Figure 1. is taken from the original paper of WL kernel. The algorithms 1 and 2 are taken from the original papers with slight modifications. \n\nThere is no discussion of [1], which uses CBOW framework, has theoretical properties, and produces good results in experiments. There is no comparison with GNN models such as [2]. \n\nI would be more interested to see explanation of the obtained results for each particular dataset (e.g. why MUTAG has 92% accuracy and PTC 67%); what so different about dataset and whether we reached a limit on most commonly used datasets. \n\n[1] Anonymous Walk Embeddings? ICML 2018, Ivanov et. al. \n[2] How Powerful are Graph Neural Networks? ICLR 2019, Xu et. al.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Strength:\n-- The paper is well written and easy to follow\n--  Learning the unsupervised graph representation learning is a very important problem\n-- The proposed approach seems effective on some data sets.\n\nWeakness:\n-- The novelty of the proposed approach is very marginal\n-- The experiments are very weak. \n\nThis paper studied unsupervised graph representation learning. The authors combined the techniques for Deep Graph Kernels and Graph2Vec, which essential extract substructures as words and the whole graph as documents and use doc2vec for learning the representations of both graphs and substructures. Experimental results on a few data sets prove the effectiveness of the proposed approach. \n\nOverall, the paper is well written and easy to follow. Learning unsupervised graph representation learning is a very important problem, especially for predicting the chemical properties of molecular structures. However, the novelty of the proposed method is very marginal. Comparing to the Deep Graph kernel methods, the authors simply changed from the word2vec style methods to doc2vec style methods. The paper could be better fit to a more applied conference. Moreover,  I have some concerns on the experiments. \n(1) The data sets used in this paper are too small. For unsupervised pretraining methods, much larger data sets are expected. \n\n(2) The results in Table 1 are really weird. Why do the performance of your method have a much lower standard deviation? It is really hard to believe the proposed methods have much stable performance compare to other methods.  Can you explain this?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a framework for learning distributional representations of graphs in the following way: First, each graph is represented as a collection of subtree patterns. Second, the neural language model of doc2vec is applied to these collections of patterns to learn graph embeddings. These embeddings are then exploited in downstream analyses such as classification. Overall, the idea of formulating graph representation learning as a language model is interesting. The experiments show that it perform better than kernel methods. I have the following major comments:\n\n1. The main issue with this method is the computational complexity due to exponential growth of vocabulary of subtree patterns size for large graphs. Particularly , for experiments with unlabeled graphs, the performance is significantly worse than CNN based models. How would the performance be on unlabeled small graphs? For example, have you verified the performance on small graphs of section 4.2 when labels are ignored? (downstream clustering task)\n\n2. The neural language models rely on the concept of context in documents. How the concept of context defined for subtree patterns extracted by Weisfeiler-Lehman algorithm?\n\n3. The issue of diagonal dominance should be clarified. How does the pruning tackles this issue?\n\n "
        }
    ]
}