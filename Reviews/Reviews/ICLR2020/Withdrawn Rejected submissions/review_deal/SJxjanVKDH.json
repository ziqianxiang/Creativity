{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper studies the problem of completion and instance segmentation of large-scale partial scenes. The problem is interesting and has relevance to multiple domains including graphics, vision, robotics and augmented reality. The proposed method builds on a network architecture presented in a previous paper [1] and modifies the 3D backbone to include the completion head and jointly learn the two tasks.\n\nThe method is evaluated on both real and synthetic datasets. Qualitative and quantitative measures indicate that the paper accomplishes impressive results compared to state-of-the-art approaches. In particular, the comparison against alternative approaches such as completion + instance segmentation and instance segmentation + completion are helpful to appreciate the contribution of the study.\n\nThe paper is well-written and explained thoroughly. However, the authors should consider the following comments to improve the paper.\n\n\nComments:\n\na. The paper achieves significant results by mostly utilizing ideas that appear in earlier works. Specifically, a major portion of the proposed network is the architecture in [1], and the modifications to the network including the loss terms for voxel occupancy prediction have been explored in other papers such as [2,3]. Clarifying this point would help evaluating the contribution of the paper.\n\nb. Anchor-based region proposal networks entail challenges in the training pipeline such as predefining the anchors to enumerate possible positions and scales of the bounding boxes. This feature however is not desirable in large-scale scenes, as it would limit the range of object sizes the network can detect and perform downstream tasks (completion and segmentation). A discussion on mitigating these challenges would be beneficial for the explanation of the method. \n\nc. In the instance segmentation benchmark (http://kaldir.vc.in.tum.de/scannet_benchmark/semantic_instance_3d), there are several methods outperforming [1], the baseline this paper compares against. The experimental evaluation of the method would improve with comparisons against [4,5].\n\n\nOverall, Iâ€™m on the border leaning toward a rejection, mainly due to the similarity to [1], and because the innovative part of the paper has been explored in various forms in earlier works including [2,3]. Nevertheless, I am happy to adjust my assessment after reading the response of the authors.\n\n\n[1] 3d-sis: 3d semantic instance segmentation of rgb-d scans, CVPR 2019.\n[2] Learning shape priors for single-view 3d completion and reconstruction, ECCV 2018.\n[3] Learning 3d shape completion from laser scan data with weak supervision, CVPR 2018.\n[4] Learning object bounding boxes for 3d instance segmentation on point clouds, NeurIPS 2019.\n[5] PanopticFusion: Online volumetric semantic mapping at the level of stuff and things, IROS 2019.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Overview:\nThe authors propose a new task of Semantic Instance Completion where given a RGB-D scan of a room, they predict 3D objects in the scene (via instance segmentation masks) and complete the partial scans at the object level. They claim the following contributions:\n1. \"We introduce the task of semantics instance completion for 3D scans\",\n2. \"We propose a novel, end-to-end 3D convolutional network which predicts 3D semantic instance completion as object bounding boxes, class labels and complete object geometry\",\n3. \"We show that semantic instance completion task can benefit semantic instance segmentation performance\".\nThey also report experiments on Scannet and SUNCG.\n\nMAJOR COMMENTS:\n1. My biggest critique regarding this submission is it's novelty/value relative to prior work (Dai et al, 2018 and Hou et al, 2019). I agree with the aim of the authors and like the idea of completing the partial scan to be able to do better instance segmentation. However, this is not a novel idea per se in my opinion. As the authors themselves mention in Section 2, Related Work: 3D Scan Completion, other works have looked into trying to complete the scan which can be followed by a semantic instance segmentation approach (a baseline the authors compare against). The main contribution as I see it is doing the two tasks of instance segmentation and scan completion jointly in a single network. From an implementation perspective, this means adding a 3D unconv stream with Instance completion loss and Proxy completion loss; binary cross entropy terms that compare prediction scan geometry with ground-truth at object category and scene level respectively. This is made clearer when comparing Figure 2 of this work and of (Hou et al, 2019). Given this context, this paper feels more like a journal submission which improves upon (Hou et al, 2019) instead of a conference submission by itself. \n2. The authors claim that they outperform state-of-the-art approaches by over 15 in mAP@0.5 on ScanNet (Abstract). I find this misleading. I give the benefit to the authors in saying that the \"correct\" comparison is between the predictions and ground-truth for \"completed\" scans. However, ground-truth is not available for this comparison. The other meaningful comparison (which the authors also do) is propagating their semantic instance labels (from completed scan) to input partial scan and then evaluate on the task of Semantic Instance Segmentation. This comparison is done in Table 2 and shows significant improvement. However, looking at the results on ScanNet Benchmark (http://kaldir.vc.in.tum.de/scannet_benchmark/semantic_instance_3d), the numbers are worse than state-of-the-art. There is also a difference in the categories on which the model is evaluated. This also leads to another critique.. The authors only compare against (Hou et al, 2019) instead of the other methods on the benchmark. The other works need to be mentioned in the Related Work and compared against in Table 2.\n\nRATING:\nGiven the two critiques above, I have marked this as a WEAK REJECT. "
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper introduces a task called Semantic Instance Completion (SIS) that completes the missing 3D structure of the scene from the RGB-D scans. The authors proposed an end-to-end 3D CNN structure that combines color features as well as 3D features for the prediction, and experimental results on SUNCG and ScanNet demonstrate the effectiveness of the approach.\n\nThe writing of the paper is clear and I appreciate authors for providing clear diagrams for the network structure to better help understand the network structure. The formulation of the network structure (which is similar to [Hou et al., CVPR 2019] makes sense and experimental results clearly demonstrate that the proposed approach is reasonable.\n\nI have a few questions regarding the paper:\n- Do authors have intuitions on using more 3D features beyond simply using TSDF to represent 3D features? From the network structure, it seems that 3D features may have fewer dimensions (channels) than 2D color features. \n- The performance exceeds previous approaches, such as [Hou et al., 2019], however, the performance is still quite low (20-30 mAP). It'll be good if authors can provide more details on failure cases and make more detailed discussions on why there is still a long way to go. The trend of using more features is helpful is also not consistent in Table 3, it'll be good if authors can provide more contexts. \n- Since this is a new task. I wonder if authors can provide a concrete application that this paper can be used. I understand there are many scientific questions and interests behind this project, but a concrete application may be more useful as motivations. For example, why a certain application can use 3D-SIS instead of [Song et al., 2017], or other RGB-D completion algorithms (tasks)? "
        }
    ]
}