{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper presents a method for training a facial recognition system, CurricularFace, that creates adaptive loss functions that is both \"margin-based\" and is adaptive over time, increasing emphasis on harder examples and decreasing emphasis on easier examples over time.  The experimental results seem compelling, but I am not very familiar with the area, so it is difficult for me to judge.\n\n It seems like the experimental results are quite good, but I unfortunately can't recommend this paper for acceptance since I found it too difficult to read.   A lot of background seems to be assumed that I don't have and wasn't really provided in the paper \n\nHere are questions and comments I have that I hope can be worked into the final version:\n\n- From the citations, this seems to be a settled issue in the face recognition field, but why create a margin-based loss function by modifying the softmax in these ways, rather than using a more direct margin-based loss function, such as the various multiclass variants of the hinge loss, where you can set the target margin at per-example level?\n- Presentation challenge: The introduction speaks of so many things without definition: \"Triplet loss\", \"semi-hard or hard samples\", \"mining-based loss functions\", \"small backbones\", the \"manually defined constant t\" (before any mention of t).  Then in the Related Work section, reference to the \"open-set face recognition problem\" is not defined. \n- Do you compare any examples that don't take G(p(x))=1?  If not... maybe drop that factor.\n- You speak of various \"training stages\" -- how exactly is the training stage defined?  When do we progress from one stage to the next?  I don't see this in Algorithm 1.  \n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposes a curriculum learning strategy for face recognition and verification learning. The proposed strategy adaptively modulates the weight of the wrongly classified samples throughout the training.  The modulation coefficient used is directly linearly dependent  on the average similarity of the positive samples (that is pairs of features belonging to the same face class in the training triplets). The average similarity is computed with momentum 0.99 across training batches. It is logical to expect that this modulation factor is small at the early stages of training as the backbone has not yet learned a good feature representation that would result in high correct similarity, yet near the end of training the positive similarity increases, and the hard examples get to be modulated with higher coefficient. The proposed training strategy is evaluated on numerous benchmarks and demonstrates consistent improvements in the range of 0.02% to 0.6% accuracy. Some claims on the improved stability of the proposed approach are made.\n\nAlthough I am not an expert on most recent face recognition techniques, I feel this paper drives an interesting point and has a useful contribution to the community. Therefore I recommend acceptance. However, I urge the authors to consider improving a few points:\n\n1. there is a certain discussion on why focal loss is not a valid choice for a curriculum learning strategy. I feel I am not entirely convinced, and I suggest the authors to simple run the comparative experiments on all of their tests.\n\n2. Some notations used are not so well explained and some are introduced only after already being used, I suggest to improve that. In particular the N(.) and the I(.) and the angles that appear without proper definition (it is possible to understand from the context, but I would not burden the reader).\n\n3. I feel like more ablation studies could be done on the design choices of the modulation function."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper presents a new adaptive curriculum learning loss (CurricularFace) that embeds the idea of curriculum learning into the loss function to achieve a novel training strategy for deep face recognition, which mainly addresses easy samples in the early training stage and hard ones in the later stage. Experimental results on LFW, CFP-FP, CPLFW, AgeDB, CALFW, IJB-B, IJB-C, and MegaFace verified the efficacy of the proposed method.\n\nThe writing is clear and the presentation is satisfactory.\n\nMy concerns regarding this paper are as below.\n1) What is the training computational complexity and testing time cost compared with previous works, e.g., ArcFace [Deng et al., CVPR 2019], Enforced Cross-Entropy [Zhao et al., CVPR 2018]?\n2) When perform evaluations on IJB-B/C, did the authors adopt specific strategies for set-based face recognition? What are the quantitative  comparisons between the proposed CurricularFace and NAN [Yang et al., CVPR 2017], MPNet [Zhao et al., IJCAI 2019]?\n3) Does the proposed CurricularFace only work on face? Can it generalize to other tasks? e.g., detection, segmentation.\n4) Any theoretical analysis on the choice of scale s and margin m?\n\nBased on the above comments, I decide to give the initial rate of Weak Reject for this paper, if the authors could solve my concerns in rebuttal, I would like to further adjust my rate."
        }
    ]
}