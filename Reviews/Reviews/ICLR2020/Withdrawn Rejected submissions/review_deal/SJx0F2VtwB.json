{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors study the problem of conversational QA, proposing SDNet, a neural architecture based on RNNs, several levels/types of attention (as with many QA models), and a novel module for incorporating the contextualized embeddings of BERT. Specifically, as in DrQA+PGNet, previous dialogue turns are prepended to the current question and passed into an encoding layer than includes static word embeddings (GloVe) and pre-trained contextualized embeddings. The resulting representation is passed to an integration layer based on RNNs with self-attention on the context/question representations and inter-attention between then. Finally, the output layers relies on a bilinear projection to compute the probability of candidate answer spans. [Obviously, there are technical details (that are described fairly well in, and is the bulk of, the paper)]. Experiments are conducted on CoQA, showing improvements over FlowQA (and other baselines), presumably the state-of-the-art when this paper was written. Ablation studies show that the specific method of incorporating BERT (weighted sum per-layer) and incorporating dialogue history are important in achieving the stated performance.\n\nGiven when this work was completed (and the paper written), it was ostensibly the state-of-the-art at the time. However, this is ~9 months from when this paper was submitted — so some of the impact of the empirical results have been attenuating in publishing. When written, the most significant novel contribution was likely the method used for incorporating BERT and combining different embeddings. However, given new contextualized embeddings developed since then, many works incorporating contextualized embeddings with several different strategies, and now knowing that ~1.5% isn’t that large of a jump on this dataset makes this contribution seem more marginal than when published. I am fairly certain that this could have been revisited between the original SDNet submission and now (and likely improving performance). Beyond this, the other neural components were fairly standard at the time of development and now are standard procedure — such that this doesn’t seem very novel at this point. While it is somewhat understandable that the performance isn’t state-of-the-art at the time of submission, it is likely necessary to contextualize this work within more recently developed work. Secondly, I would expect the authors to also evaluate their method on QuAC and complete a more thorough error analysis. Unfortunately, at this point, this paper has likely already had some impact, but seems to have missed its (time-sensitive) window for getting into a top-flight conference without continuing to innovate and be near the state-of-the-art at the time of submission and be appropriate situated. Thus, I recommend rejecting from ICLR as the paper simply reads like a resubmission that has missed its time (even if it is a nice paper in many regards).\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "1. Summary: In this work, the author proposed a quite complicated model with multiple self-attention and inter-attention mechanisms for multiple-round conversational question answering. They evaluated this model on the CoQA dataset and compare it with several deep learning-based baselines.\n2. Overall assessment: This paper studied a very interesting problem and its motivation to use self-attention and inter-attention to thoroughly model and contextual information carried by passage and previous rounds of conversation make sense. However, this paper still has some space to get improved and it's not good enough to be published at ICLR yet.\n3. Comments:\n3.1 Figure 2 does not help much in this paper. It's hard to relate the explanation with components in this figure. I feel it would be helpful to label some symbols at some key positions and decompose this figure into multiple figures, with one for one component of the model. The authors can then include some more details into the figure and make it easier to read.\n3.2 There lacks some explanation of designs in the model. The reasons to stack soe many layers of attention mechanisms are not well explained. This also makes a bit hard to fully understand the model and the motivation behind it.\n3.3 One big concern to me is it seems to me that this model is overly designed and this design does not really capture the most important information for answering questions. Some error analyses and comparisons over variations of models may be able to prove the effectiveness of each component. But these do not appear in the paper.\n3.4 The baseline models are quite old and many recent proposed models are not used for comparison. Meanwhile, the performance presented in this paper is also outdated. It would be more convincing to see the model outperformed most recently proposed models."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a model architecture for conversational question answering (QA), which incorporates conversation context into the traditional QA model. It leverages word embedding and BERT encodings, and uses multi-level attention (from question to context) and self-attention in order to output the answer.\n\nOverall, I am not convinced that the paper is worth to be published in ICLR. First of all, I do not see a meaningful model innovation in the proposed architecture---lots of components (question-context attention, self-attention and recurrent units) are traditional, commonly-used components in any QA model for many years (since 2016). Second, the proposed model is only evaluated on one dataset, CoQA, where the proposed model gives much worse performance than BERT baseline or other models. In particular, the fact that the proposed model leverages BERT but is worse than simply finetuning BERT (76.6 vs 78.7, single model) means that the proposed model does not contribute to the performance. In addition, the proposed model is significantly worse than SOTA in the leaderboard (90.4 single and 90.7 ensemble using RoBERTa, 86.8 single and 87.8 ensemble with ConvBERT, which I do not know about the architecture but probably also uses BERT, not better pretrained models). These baselines, including just finetuning BERT, are not reported in the paper, although they are accessible in the public leaderboard.\n"
        }
    ]
}