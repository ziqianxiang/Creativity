{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes an approach for architecture search by framing it as a differentiable optimization over directed acyclic graphs. While the reviewers appreciated the significance of architecture search as a problem and acknowledged that the paper proposes a principled approach for this problem, there were concerns about lack of experimental rigor, and limited technical novelty over some existing works. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "I. Summary of the paper\n\nThis paper describes a principled strategy for searching for the most\nsuitable neural network architecture out of a particular class of\narchitectures. Specifically, the problem is framed as an optimisation\nproblem over a set of directed acyclic graphs (DAGs) that correspond to\npotential network architectures. By optimising the edge weights of this\nrepresentation, a suitable architecture can be generated.\nIn addition to the aforementioned optimisation scheme, the paper also\npresents a regularisation that results in *sparse* networks, i.e.\nnetworks with a smaller number of edges. Multiple experiments on\n'tuning' existing architectures on several data sets conclude the paper.\n\nII. Summary of the review\n\nThis paper discusses a highly relevant subject, namely how to select\nneural network architectures in a principled manner. While the presented\nwork already goes into a good direction, I cannot give it my endorsement\nfor acceptance because of the following reasons:\n\n  - The paper is lacking clarity: concepts could be explained somewhat\n    better, and the paper is suffering from language/grammar issues that\n    make it harder to understand the contents.\n\n  - Lack of experimental or theoretical depth: the proposed method is\n    presented as-is; no theoretical analysis of its behaviour is\n    performed; while this is not necessarily a problem, as there are\n    several empirical experiments, the experimental section is not\n    sufficiently detailed: for example, no limitations of the method are\n    being discussed and the presented results are not state-of-the-art\n    accuracies.\n\nNevertheless, I want to point out that this paper has the potential to\nbecome an important contribution to the community; it is absolutely\nclear that more principled approaches are required to select network\narchitectures.\n\nIn the following, I will comment on the individual aspects in more\ndetail.\n\nIII. Detailed comments (clarity)\n\n- The abstract could be improved in terms of its logical flow. Instead\n  of trying to introduce new terminology (macro/micro etc.) here, the\n  abstract should rather state directly that this paper frames network\n  architecture selection as an optimisation problem over DAGs.\n\n- The use of topology is slightly non-standard here. What is the meaning\n  behind the 'macro' and 'micro' operations? This should be explained\n  somewhat better.\n\n- Figure 1 should be extended to show an example of how the depicted\n  graphs are described through the terminology mentioned in the paper.\n  For example, individual edges or nodes could be highlighted and\n  referred to in the text to make the 'mapping' clearer.\n\n- I do not understand how operations such as *addition* are represented\n  in the DAG. Ideally, this should also be elucidated by a figure.\n\n- When discussing 'intervals' of residual connections, I am assuming\n  that the paper refers to how many layers are skipped? If so, this\n  should be mentioned and defined explicitly.\n\n- The term 'searching space' should be replaced by 'search space', as\n  the latter is more standard usage.\n\n- I do not understand why the optimisation of the topology can decrease\n  the computational burden, as claimed on p. 2. The optimisation process\n  still has to be performed, just like the training of the network,\n  correct? Am I misunderstanding this?\n\n- The caption of Figure 2 could be extended; does a single node type\n  mean that the complete network only consists of nodes of that type?\n  Moreover, accuracies/errors should be shown in addition to the loss\n  curves.\n\n- The term 'dense connection' is vague; I think the paper should use\n  'densely-connected graph' here.\n\n- The sentence 'Among these nodes, [...]' refers to the *whole* network,\n  and not to the way the output tensor $\\mathbf{x}_i$ is processed. Am I\n  understanding this correctly?\n\n- I do not understand the initial sentences in Section 2.2; what is the\n  meaning of 'cell' in this case?\n\n- The term 'topological structures' should be renamed to '(sub)graphs'\n  in order to improve clarity.\n\n- I do not understand the comment on sparsity in Section 2.4. How are\n  'moderate sparsity' and Figure 2 connected?\n\n- In the algorithm, I would use 'Sparsity Type' instead of 'Sparse Type'\n  to refer to the parameter.\n\n- What does 'Complete' (without $\\mathbf{\\alpha}$) mean in Table 3?\n\n- The footnote below Table 3 is not referenced anywhere in the text or\n  in the table.\n\n- In Figure 3, are the adjacency matrices consistent? What happens if\n  the training process is repeated? It would be highly interesting to\n  show 'averaged' matrices over multiple runs.\n\nIV. Detailed comments (experiments & theory)\n\n- A theoretical analysis of the proposed method would be interesting.\n  Does the optimisation always converge? Are minima unique? What is the\n  computational complexity?\n\n  At least some of these aspects should be discussed.\n\n- The limitations of the proposed method are not explained. For example,\n  what is the meaning of the sentence on p. 2 about 'excluding the\n  influence of the mixture of different layers/nodes'?\n\n  It is my understanding that the proposed method can only change the\n  *connections* between blocks of a network, but not the type of layers.\n  Is this correct? If so, it would be a major limitation that should be\n  mentioned explicitly.\n\n- Another limitation that is not discussed is the scaling to very deep\n  networks. How problematic is it to model all potential connections in\n  such a network? Are there limits to the current optimisation scheme?\n  This needs to be assessed in the experimental section.\n\n- For all experimental tables, standard deviations and means should be\n  provided. This is necessary in order to assess the stability of the\n  proposed method, because there are multiple sources of stochasticity:\n  one arising from the optimisation procedure, the other one arising\n  from the training of the network itself.\n\n- The results reported for the experiments are somewhat behind the\n  state-of-the-art in terms of accuracy values. This should be stated\n  more clearly; I assume that it is caused by limitations of the\n  proposed method, which prohibit an application to very recent\n  architectures. Is this correct? If so, it should at least be\n  mentioned.\n\n- The claim that nodes at the start of a topological ordering contribute\n  more to specific stages needs to be (empirically) proven.\n\nV. Style issues\n\nThe paper is not easy to read because of several non-standard phrases or\nexpressions.\n\n- The phrase 'in topology' is often added to a sentence where it does\n  not entirely make sense. For example, '[The] architecture can be\n  expressed as a directed acyclic graph (DAG) in topology'. I do not\n  see the necessity of adding 'in topology' here. There are other places\n  at well from which I would remove this phrase.\n\n- 'effective networks' --> 'effective network architectures'\n\n- 'largely affects' --> 'largely affect'\n\n- 'Motivated by which' --> 'Motivated by this'\n\n- 'innovative method' --> 'method' (or 'novel method')\n\n- 'as a complete graph, through' --> 'as a complete graph, and through'\n\n- 'auxiliary sparsity constraint' --> 'an auxiliary sparsity constraint'\n\n- 'named as TopoNet' --> 'called TopoNet'\n\n- 'At initial periods' --> 'Previously' (I am not sure I understand\n  this correctly)\n\n- 'red signs' --> 'red arrows'\n\n- 'for its topology' --> 'in terms of its topology' (?)\n\n- 'both combining' --> 'both a combination' (?)\n\n- 'number of interval' --> 'number of intervals'\n\n- 'straight connected' --> 'directly connected'\n\n- 'conduct transformation' --> 'performs a transformation'\n\n- What is the meaning of the phrase 'These may cover the influence\n  [...]'? Is this a reference to limitations of existing networks?\n\n- 'opted from' --> 'chosen from'\n\n- 'Following two simple design rules' --> 'We follow two simple design\n  rules'\n\n- '1000-dimension' --> '$1000$-dimensional'\n\n- 'We raise two ways' --> 'We describe two ways'\n\n- 'consited' --> 'consisted' / 'consists'\n\n- 'deepen the depth' --> 'increase the depth'\n\n- 'origin' --> 'original'\n\n- 'promotions' --> 'improvements'\n\n- 'can make more profit' --> 'can be useful to improve performance' (I\n  am guessing this from the context)\n\n- 'sparseness on representation' --> 'sparsity'\n\n- 'Adaptive one' --> 'The adaptive one'\n\n- 'At the fore' --> 'At the beginning/start'\n\n- I do not understand the sentence about the 'free lunch'. Does it refer\n  to the fact that some connections can still be removed from the\n  network without decreasing accuracy?\n\n- 'less computation costs' --> 'lower computation costs'\n\n- 'shortcut offers' --> 'shortcuts offer'\n\n- 'benefits optimization' --> 'benefit optimization'\n\n- 'feasible way to the optimization' --> 'feasible way for the optimization'\n\n- Some references in the bibliography are not capitalised consistently"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #5",
            "review": "-------------------------      Update after rebuttal    ---------------------------\n\n\nThank you for addressing my concerns. I feel the rebuttal did improve the paper, e.g., the significance of results can be evaluated better now. I still like the overall idea of the paper as optimizing connectivity patterns in architectures has so far mostly been ignored while it is actually straight-forward to do (as shown in this work).  I increased my score accordingly. However, the novelty and significance of this work is still limited in my option and therefore I do not argue heavily in favor of accepting this submission.\n\n\n--------------------------------------------------------------------------------------------------------------------------------------------------\n\n\n\n\nThe authors propose a method for learning the connections/ connectivity pattern (dubbed: the topology; meaning which layer is directly connected to which other layer) in neural networks. They do so by weighting connections between layers (e.g., by weighting skip connections) with a real valued parameter. This real-valued parameterization of the connections is then optimized by gradient descent along with the weights of neural networks. The authors also propose L1 regularization on the connectivity parameters to induce sparsity. The proposed method is evaluated by optimizing the topology for ResNets, MobileNetsV2 and their proposed “TopoNets”.\n \nOriginality and significance. The manuscript addresses an interesting problem: while there has been lots of work on manually designing better architectures as well as automated design (a.k.a. neural architecture search, NAS), there is little work on optimizing the overall topology (meaning the connectivity patterns between layers). Most prior work solely focuses on search for blocks or cells and then stacking these cells in a pre-defined, not-optimized manner. However, there has been some work also including this in architecture search (e.g., [1,2,3]), and especially the work [4] seems very related but is not discussed. The authors of [4] propose, very similar to this submission, a gradient-based optimization of the connections (in a different way though). The proposed method for optimizing the topology is also very similar to DARTS, simply applied to the connectivity pattern rather than on the operations-level. However, here the topology is optimized along with the network’s weights on the training data rather than the bi-level optimization from DARTS, where the architectural parameters are optimized on the validation data instead (which is very reasonable as one usually considers the architecture as a hyperparameter). I wonder if this has also been considered/tested by the authors of this paper as I would consider the topology to be a hyperparameter which should not be optimized on training but rather validation data. Knowing [4] and DARTS, the proposed method seems to be rather incremental and straightforward rather than ground breaking. While the proposed method allows for more flexible topologies, it introduces different “stages” for their TopoNets, which are actually a similar concept as blocks or cells from the NAS literature. This again does not allow connections between arbitrary layers (but rather only between layers in the same stage; to the best of my understanding). Empirical results show rather small improvements and their significance is unclear (see next paragraph).\nClarity and quality. The paper is mostly well written, well motivated and easy to follow. The mathematical formalism is a little vague at some points (e.g., in Section 2.1., the notation G is used for defining a graph and later in Equation (2) as a function computing feature maps in networks). While the literature on manual design of architectures is thoroughly reviewed, there is missing related work in the context of neural architecture search, as already discussed above. The quality of the results is questionable as differences in accuracies are in almost all experiment rather small (e.g., tuning MobileNetsV2 on Imagenet: 72.62% (original) Top-1 accuracy vs. 72.84% (optimized) and it seems that the authors do only report results for a single run of experiments. In order to assess if the differences are actually statistically significant, the authors would need to report several runs and would need to state, e.g., means and standard deviations. \n\nOverall, the authors address an interesting problem, which seems to have fallen into oblivion in current NAS literature: while researcher optimize cells, which are then stacked to build the final model, not many researcher look into connectivity patterns / topology on the macro level, meaning connections across cells and how cells should be stacked. This paper addresses this problem to some extent. However, the proposed optimization method is, in my opinion, rather incremental (with respect to DARTS and [4]) and therefore of limited novelty and significance. It is currently hard to assess the empirical results due to rather small improvements and the lack of repeated runs of experiments. Mainly for these two reasons, I do not recommend the paper for acceptance.\n\n\n\n[1] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Quoc V. Le, and Alex Kurakin. Large-scale evolution of image classifiers. ICML, 2017.\n[2] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efficient multi-objective neural architecture search via lamarckian evolution. ICLR, 2019.\n[3] Hieu Pham, Melody Y. Guan, Barret Zoph, Quoc V. Le, and Jeff Dean. Efficient neural architecture search via parameter sharing. ICML, 2018\n[4] Karim Ahmed and Lorenzo Torresani. Maskconnect: Connectivity learning by gradient descent. ECCV, 2018\n\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #3",
            "review": "*UPDATE* I have read the other reviews, authors' comments and the revised version of the manuscript. I have modified my rating to accept. The updated version, with variance across runs reported, comparison to randomly wired networks, and clearer writing, is substantially better. The core idea is simple to understand in retrospect, and could lead to more follow-up work in the vein of DenseNets and DART (with a more constrained search space).\n\nThe paper proposes a refinement of the idea behind DenseNets -- rather than summing over all previous layers' outputs, sum a weighted combination instead where the weights are learned. This idea can be extended to search through the space of all possible residual connections, which they call TopoNet. This is practically achieved by enforcing a sparsity constraint on the learned weights. There is an additional nuance when enforcing the sparsity constraint: downstream layers have many more incoming residual connections, and may need an appropriately scaled sparsity penalty.\n\n It may help to clarify in the text that weights can be positive or negative (the current motivation from the point of view of residuals at different intervals suggests all the weights should be non-negative).\nTable 3 is baffling. Were the initial edge values (column 2) chosen so as to make the number of params and FLOPS somewhat comparable across different rows? How were these initial edge values set (seems very specific for N_3 to go from 46 to 103 when residual interval goes from 4 to 2, etc.)? The comment that number of params and FLOPS changes are negligible is puzzling; clearly Random, p=0.01 should use much fewer FLOPS than Random, p=0.99.\nWithout additional information behind the numbers for the baselines in Table 3, it is unclear if TopoNets indeed give an improvement over the baselines (Random and Residual).\nThe text will also benefit from a careful elaboration of the differences in the Random baselines in the paper vs. the Xie et al approach of trying random architectures.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The authors proposed a method to optimize the topology of neural networks in a soft fashion. The main idea is to formulate the network as a complete graph (or a sequence of complete subgraphs), and to optimize the relative importance of each edge using gradient descent. The overall approach is similar to differentiable architecture search, except that (1) the continuous architecture is optimized wrt the training set (instead of the validation set), and (2) the learned architecture is never discretized at the end of training.\n\nThe paper is well-organized and easy to follow. The authors have also conducted controlled experiments to convincingly show that the method is leading to improvement. \n\nI'm a bit concerned about the technical novelty, however, as the approach can be viewed as an application of (a simplified version of) differentiable NAS to a search space analogous to the one used in [1]. In fact, the notion of soft topology has already been introduced in this prior work (Figure 2 in [1]: \"The aggregation is done by weighted sum with learnable positive weights w0, w1, w2\"), which was also optimized using gradient descent. A difference between this work and [1] is that whether the underlying graph is complete or randomly generated, but such a distinction is minor (we can always get densely connected random graphs by adjusting the hyperparameters of the graph generator).\n\nIn addition, I'm not sure whether the learned continuous \\alpha can be conveniently referred to as \"topology\". Note the mathematical definition of topology is discrete by nature. I believe the authors would need to either revise this terminology (e.g., by referring to it as “soft topology”, as a generalized definition of hard topology), or provide a way to induce discrete subgraphs from the continuous architecture. Sparsity regularization alone may not be sufficient as the non-zero \\alpha's are still real-valued.\n\nMinor issue:\nI like Figure 1 a lot. However, it seems the equivalence between the 3rd and the 4th sub-figures in Figure 1 can only be established for ResNet-V2 blocks, where there is no ReLU after each addition. It is not immediately obvious how this analysis can generalize to ResNet-V1 blocks (which still offers reasonably good empirical performance).\n\n[1] Xie, Saining, et al. \"Exploring randomly wired neural networks for image recognition.\" arXiv preprint arXiv:1904.01569 (2019)."
        }
    ]
}