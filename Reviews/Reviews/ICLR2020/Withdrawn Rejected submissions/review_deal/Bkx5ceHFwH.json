{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper studies the role of auxiliary semantic supervision for visual navigation tasks. The paper proposes auxiliary losses of the form or predicting the room the agent is currently in, the rooms around the agent, and the next room that the agent should go to. The paper applies these ideas in context of the RoomNav task in realistic indoor scenes. The paper also proposes a self-supervised imitation learning that  is used to improve the model in novel previously unseen environments.\n\nStrengths:\n1. The paper tackles an important problem in a good and realistic experimental setup.\n\nShortcomings:\n1. Contributions are weak. Paper proposes use of auxiliary losses as a way to include semantic information into the policies (and use of self-supervised imitation learning). The idea is straight-forward, but would still make for a good paper is it was evaluated thoroughly and was found to work very well. I am not sure if this is the case either:\na. Improvements are typically small and conclusions are different when looking at different metrics. Some of the proposed losses seem to be hurtful (ex CS_RS)\nb. Ablations have been conducted on the test set. They should be done on a held out validation set and the best model should be evaluated on the test set against baseline (and other methods as applicable). Given the small improvements and that ablations are being done on the test set itself, I am not sure if we can conclude that the method works better than the baseline.\nc. Lack of any comparison to past work. I can think of at least 2 papers which have tackled semantic goals on similar datasets before [A,B]. Some of them also have code and experimental setups available, so comparisons should be attempted.\n\nThus, in my view the paper has limited technical depth, weak experimental evaluation and inconclusive results.\n\n\n[A] Bayesian Relational Memory for Semantic Visual Navigation (ICCV 2019)\nYi Wu, Yuxin Wu, Aviv Tamar, Stuart Russell, Georgia Gkioxari, Yuandong Tian\n\n[B] Cognitive Mapping and Planning for Visual Navigation\nSaurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik\nComputer Vision and Pattern Recognition (CVPR), 2017"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This submission aims to understand whether common sense and semantic understanding can benefit in a room navigation task. The authors design several interesting auxiliary tasks to understand the significance of common sense and semantic understanding for room navigation. They conduct experiments to understand the importance of each auxiliary task empirically.\n\nHowever, the clarity and quality of writing is poor which makes it difficult to follow the manuscript at many places and evaluate its significance. Some auxiliary tasks are not defined well, several ideas are not motivated well and some paragraphs seem unnecessary. I find the experiments inconclusive due to lack of statistical significance testing, insufficient data, and inadequate hyperparameter tuning. More details in the following comments.\n\nComments on Section 3:\n\n- The agent architecture description is not easy to follow. Terms are used without defining them. For example, acronyms CS and SU are stated in Sec3 Baseline model without defining what they stand for or what they mean, in Sec 3 Input, what are “ generating grounding questions”?. There are too many acronyms to keep track of and they are often unintuitive until you read the whole section. There are many cross-references between paragraphs which makes it very difficult to follow the description. For example, In Section 3.1 “generic sequence of rooms (between source and target room) generated by the common sense planning module (CS_RS)”. Here, what is a generic sequence of rooms? How are they generated? What is the common sense planning module? Why is it abbreviated CS_RS? \n\n- Why is Das et al. (2017a) cited for LSTM baseline? I can think of many papers before this doing reinforcement  and Imitation learning using LSTMs or recurrent networks.\n\n- I do not understand the motivation behind the proposed method of self-supervised learning. The prediction of SU_RD (room detection) is used as labels for fine-tuning SU_CR (current room detection) and SU_PN (post navigation grounding) in unseen environments. Why would SU_RD generalize well on unseen environments but not SU_CR and SU_PN?\n\n- What is the need for SU_CR (current room detection) when SU_RD (room detection) already consists of current room? Why not just use SU_RD as an auxiliary task for SGN instead of SU_CR?\n\n- The motivation behind SU_PN is unclear. Why does the agent need to remember which rooms during the trajectory when it stops? How is this supposed to help in navigation?\n\n- The definition of some auxiliary tasks is not clear. It is not clear how the labels were generated for some auxiliary tasks. What are the left, front and right rooms in SU_RD? Do they need to be visible in the current image or does the agent need to predict likely rooms which are behind walls? What if there are multiple rooms in one direction? The CS_RS task is also not defined clearly. It seems from Figure 2 that the input to the CS_RS model is the sequence of rooms in the ground truth trajectory but it is not stated anywhere. What is it predicting? What are “generic room sequence patterns”?\nHow are the labels of CS_RS, SU_RD generated?\n\n- If CS_RS is taking the sequence of rooms in the ground truth trajectory as input, the model makes a strong assumption of having access to sequence of rooms to reach the target room at test time. It also makes the comparison to the baseline unfair. If it is not taking sequence of rooms as input, what is the input to the CS_RS model?\n\n\n\nComments on Section 4:\n\n- What is the meaning of ‘games’ in this context? I am assuming trajectories.\n\n- What hyperparameters were tuned using the validation set?\n\n- Looking at the results in Table 1, I am not sure whether the difference in the performance of different models is significant or not. I believe statistical significance tests need to be conducted especially because the difference between the performance of different models is small and the test set consists of only 324 trajectories, which are further split into easy, medium and hard sets. \n\n- The performance of several models is unintuitive in many places. For example, the performance of CS_RS lower than baseline in medium trajectories, CS_RS improves the performance when added to CS_Nxt in easy and medium but not in hard, SU_PN is better than SU_CR in medium but not in easy, however, SU_PN+CS_Nxt is better than SU_CR+CS_Nxt in easy but not in medium. SU_PN+SU_RD is worse than SU_CR+SU_RD across all settings but SU_PN is overall better than SU_CR. The authors have provided some explanation for some of the above but not for all. I am suspecting that many of the above observations are due to the variance in the results, because of small test sets and not due to the difference in auxiliary tasks. And this again raises questions about the statistical significance of all the results.\n\n- Several statements in the results indicate inadequate hyperparameter tuning such as “We hypothesize that hyperparameter tuning is important in combining different modules” and  “the performance generally degrades as the agent tends to focus more on the auxiliary tasks than the action prediction task”. I believe it is essential to tune the hyperparameters for auxiliary tasks if the goal is to understand the importance of auxiliary tasks empirically. \n\n- What is the purpose of Section 4.2? What are we supposed to learn from this experiment?\n\n- It is not clear how are the embeddings visualized in Sec 4.4 and Figures 5 and 6. How are the embeddings projected in two dimensions? How are they aligned?\n\n\n\nSuggestions for improvement:\n\n- The writing can be improved significantly. I suggest first describing all the auxiliary tasks and then describing the agent architecture for training the auxiliary tasks. The description of auxiliary tasks can be improved, especially SU_RD and CS_RS.\n\n- I suggest increasing the size of the test set and performing statistical significance tests for making sure that the difference in performance numbers is not due to noise/variance. \n\n- The manuscript needs to be proof-read. There are some grammatical errors."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "\nPaper Summary: The paper addresses the problem of Room Navigation, where the goal is to navigate to a room that is provided as target using language. The paper tries to incorporate common-sense knowledge and semantic understanding (room detection) via a set of auxiliary tasks. The model has been trained using three techniques: (1) imitation learning, (2) imitation learning + reinforcement learning and (3) self-supervised imitation learning.\n\n* Originality:\n\nThe idea of using detectors and prior object-room or room-room knowledge for navigation has been around for a while (for example, Mousavian et al., ECCVW 2018, Yang et al. ICLR 2019, etc). There is not much novelty in the model or the training strategies either, and standard techniques have been used.\n\n* Quality:\n\n- There is no comparison with any other method in the paper. The authors mention \"we do not know of any other work related to RoomNav on MatterPort3D environment and therefore we could not directly compare against previous work\". There are several navigation papers with released code. They could have applied those methods on this dataset.\n\n- There are some claims that cannot be justified. For example, it is mentioned that \"These approximate maps obtained from embeddings can be an alternative to SLAM\". The structure of the environment such as connectivity of the rooms is obtained by SLAM, while these embeddings cannot provide such information. They are just locations of some rooms in the training set.\n\n* Clarity:\n\n- Some details are missing. For example, it is not clear how the questions is generated in SU_PN task or what the parameters are for RL training.\n\n- It is hard to read the information from Figure 3. Also, standard deviation should be provided for RL experiments.\n\n- In Table 1, what is the difference between the bottom two rows and SIL baselines above it?\n\n- There is no additional information provided by SU_RD during self-supervision as room detection is already included in the model. Why should it improve the performance?\n\n* Significance:\n\n- A number of auxiliary tasks have been introduced, but for each experiment a different subset of them is being used due to inconsistency of the results. This shows that proposed auxiliary tasks are not really helpful in most scenarios. It is mentioned that \"when several auxiliary tasks are introduced to the agent, the performance generally degrades as the agent tends to focus more on the auxiliary tasks\". What is the point of the paper then? In some cases even adding one auxiliary task makes the performance worse. For example, it is strange that CS_nxt is much better than CS_nxt+CS_RS.\n\n- When the method is trained using imitation learning+RL, the performance drops significantly (from > 0.15 to less than 0.07). What is the advantage of using RL?"
        }
    ]
}