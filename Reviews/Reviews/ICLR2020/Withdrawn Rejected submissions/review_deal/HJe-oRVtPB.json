{
    "Decision": {
        "decision": "Reject",
        "comment": "The article studies the stability of ResNets in relation to initialisation and depth. The reviewers found that this is an interesting article with important theoretical and experimental results. However, they also pointed out that the results, while good, are based on adaptations of previous work and hence might not be particularly impactful. The reviewers found that the revision made important improvements, but not quite meeting the bar for acceptance, pointing out that the presentation and details in the proofs could still be improved. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper presents a non-asymptotic analysis of ResNet stability which determines a 'cutoff' value for the residual block scale factor characterizing stability vs output explosion, and improves upon prior results by finding a larger range for the scale factor that lead to global convergence in non-convex optimization. Theoretical findings are corroborated via experiments confirming the validity of the 'cutoff' value. Additional experiments are conducted indicating that using the cutoff value, ResNet can be trained even without normalization layers, and that the cutoff value is also beneficial with normalization as it allows effective training of very deep ResNet.\n\nThis is an interesting submission which presents important theoretical results while also showing their practical pertinence via experimental validation. \n\nThe paper is well written and the presentation is clear. Prior work is reviewed appropriately.  The reviewer found it particularly informative to provide intuition and present a proof sketch next to each theoretical results. \n\nThe convergence analysis leads to a depth dependence of ResNet and the authors claim that this is not a *real* dependence and  attributes it to bounding techniques handling non smooth activation. The authors further indicate in their experiments that for a given width the convergence of ResNet does not depends *much* on depth, compared to feedforward networks. So there might still be a dependence though much milder than that found by the current theory.  It would be interesting to more precisely characterize the dependence observed in experiments.\n\nThe authors claim that their analysis leading to the non-asymptotic bound on the spectral norm of the feedforward process via martingale theory might be relevant for other problems. It would be useful if the authors could elaborate and in particular indicate if their technique could carry to analyse other structures beyond ResNet. \n\nMinor comments:\n\"What else values\" ---> Are there other values \n\"our first claim is a new spectral norm\" -->  is a new bound of the spectral norm\n\"we does no make such\" --> we do not make\n\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors analyze the stability of a randomly initialized ResNet in terms of the scaling of the network and its depth. They claim to characterize the spectral stability of a randomly initialized network (and local perturbations of it), as well as that of the forward and backward process. These results, along with some further results, are used to extend convergence proofs of ResNets towards more standard initializations. Some of the claims are evaluated empirically.\n\nUnfortunately, this paper is not a good contribution at the moment, as the work, although potentially interesting, is somewhat incremental and poorly presented. In particular, the results (including the proofs) only extend the results from Allen-Zhu et al, and are often hard to follow. The presented empirical results are nonetheless interesting and should be expanded upon. Here are some detailed comments\n\n- The theorems should be stated more carefully, and in particular, care should be given to make precise which constant terms are hidden away in the big-O and big-Omega notations. E.g. in theorem 2 the probability must have some dependency on c hidden away, which should be made explicit, especially given the assertion “where c can be arbitrarily small”. Similarly, in theorem 1, it is claimed that c is constant, but the theorem is only non-vacuous for either m growing or c ~ log L. There are also some typos in theorem statements (e.g. missing O in theorem 3 and lemma 3), which can easily confuse the reader.\n\n- Although I did not have time to check all the proofs, they should be treated with more care. I recommend avoiding big-O and big-Omega notations in the proofs as much as possible, as it can make some steps extremely confusing: in the proof of theorem 1, after “taking an \\epsilon-net over …”, the probability does not change despite applying an union bound! The union bound argument should be more clearly spelled out, especially given the fact that some smoothness should also be established for this argument (e.g. the trivial sub-multiplicative bound).\n\n- The empirical results are interesting, and I think could be taken even further. Indeed, the authors show promising results for learning a global scale parameter (in terms of performance), and it would be interesting to explore: 1) the value of the learned \\tau after training: is it still of the order 1 / \\sqrt{L}? Or does it take some other value. 2) whether it is better to have a \\tau per layer or one for the whole network. The presentation of the current results could be improved by including standard deviation when averages are reported, and changing figure 4 with axes starting at zero (bar charts which do not start at zero are extremely misleading).\n\n- There are also many typo and grammar issues. Although these do not seriously impede understanding, the paper could be improved by addressing those (at least running a spell-check)."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "This paper shows the stability of ResNet for the output scale of resblock: $\\tau < \\sqrt{L}^{-1}$ and shows the explosion for $\\tau > L^{-0.5+c}$. Based on this analysis, a linear convergence rate of the gradient descent for the squared loss is also shown. In addition, this paper empirically verifies the efficiency of the initialization of $\\tau = \\sqrt{L}^{-1}$ with/without batch normalization.\n\nContributions of the paper are summarized below:\n- Provides a sharp characterization of the largest scale of outputs of resblocks: $\\tau$.\n- Enlarges the class of ResNets with the global convergence guarantee of the gradient descent w.r.t. the scale of $\\tau$. (A related study [Allen-Zhu+2018b] focused on the scale of $\\tau = 1/L$).\n- Improve the depth dependence in the network width and the iteration complexity compared to existing studies.\n\nSignificance:\nTo theoretically justify the success of ResNets, this line of research is important, and I think this paper makes a certain contribution in this sense, although there seems to be much room for improvements regarding the depth dependence in the network width. (Unrealistically high over-parameterization: $m=n^8L^7$ is still required). In addition, a proposed technique (Theorem 1) to derive gradient bounds is surprising and beyond the intuition. Indeed, a simple way of using a natural spectral bound cannot explain the stability of ResNets with $\\tau=L^{-0.5}$.\n\nClarity:\nThe paper is well organized but there are some technical concerns as described below.\n\nQuality:\nI think the quality of the paper can be improved. \n- Maybe probabilities in some statements are not correct, for instance, a probability in Theorem 2 should depend on the value of $c$.\n- It is better to specify the value of $c$ in Theorem 1. It may take an arbitrary small value at the expense of the probability.\n- In the convergence analysis of the gradient descent (Theorem 5), the dependency on $\\tau$ should be mentioned. Is there no effect on the convergence rate by the choice of $\\tau$?\n\nMinor comments:\n- The network architecture is different from the standard ResNets. Usually, non-linear activation functions are applied after resblocks. Can theoretical analyses be extended to such a setting?\n- Initialization scales of parameters in related studies, as well as $\\tau$, should also be clarified for a fair comparison of $\\tau$.\n- In proofs, citations of theorems and lemmas seem incorrect, e.g., Theorem 1 may be used in the proof of Lemma 3 (and Theorem 3 too?) instead of Lemma 1.\n- Typo In Theorem 1: for all $a > l$  --> for all $b > a$ (?).\n- There is no definition of $h'_{i,l}$ in Lemma 4. Is it a perturbation of a hidden node h?\n\n-----\nUpdate: \nI thank the authors for the response and hard work.\nI would like to keep my score. Some concerns have been resolved. Moreover,  I think highly of a proof technique (Theorem 1). However, the advantage of ResNets is still unclear because high over-parameterization is required.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."
        }
    ]
}