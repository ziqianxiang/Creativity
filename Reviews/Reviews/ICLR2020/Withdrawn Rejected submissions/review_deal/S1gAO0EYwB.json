{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes an approach for learning disentangled representation in GAN models, while having an imbalanced training data. They build their model on top of the InfoGAN model. They also show that when the InfoGAN model is trained on imbalanced dataset, they produced imbalanced sample outputs.\n\nPros:\n1. The setting is very useful as most of the public datasets are imbalanced.\n2. The background on InfoGAN is clearly explained, makes it easy for any new reader\n3. The paper is clearly written and is very easy to follow\n4. The ablation studies is the key for this paper - it is well performed and neatly explained.\n\nCons:\nThis involves a couple of questions at philosophical level and few at technical level:\n1. I find the comparison between InfoGAN and Elastic InfoGAN biased in terms of imbalanced dataset experiment. While the Elastic InfoGAN gets the Gumbel smoothened coding vectors as input (c), the InfoGAN gets only a hard uniform probability distribution Cat(K = 10, p = 0.1) as input. Of course, this is with the assumption that InfoGAN authors have made with balanced dataset. The easiest extension to imbalanced datasets, is to have a non-uniform probability distribution p, Cat(K = 10, p = []), based on the imbalance in the training data. This would be a more suitable comparison of InfoGAN and Elastic InfoGAN\n\n2. [Philosophical] Even for an imbalanced dataset, I really dont find InfoGAN performing poorly, especially in Figure 1 (center). Most of the digits are still performing good, while there are obvious and interesting to observe overlap between (6,9) , (2,8), (3,7). I would not consider this poor learning from imbalanced dataset. As a matter of fact, a similar number of overlap is found for Elastic InfoGAN as well in Figure 6\n\n3. Also, the center image in Figure has many digits, still as random shapes. I really wonder, if the authors have kept all the hyperparameters constant between center and the right figure. Or maybe, the center figure is the output of an earlier epoch while the right figure is the output of a later epoch! \n\n4. I do not completely understand the reason for adding the L_ent part to the loss function. A part of the entropy loss is already handled in the L_infogan part. The authors of the InfoGAN paper, in the Eqn 4 of their paper has quoted this, \"However, in this paper we opt for simplicity by fixing the latent code distribution and we will treat H(c) as a constant.\" This is the exactly equivalent to L_ent of this paper. And to further state that L_ent can be an overkill in the loss function, we can observe in the results that in Table 1 for YTF, the ENT is performing poorly after adding L_ent loss.\n\n5. Also, this paper fails to mention, discussion, and compare an important references:\na. \"SCGAN: Disentangled Representation Learning by Adding Similarity Constraint on Generative Adversarial Nets\"\nThe similarity measure proposed in SCGAN and the L_dist proposed in Elastic InfoGAN is essentially the inverse same of each other.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Contributions:\n1. The paper modifies InfoGAN by replacing the uniform categorical distribution with a Gumbel-Softmax to adapt imbalanced class distribution.\n2. The paper proposes a data augmentation method to learn object identities invariant of predefined transformations.\n3. The paper evaluates its method on two datasets to demonstrates its effectiveness.\n\nThis paper fixes some shortcomings in InfoGAN, but it is hard for me to categorize it into an unsupervised method. Besides, its contribution seems incremental to me so I will vote for a reject.\n\nDetailed comments:\n1. The first concern to me is the data augmentation since the data augmentation itself seems to incorporate human supervision regarding the pre-defined transformations. Since traditional disentanglement methods barely rely on observations, it is hard to categories this method as an \"unsupervised method\". In Sec. 4.6 in the experiment, the proposed method seems \"re-learned\" the transformed data, which against the unsupervised setup. Moreover, this supervision could be unavailable for more complicated datasets, and the data augmentation scheme may bias the true data distribution.\n\n2. I compared the interpolation result in Figure 6 with a similar experiment in InfoGAN. But I found the interpolation quality in this paper is even worse than the one shown in InfoGAN. For example, I cannot see a salient stroke width change in RHS. of Figure 6, while InfoGAN learns a much more significant pattern change.\n\n3. I think this paper makes some modifications to InfoGAN, but I cannot see significant novelty since all modules presented are existing in the machine learning literature. I do think the imbalanced adjustment will work in this case, but I do not think this is an urgent problem to solve base on InfoGAN."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Overview:\nThis work introduces a GAN-based generative model that allows to learn disentangled representations of images, and in particular automatically extract the object identity (e.g. digit number or human identity).\nThe model is based on an earlier approach, InfoGAN, and adapts it to better model imbalanced data. Namely, authors propose to put a learnable categorical prior (instead of the uniform) that presumably better models the discrete factors of variation in the data. In order to make the process differentiable, authors rely on reparameterization based on gumbel-softmax trick. In addition, authors propose a data-augmentation technique where they apply identity-preserving transformations (rotations / illlumination changes / etc) to the input images while forcing their latent codes to be close to each other. \nNumerical evaluation is conducted on MNIST and Youtube Faces, and indicates that the model performs better than the original in terms of how well it models discrete factors of variation.\n\nDecision:\nGenerally, the proposed solutions make sense: it is meaningful to use a more suitable prior and enforce consistency of latent codes for the same “object identity”. However, it is worth noting that the problem that authors are trying to tackle is in fact very specific, and the proposed solution is only beneficial in settings where there exists a simple categorical representation of object identities and where it is easy to define identity-preserving transformations. \nNumerical results evaluation seems to indicate that the model indeed performs better than InfoGAN, which is not surprising given that the proposed model is essentially tailored to maximize the metrics used. However, some important baselines might be missing.\nIn general, I am not fully convinced that the method is generic enough and that experimental evaluation is sufficient, thus the current rating “weak reject”. However, I believe that the work actually has some potential and open to reconsider given that the concerns are addressed.\n\nQuestions / concerns:\n* The identity-preserving transformations make sense in some scenarios such as images of digits, but in some other cases might be quite hard to define, or require additional expertise. E.g. if one wants to learn generative models of molecules or high-dimensional physical measurements, using standard data augmentation pipeline is no longer trivial. Arguably, one of the reasons to design new methods for unsupervised disentanglement is to avoid these manual decisions. In this case, however, authors in some sense provide an additional supervision signal through these additional manually defined transformations.\n* Comparisons to state-of-the-art VAE-based disentanglement methods such as FactorVAE are missing. It would be also beneficial to see if the difference in performance would still persist when using a more stable version of InfoGAN (as e.g. InfoWGAN-GP from Kim’2018).\n* The application to faces in general makes sense and has potential, but does not look scalable: Figure 5 states that there is an additional latent variable for each identity, which would mean that in practice, when one wants to learn a generative model for face images, we would have to add the number of variables corresponding to the number of identities. What would probably make more sense if one can extract some more meaningful discrete factors of variation such as e.g. gender / hair color / etc. \n* (minor) I find it a bit concerning that the metrics are actually computed on the output of the classifier, which is trained on the real data whereas at evaluation time it is fed with artificially generated images. \n\n"
        }
    ]
}