{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper addresses the problem of deriving better predictive uncertainty estimates from pre-trained models. As the paper notes in the introduction, this is an important problem for machine learning models embedded in real-world decision-making systems, where the consequences of inaccurate predictions may be high, and better insight into model uncertainty can help avoid bad outcomes due to misplaced trust in model predictions. The paper focuses on training-free uncertainty estimation, in which uncertainty estimates are driven by input perturbations and modifications to ‘grey-box’ models (such as application of dropout to a trained model). These methods avoid the computational burden of applying uncertainty estimation during the training process. The paper evaluates the methods on two regression tasks: image super-resolution and monocular depth estimation. Evaluation metrics are based on the correlation between predictive variance and loss. The paper concludes that the training-free methods examined give comparable or better performance for uncertainty estimation, compared with state-of-the-art methods applied during training.\n\nThe paper addresses an important topic, and is generally well-executed and well-written. However, it focuses on a narrow set regression tasks (with little indication that results generalize further), and uses evaluation metrics that are nonstandard and not well-substantiated. For these reasons, I recommend that the paper be rejected.\n\nA systematic study of training-free uncertainty estimation, which is the goal of the paper, would be a significant contribution. The paper chooses to focus on regression tasks, which is a valid narrowing of scope (although the implication that uncertainty estimation for classification is a solved problem, via entropy methods, I believe is incorrect). Image super-resolution and monocular depth estimation, the two regression tasks that the paper examines, are important but do not represent a broad enough set of tasks to justify claims about the general performance of the methods (though they enable the observations in equations (1) and (2)). The performance of the methods on these two tasks alone I don’t believe is a significant enough finding to warrant acceptance to ICLR.\n\nMy second major concern with the paper is in the choice of evaluation metrics, which are based on the correlation between variance of predictions (over a sample of perturbations) and the value of the loss. While I understand how these metrics provide intuition on the quality of uncertainty estimates (i.e. when a model is “more wrong”, ideally it should be more uncertain), the paper does not give a good justification for the validity of the metrics beyond intuition (nor does the cited paper, Zhang et al. (2019)). The correlation metrics obscure a lot of information on the relationship between the predictive distribution and the true values, and without further justification, it is difficult to imagine trusting the resulting uncertainty estimates in a consequential downstream decision-making task. Proper scoring rules (Gneiting and Rafferty (2007), Lakshminarayanan et al. (2017)) could provide a more principled and useful basis for assessing the quality of uncertainty estimates.\n\nSmaller points:\n- The paper makes the implicit assumption that training/test sets are IID. While I understand that prediction on out-of-distribution (OOD) samples is beyond the paper’s scope, high uncertainty on OOD samples is a desirable property for uncertainty estimation methods, and some discussion of this or indication of the methods’ performance on OOD samples would strengthen the paper.\n- The paper implies that uncertainty estimation via input perturbation is a novel contribution; a similar method was used by Liang et al. (2018; https://arxiv.org/pdf/1706.02690.pdf) for the different but related task of out-of-distribution detection.\n- Is there a way to derive error bars for the results in Tables 1 and 2? Some of the correlation coefficients, reported to 4 decimal places, are very close and it’s unclear whether the results are significantly different.\n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper addresses the problem of robustness for deep learning models in computer vision tasks. The authors proposed a new method to quantify uncertainty of output from a trained network under three ways of simple perturbations: infer-transformation, infer-noise, and infer-dropout. The perturbations were only operated during inference without influence the training process. The authors claimed that this method could achieve uncertainty estimation without to re-train, re-design, or fine-tune the model. The authors evaluated the effectiveness of proposed method in two regression tasks: super-resolution (via SRGAN) and depth estimation (via FCRN).\n\nThe paper is a pure experimental one. It is well written and easy to follow. The \"tolerable\" idea and corresponding methods are straight forward and easy to scale up. The reviewer hasn't found major flaws in the problem statements and experiments. The reviewer is not working on related topics and is likely to miss related work. The authors should compare the proposed evaluation methods with existing methods on quantifying uncertainty of neural nets."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper tackles an interesting challenge: can we build neural nets which output a notion of uncertainty in an efficient way: i.e. without having to re-train them or using expensive approximations. The three proposals in this paper are intuitive and practical: perturb the inputs (infer-transformation in the paper), perturb middle layers using gaussian noise (infer-noise in the paper) and perturb middle layers using dropout (infer-dropout in the paper). The authors show results on regression problems such as single image super resolution and monocular depth estimation.\n\nThis paper tackles an interesting problem but I believe it barely warrants acceptance as a conference paper. My concerns are\n- The paper introduces algorithms which output an additional number which they call uncertainty. What it is lacking though is a theoretical foundation/model of what type of uncertainty is captured? In my opinion this paper relies heavily on the correlation of this additional \"uncertainty\" with the error metric in the experiments. For this reason, I classify the paper as sharing an interesting observation but not strong on bringing in new insights.\n- The writing quality of the paper could be better. I find that Section 1, 3, 3.1, 3.2, 3.3 repeat the basic idea of the paper three times. Similarly section 1 and 2 repeat a lot of info on related work. I think this can be done more succinctly.\n- In the foundation sections (e.g. 3.1) the authors assume E[T' o F o T(x)] = F(x). It is unclear why in a non-linear world this assumption is justified? I actually feel that because the authors don't use rotation in their experiment section on monocular depth estimation, they actually themselves don't believe this assumption is true.\n- In section 4.3 the authors introduce the error metric. Although I am not familiar with the previous work they cite, I find the evaluation of the uncertainties insufficient: e.g. a minimal requirement for uncertainties are that they are calibrated."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This submission belongs to the field of uncertainty estimation in neural networks. In particular, this submission described several approaches for measuring uncertainty in an arbitrary neural networks. These approaches are (i) input distortion through a simple set of transformations, (ii) distortion of mapping between input and output using noise added to intermediate layers (iii) distortion of mapping between input and output using the same masking approach as the one employed in dropout. Some of these approaches can be applied to white-box models only and grey-box models only. All of these approaches can be used with black-box models. \n\nAll the ideas described in this work have been published before, the novel aspect the authors are claiming is the absence of distortions in training. I find the content to be very light, the technical description adequate and the experimental validation limited. The presentation of this work has a number of issues:\n\n1) I do not believe what you are doing should be directly called uncertainty. I believe what you are doing is a perturbation analysis where you are using a measure of perturbation stability to assess uncertainty. \n\n2) I believe you are not referencing properly a large body of literature clearly indicating poor uncertainty estimates that arise from the use of MC-dropout. The same applies to ensembles. \n\n3) The technical content is very light and limited to a very specific configuration. You should have considered input distortions for speech, text, image and other types of data. \n\n4) Given how light the technical content is, I expected to see a very solid experimental part focused on comparison of your 3 approaches to published work. Unfortunately, this is not the case. The large bulk of experimental results shows the performance of the proposed approaches at almost every possible settings. Though I appreciate thoroughness, I would rather see the best configuration compared to the best published numbers on most commonly used tasks in the area. "
        }
    ]
}