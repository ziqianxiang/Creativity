{
    "Decision": {
        "decision": "Reject",
        "comment": "The authors proposed a new problem setting called Wildly UDA (WUDA) where the labels in the source domain are noisy. They then proposed the \"butterfly\" method, combining co-teaching with pseudo labeling and evaluated the method on a range of WUDA problem setup. In general, there is a concern that Butterfly as the combination between co-teaching and pseudo labeling is weak on the novelty side. In this case the value of the method can be assessed by strong empirical result.  However as pointed out by Reviewer 3, a common setup (SVHN<-> MNIST) that appeared in many UDA paper was missing in the original draft. The author added the result for SVHN<-> MNIST  as  a response to review 3, however they only considered the UDA setting, not WUDA, hence the value of that experiment was limited. In addition, there are other UDA methods that achieve significantly better performance on SVHN<->MNIST that should be considered among the baselines. For example DIRT-T (Shu et al 2018) has a second phase where the decision boundary on the target domain is adjusted, and that could provide some robustness against a decision boundary affected by noise.\n\nShu et al (2018) A DIRT-T Approach to Unsupervised Domain Adaptation. ICLR 2018. https://arxiv.org/abs/1802.08735\n\nI suggest that the authors consider performing the full experiment with WUDA using SVHN<->MNIST, and also consider the use of stronger UDA methods among the baseline. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents a method for unsupervised domain adaptation. The problem is well known in literature and follows the setting of labeled source and unlabeled target set. This work proposes the “butterfly network” suitable to train on noisy data (labels) and assign pseudo-labels to the target set. The butterfly network consists in two branches one for source+target and one for target only data. Both use the same optimization objective and a “checking” mechanism has been devised for pseudo-labelling the data.\nDecision: weak reject\nMotivation: This paper has some merit but does not present the method in a clear way, it requires some additional effort to go through the method and retrieve from the appendix information useful for full understanding. For example algorithm 2 is a key component of the method and is in appendix, notation is not always clear nor explained (the loss in algorithm 2 is ), the networks F_1, F_2, F_t1, F_t2 could be added in figure 3 for more clear understanding and R(t) along with u_i could be more clearly defined (how to obtain u_i is still not clear to me after several re-reading of the paper). The introduced losses should be better justified. \nAnyway, as I said the paper has some merit, it provides many insights and extensive analysis on “butterfly” method for unsupervised domain adaptation. The experimental section is extensive and demonstrates improvement in performance using this method compared to state-of-the-art, even though for some \"real world\" datasets (e.g. SUN, Caltech, ImageNet) the improvement is not so significant as in the case of MNIST-SYND. Could the authors provide results on MNIST-SVHN to help compare with other papers in literature? Did the authors observe the same evolution of the accuracy (decreasing and increasing after a few epochs) also on the \"real world\" datasets as in MNIST-SYND?\nReplicability: as I said the method is not really clearly explained and therefore I am not confident I could implement and replicate the results. This not because I think the method is complex but because some key components that I pointed out previously about the method are not clear and I strogly believe these are key components to replicate the results.\n"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes a new problem setting in the domain adaptation field. Since it is impossible to obtain perfectly clean labeled source data in the real world, existing UDA methods cannot well handle real-world data. However, in wildly unsupervised domain adaptation (WUDA), we do not need a perfectly clean source data, which means that WUDA problem is a more general and realistic problem than existing ones. \n\nTo address WUDA problem, the authors proposed Butterfly framework (based on dual checking principle) to simultaneously reduce the 1) noise effects in source domain and 2) distributional discrepancy between source and target domains. They tested the proposed method on simulated and real-world WUDA tasks (35 tasks in total), and the accuracy of proposed method is higher than those of representative UDA methods. They claim that Butterfly can eliminate noise effect, which is strongly supported by Figures 2, 4 and 5. They also present the ablation study to show that each part in Butterfly is meaningful. \n\nIn general, this paper is easy to follow and clearly presents the main idea and learning procedures of Butterfly. Since WUDA, as a new problem, could lead a new research direction in the domain adaptation field, this paper should be presented in ICLR 2020. Detailed comments can be seen below.\n\nPros:\n\n+ WUDA, as a new problem, is very important for the domain adaptation field.\n+ Butterfly, as a solution to WUDA, outperforms representative UDA methods on simulated and real-world WUDA tasks.\n+ All claims are strongly supported by experimental results, and ablation study shows that each part in Butterfly is a necessary component.\n+ Following Ben-David's paper, this paper also presents an upper bound of the target domain risk. This is 1) the first WUDA bound and 2) probably the first DA bound related to pseudo labelling function. The conditions in Remark 3 are very interesting.\n+ It is very nice to use noise effect \\Delta to explain the abnormal phenomenon in experiments.\n\nCons:\n\n- The color scheme of figures is not friendly to color-blind people. The authors should do different line styles or marker styles.\n- The interaction between DIR and TSR happens via shared CNN, right? The authors should explain this in the caption of Figure 3.\n- When T = 1, you directly use source data as the pseudo-labeled target data since there are no pseudo-labeled target data in the first step (based on Algorithm 1). Is that correct? If yes, the authors should explain this in Figure 3 and the description of Algorithm 1. If no, please give a detailed explanation.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper introduces the idea of wildly unsupervised domain adaptation, where the source labels are noisy and the target data is unsupervised. To tackle this, the authors propose an architecture based one two branches: one acting on the mixed source-target data and the other on the target data only. During training, each branch is updated using the idea of co-teaching, by finding the samples with the lowest loss values. Pseudo-labeling is then applied to the target data, and the process iterates.\n\nOriginality:\n- In essence, the proposed method combines co-teaching (Han et al., 2018) and pseudo-labeling (Saito et al., 2017). While it goes beyond the naive two-stage approach, used here as a baseline, the technical novelty remains limited.\n- The main novelty consists of using two branches to model the two domains. However, the necessity for the second branch is not very clearly explained, and remains obscure to me, since the first branch already acts on the mixed source-target data.\n\nClarity:\nIn addition the fact that, as mentioned above, the design of the overall framework is not entirely well motivated, I found the paper somewhat hard to follow. In particular:\n- One has to go to the appendix to find Alg. 2, which describes one of the key components (the Checking method in Alg. 1). The steps performed by Alg. 2 are not explained anywhere.\n- In Alg. 1, it seems surprising that \\tilde{D}_T^l is initialized as \\tilde{D}_s, since, according to Fig. 3, the second branch should act on the target data only.\n- In Alg. 1, the authors do not explain how they initialize the values R(T) and R_t(T).\n- I would expect that, to obtain meaningful results from the Checking method, the parameters of the networks F_1, F_2, F_{t1} and F_{t2} should already be initialized to reasonable values. Can the authors comment on the initialization procedure?\n- In Alg. 2, how are the inner minimization problems solved? Are u_1 and u_2 truly enforced to be binary variables? How fast can one obtain the solutions to these problems?\n- In Fig. 1, it is not clear to me what the term Interaction between the two branches represent. From the text, I could not find any reference to explicit interactions between the branches.\n- In Eq. 1, the loss function \\ell() is not defined (although I imagine that it is the cross-entropy).\n\nExperiments:\n- The experiments show the good behavior of the method. However, while I understand the motivation behind defining the two-stage baseline using ATDA, which is used in the proposed method, there seem to be no strict constrain on using this specific method in the two-stage scenario. For example, based on the results in Table 1, one might rather want to use TCL as the second stage, i.e., have a baseline Co+TCL. \n- As I mentioned before, the motivation behind the second branch in the framework is not clear to me. I would appreciate it if the authors could explain the reasoning behind this branch and evaluate their method without it.\n\nSummary:\nThe novelty of the proposed method, relying on a combination of co-teaching and pseudo-labeling, is limited. Furthermore, the clarity of the paper could be significantly improved.\n"
        }
    ]
}