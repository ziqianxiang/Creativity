{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a deep neural network for weakly-supervised pointwise localization. The authors formulate it using conditional entropy that separates relevant and irrelevant regions for classification, which is different from typical attention maps that are prune to false positive. The architecture consists of a localizer and a classifier, where the localizer is designed to separate relevant and irrelevant regions (in a CAM-style) guided by the conditional entropy loss. Experimental results show good results on three benchmark datasets including one medical and two natural datasets.\n\nThe proposed formulation and architecture is interesting in the sense that it explicitly models foreground and background separation. But, I have serious concerns about the positioning of this paper and the experimental evaluation as follows. \n\n1) Weakly-supervised point-wise localization? \nThe problem this paper tackles, called weakly-supervised point-wise localization in this paper, actually appears to be the problem of weakly-supervised semantic segmentation; the proposed loss and architecture are specifically designed to separate the foreground from the background. While this paper cites many of weakly-supervised semantic segmentation, the authors do not name their method as one of them, which looks strange to me. I think the title, introduction, and related work need to be revised more explicitly referring to weakly-supervised segmentation.  \n\n2) Experimental comparison\nThe authors compare their methods mostly to visual attention (or explanation) methods, e.g., CAM variants, avoiding direct comparison to recent weakly-supervised segmentation methods. And, the motivation for choosing their particular dataset and evaluation metrics is also unclear. If they want dataset with \"both image and pixel-level labels\", then the standard benchmarks for semantic segmentation, such as PASCAL VOC or COCO would be a natural choice.  Otherwise, if the authors want to present their work as visual attention or explanation methods such as Grad-CAM, then they should have evaluated it in the corresponding evaluation benchmarks (e.g., using pointing game). I recommend the authors evaluate their method on standard semantic segmentation benchmarks. \n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "= Summary \nThis paper presents a new weakly supervised approach for learning object segmentation with image-level class labels. The proposed neural network is trained to estimate an object mask and predict the groundtruth class label of the masked input image in which pixels outside of the object mask are filtered out. Further, the network is learned to predict the uniform class distribution when the the estimated object area is filtered out. The entire architecture is trained in an end-to-end manner with classification losses only. On a few datasets, the proposed model outperformed a set of existing techniques computing class attention maps.\n\n= Decision\nI would recommend to reject this submission. \n\n(1) The target task is neither practically useful nor technically interesting.\nThe target task is to learn class-agnostic object segmentation, and is a reduced version of ordinary weakly supervised semantic segmentation whose goal is to predict class-wise segmentation masks. Thus, the target task seems too restricted, not practically useful, and not interesting technically.\n\n(2) The experimental evaluation is not sufficient due to the following reasons. \n- All previous methods listed in the table are out-of-dated. Also, many important and closely related models (e.g., adversarial erasing, Grad-CAM, and many others drawing class attentions) are not compared with the proposed one. \n- The comparisons do not look fair enough since the experimental setting is favorable to the proposed method. The goal of the experiments is aligned with that of the proposed model, i.e., predicting only binary object masks in a class-agnostic manner, while most previous models in the table generate class-wise segmentation masks. More importantly, the datasets used for evaluation are not common benchmarks in weakly supervised segmentation, and the proposed model may not work on the public benchmarks like PASCAL VOC and MS-COCO since they in general have no restriction on the number of classes and instances appearing in an image. \n\n(3) The writing quality of this paper is far below the standard of ICLR.\n- The task \"point-wise localization\" is not clearly defined in the paper, and its name sounds quite confusing as it is also not clearly described in the paper what is the \"point\".\n- The recursive erasing algorithm given in an appendix must be introduced in the main manuscript since the authors claim that it is one of the most important contributions of the paper.\n- It is not clearly described what is the main difference between the proposed recursive erasing algorithm and the closely relevant approaches.\n- Overall, the manuscript is badly organized with paragraphs often too long, awkward sentences, grammatical errors, typos, and so on.\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This work explores the problem of WSL -- in particular, learning to segment with only global labels. The contributions are mainly two-folded. (1) An novel design of regularization terms that promotes the identification of relevant and irrelevant regions for global classification. (2) An recursive erasing algorithm to predict those two types of regions. \n\nPros:\n1. The writing is clear\n2. The literature review seems thorough\n3. Segmentation results provide ample evidence\n\nCons:\n1. The global classification results are not convincing enough to claim \"significant improvements in terms of image-level classification\", as stated in the paper, according to the experiments.\n2. There's a missing link between \"false positive reduction\" claims and the proposed algorithm. The design of the entropy-based loss terms does not seem to directly address this, but covers both false positive and false negative scenarios. Equ (10) used in Algorithm 1 (in the appendix) seems to explicitly reduce false negatives with as it iteratively adds relevant regions that are missed with a max operation.\n3. Using multiple iteration for the mask prediction model seems to experimentally perform better. Without any theoretical analysis, it is however not clear why this is the case at least intuitively. Each step depends on the previous one and could be facing the problem of error accumulation over time (e.g., at the first step, the relevant regions could already been erased). It is hard to imagine how the mask model can produce a better output with simple global classification objectives.\n4. Section 2 is thorough but not entirely clear on the difference between the proposed methods and existing ones. For instance, (1) the following claims are not clear: \"... but places more emphasis on mining consistent regions, and being performed on the fly during backpropagation...\". What is \"consistency\"? What is \"on the fly\"? (2) \"... provide automatic mechanism to stop erasing over samples independently from each other...\" What is the automatic mechanism? What does \"samples independently\" mean? (3) \"...To avoid such issues, we detach the upscaling operation...\" If the upscaling op is not parameterized by the model, what is its formulation?\n5. DICE and F1 score seem to be computed with a threshold of 0.5. Is that an optimal choice? To make the evaluation independent of the threshold, one could use the continuous version of DICE instead.\n6. With small datasets, one would expect to see confidence interval/standard error on the results table, with multiple runs of the same experiments, for instance."
        }
    ]
}