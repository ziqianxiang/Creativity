{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes to perform both neural architecture search and pruning+quantization under the same evolutionary search process. However, as it is computationally expensive to quantize and fine-tune a model and then measure the resulting accuracy, the authors propose to use a machine-learned predictor as a surrogate. \n\nI have two main concerns with this paper: \n\nFirst, I think the method is not really am end-to-end joint optimization approach as motivated in Eq 4. By using a machine-learned predictor to approximate the accuracy, there are no guarantees in this method. It is misleading to call it end-to-end joint optimization. \n\nSecond, I think more details are needed to convince me to trust the results; currently I do not know if all the comparisons are fair. In particular:\n\n(a) It is difficult to discern how much computation is needed for this machine-learned predictor. The use of transfer as illustrated in Fig. 2 is a reasonable idea, but in practice how do we know if this predictor is good enough? How many samples do you actually used in the experiments, and how do you kow that number is sufficient? \n\n(b) The the paper makes claims that need to be backed up, e.g. in pg. 6: \"In fact, we find that 80k data pairs is a suitable size to train a good full precision accuracy predictor.\" Is this referring to this dataset, or in general? \n\n(c) Table 2 compares many methods, but it is not clear if they are apples-to-apples comparison, esp. in terms of search space and resource constraints. It is also not clear if all the computation is reported in the Search Cost and CO2 columns. \n\n(d) Sec 5.1 says \"Our small model (Ours-B) can have 2.2% accuracy boost than mixed-precision MobileNetV2 search by HAQ (from 71.9% to 74.1%); our large model (Ours-C) attains better accuracy (from 74.6% to 75.1%) while only requires half of BitOps. How do you define the small model and the large model? Were they just two models that handpicked from the final population of evolutionary search? What criteria is used to pick them? \n\n(e) Figure 6 measure predictor accuracy, which is extremely pertinent. But it is not clear how the data (train and test) were created. Was this based on generating the quantized dataset, which is expensive? \n\n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper aims at optimizing the process of automatically searching for network architectures. It starts by proposing three parts to the process, of model search, connection pruning and weight quantization. The claim is then presented that executing the three in sequence is sub-optimal, as each step independently de-centers the previous. The proposed solution is then to integrate the three steps in a single end-to-end method, improving performance. While this is in principle interesting, the method proposed depends on strict experimental conditions which limit broader application.\nI would like to highlight the following points:\n- The paper is both unnecessarily and extraordinarily complicated. The core idea is simple and in itself actually elegant, but I find myself unable to suggest a simple set of edits which would bring this paper into publishable state.\n- The \"super network\" at the core of the proposed method acts as an upper bound of the complexity that can be found. The architecture search mode is then of ablation, selecting and reducing parts until an acceptable solution is found. This may work on ImageNet starting from knowingly oversized structures, but attacking an unknown problem would require a significant amount of trials, effectively limiting performance and efficacy.\n- The literature review is excessively technical, surprising as such a comment may sound. The role of such a section is typically to introduce the already-published foundation required to understand the standing of the presented work. In this case instead it lists a broad selection of very similar work, with each corresponding minute difference, which requires the establishment of an expertise in the specific subfield only to be able to evaluate the scale of effective contribution. That an extremely high percentage of the cited work has not seen peer-reviewed publication greatly limit the reader's interested into such an investment.\n- The claims presented often sound overinflated and purposely generic (super-fast speed, orders of magnitude), and lack statistical significance. An attentive read highlights a careful selection of performance metrics, axis scales in the figures, and relatively minor contributions under most metrics. This work does not improve orders of magnitude over SinglePathNAS (admittedly the inspiration of this work) under any metric, cost included. Such selling techniques are often frown upon at top conferences such as ICLR."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes an end-to-end design method for architecting mixed-precision model. The proposed method directly searches for the mixed-precision architectures, unlike previous methods that separately optimize the neural network architectures, pruning policy, and quantization policy. The experimental results show that the proposed method can find better models than the state-of-the-art models.\n\n- It is not clear how the evolutionary algorithm applies to the framework of the proposed method, i.e., which parts are optimized by the evolutionary algorithm? Also, the details about the evolutionary algorithm part are not provided, for example, what is a genotype and a fitness function used in this paper? \n\n- It would be better to provide a comparison with a random search to make the contribution of the evolutionary algorithm clear.\n\n- It is not clear about the procedure of the proposed method during training and testing. Please elaborate on it by providing an algorithm table of the proposed method.\n\n- Please add an explanation about N in Table 2."
        }
    ]
}