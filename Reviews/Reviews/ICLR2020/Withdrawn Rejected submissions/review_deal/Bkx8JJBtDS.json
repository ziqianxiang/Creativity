{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper suggests a measure for predicting the performance of deep networks by counting number of paths. Authors further evaluate this measure empirically and show that it is predictive of generalization.\n\nFinding a measure that predicts generalizability for a given architecture is very interesting and potentially impactful since it can be used to make design choices.\n\nOverall, I enjoyed reading this paper but I have some concerns and I hope authors can address them:\n\n1- Definition: I think the definition make sense for layered fully connected networks but it seems like the way authors extend it to convent is not elegant. In particular, we know that convnets generalize even without weight sharing in which case they can be presented as a simple feedforward network. Is it possible to generalize the definition to any feedforward network presented with a directed acyclic graph (including convnets)? Another reason that this definition for convnets bothers me is that we often observe that the number of channels increases after pooling which connects #channels in each layer to the size of the image in the layer. But this definition completely ignores size of the image in the layers. Another issue is that at least in the main text, the definition is provided for a layered networks and it is not obvious to me that how it can be extended to densenet and resnet.\n\n2- Experiments: Authors provide many experiments to support their claim which is great but they avoid the most direct way to evaluate the measure. I think the best way to evaluate the measure is to train many networks (layer fully connected to layered convnets) with different number of neurons and plot generalization vs #paths. This would be a very convincing experiments. Many experiments provided here do not directly evaluate the measure in a convincing way.\n\n\nI hope authors address above concerns in which case I will increase my score."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "TITLE\nCounting the Paths in Deep Neural Networks as a Performance Predictor\n\nREVIEW SUMMARY\n\nPAPER SUMMARY\nThe paper presents a method for counting paths in deep neural networks that arguably can be used to measure the performance of the network. The paper demonstrates a correspondence between the proposed measure and the empirical performance in a set of experiments. It is argued that more effective neural network structures can be found by maximizing the proposed measure: This is demonstrated by a novel modification of a densenet architecture that increases the proposed measure and leads to a modest performance gain.\n\nQUALITY AND SIGNIFICANCE\nI really enjoyed reading this paper since it aims to give a better understanding of the relation between neural network archtecture and performance, for which there is really not much theory. And while I understand the motivation behind the simple measure of path complexity proposed, I am not convinced that the measure really is very informative. The paper only provides weak empirical justification and no theoretical justification. Demonstrating an improvement on CIFAR-10 with a relatively minor change of the densenet architecture is interesting and worth noticing. But I am simply not convinced about the explanation, and as a demonstration of the particular improvement the experiment is too limited (as the improvement in accuracy is marginal).\n\nCLARITY\nThe presentation is clear and the paper is in general easy to follow. Experiments are described in sufficient detail and source code is provided, which is commendable!\n\n\nFURTHER COMMENTS\n\nI am not sure why you refer to number of layers etc. as \"hidden\" hyperparameters. Why \"hidden\"?\n\n\"with continuous mathematics\" Unclear\n\n\"The goal of DNN models...\" This is not the goal, I think. The goal must be something along the lines of making correct classification on new data (generalization). This section describes your interpretation of how an effective NN works.\n\nI am not sure how your claim regarding \"increasing the dimension of the representation\" is supported by the reference (Vapnik, 2013).\n\n\"comply with many of the assumptions behind complex networks\" What you mean by this is unclear to me. Is there some technical definition of \"complex network\" that I am not aware of? If so, provide a reference. (To my knowledge, the term \"complex network\" usually refers to any network with a non-trivial topology.)\n\nAt first read, the definition in eq. 1 is confusing. What happens when layer i+1 has fewer nodes/channels than layer i? Then the definition of \"paths\" seems inappropriate. Consider explaining this up front.\n\nThe claim that the number of paths quantifies quality (in some sense) is not justified well enough in my view.\n\nThe assumption that Ni+1 = Ni*(1+alpha) seems unjustified.\n\nIn fig. 2 I would have liked to see all the data from the experiments including results for different initialization and learning rates. Also, in this experiment, can you justify using the same weight decay in all settings?\n\n\"...the optimal variance of the weight initialization\" I think \"optimal\" is too strong here.\n\nIf I read fig. 2 correctly, the optimal accuracy and Z are not the same (although it seems there is some correspondence)\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper proposes a novel quantity that counts the number of “path” in the neural network which they argue is predictive of the performance of neural networks with the same number of parameters. The paper shows a continuous approximation Z for the proposed path counting model and provides a close-form solution on how the channel number in the architecture should be chosen. The experiment investigates the correlation between Z and generalization of model and also compute Z on a number of popular architectures to show that Z correlates with the empirical performance observed. Finally, the paper proposes a small modification to the DenseNet architecture which increases Z and shows that this change improves generalization with no extra parameters.\n\nI think understanding the influence of the network architecture is a valuable direction for deep learning that is currently under-explored and the approach proposed seems very reasonable. Some existing works that the authors that might want to discuss are [1] and [2]. [1] proposes an architecture dependent quantity that can be optimized to improve generalization and [2] explores using different classes of random networks for neural networks. [3] proposes a quantity that predicts generalization albeit with a very different approach.\n\nI have a number of concerns regarding the motivation, experiments and overall writing quality, so I am currently leaning toward rejecting this paper:\n\n    1. The justification for paths in section 2.1 is extremely hand-wavy. This is the core mathematical object the paper studies, and if theoretically rigorous justification is difficult, then at least a detailed heuristic justification or an illustration should be provided. The paper claims the proposed quantity provides an increased understanding of generalization but I do not see sufficient support for this claim in the current draft.\n\n    2. The paper argues that neural networks is a complex network and make another loose connection to path entropy, a concept from physics. I believe making connections to physics is great, but this is not a common knowledge for the ICLR community and warrants more elaboration.\n\n    3. Figure 2, 3 and 4 ‘s points (especially the converged points at the the top) do not track the predicted Z closely. In fact, in some cases, the networks with smaller Z performs the best (Figure 3). If this is due to randomness, then the experiments should be repeated for statistical significance. Further, different time steps should be colored differently to increase readability.\n\n    4. In my opinion, the paper positions itself to be a heuristic paper with empirical verification rather than a theory paper. As such, I think the paper needs much more empirical evidence to support the claim. For example, table 1 is missing many entries which makes comparison hard. Given that in the toy models the performance sometimes does not track the prediction closely, I would like to see more results on realistic models and more thorough analysis on the results (especially when the prediction fails to explain the observed results).\n\n    5. The proposed method, like the paper points out, requires non-trivial understanding of the architecture. This limits its utility. One of the largest potential I see is using it for architecture search but this need for understanding makes it difficult. Is it possible to derive an algorithm to compute Z for arbitrary DAGs? i.e. NASNet or randomly wire neural networks.\n\n    6. For methods of improving Z, only one possible modification is made to a single architecture (DenseNet). This is not very convincing evidence for the effectiveness of Z. I would like to see such modification made more more architectures such as a vanilla Resnet or wide resnet. Some proof of universality would strengthen the argument.\n\nMinor comments that did not affect my assessments:\n    - Typographical Errors: inconsistent usage of double quotation, page 3 bottom “feeedforward”.\n    - Figure 5/6 look hand-drawn and are extremely confusing. I encourage the authors to remake them with either PowerPoint, Google Slides or TikZ and add more details + clarification.\n\nReference\n[1] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, Tan et al. 2019\n[2] Exploring Randomly Wired Neural Networks for Image Recognition, Xie et al. 2019\n[3] Predicting the generalization gap in deep networks with margin distributions, Jiang et al. 2019\n"
        }
    ]
}