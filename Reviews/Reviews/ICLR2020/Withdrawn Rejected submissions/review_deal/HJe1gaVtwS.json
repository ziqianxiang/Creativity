{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors consider a bilinear matrix normalized network and how to implement the bilinear matrix normalization, which uses SVD, in a manner that allows end-to-end training. The key idea used is to use the fact that singular values of a matrix can be found via the power method, which requires repeatedly multiplying a matrix with a vector and normalizing the resulting product.  Experiments are demonstrated on some multi-class datasets of fairly small size. The writing could have been better and i found the exposition somewhat hard to follow. Here are  a few comments and questions.\n\n1. It was hard to follow the RUN procedure. I was able to figure out the use of power method, but it was not clear to me what the key result of Section 3 is? I would suggest that the authors rework the exposition in this section to clearly state what the main result is.\n\n2. The latter half of Section 3 shows the derivatives w.r.t. F. I appreciate the authors working through these calculations, but I do not see why it is important. It was not clear to me what the authors wanted to express via the calculation of the derivative.\n\n3. The datasets used for experimentation are rather small. Why is this the case? What is the limiting factor?\n\nI like the idea of using power iterations and exposing the power iterations as a sequence of layers. However, the exposition needs a lot to be desired."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a rank-1 update normalization (RUN), which supports the normalization on compact bilinear features and feasible to be plugged into end-to-end training. RUN uses power method to estimate the bilinear matrix, which is more friendly to GPU. The idea of RUN is well motivated but not surprising. The experimental results show RUN achieves comparable accuracies with much lower running time than NS iteration and SVD-based normalization. \n\nThe presentation of this paper should be improved. It is necessary to summarize the procedure of RUN like Algorithm 1 and 2. The main results in Section 3 are desired to be presented by Theorem or Proposition. Some detailed derivation could be deferred into appendix.\n\nSome questions:\n\n1. It is unclear how to obtain (16) from previous derivation. Where does \\epsilon come from?\n\n2. Can you provide non-asymptotic results of (18) and (21) (which may be rely on the eigengap)? \n\n3. It would like to plot how the accuracy of each algorithms varying with epochs and running time. \n\nMinor comments:\n\n1. Should v_i in section 3 be standard normal distribution?\n\n2. In the sentence between (11) and (12): normalization distribution -> normal distribution.\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper explores fast normalization schemes for bilinear pooling in deep neural networks. The scheme basically approximates an SVD through K iterations of the power method, applied to a random vector. The performance is compared, in terms of computational time and accuracy degradation, with max pooling, sum pooling, and other SVD-based normalization schemes in bilinear pooling. \n\nI am not too familiar with this area, so I cannot speak to whether this idea is novel or whether the practice of bilinear pooling is widely used; however, it seems from the experiment that the idea works well and can greatly increase performance in smaller neural nets (VGG is used) on complex tasks. Additionally, using the power method to approximate an SVD makes sense, and the tests seem fairly extensive. \n\nOne comment is that I find the method seems a bit complicated in description. Why is the expectation subtracted? Why are so many steps needed after the power method is completed? Additionally, the details for the back propagation seems not very relevant to the method idea, and could be moved to the appendix, with just a description on computational complexity in the main text. While I believe that each step was important, I think the motivation behind each step should be emphasized over the mechanics, which are a bit convoluted. \n\nOtherwise, I think the paper is a nice contribution."
        }
    ]
}