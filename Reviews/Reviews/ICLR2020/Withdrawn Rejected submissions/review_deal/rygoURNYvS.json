{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper presents  CuBERT (Code Understanding BERT), which is BERT-inspired pretraining/finetuning setup, for source code contextual embedding. The embedding results are tested on classification tasks to demonstrate the effectiveness of CuBERT. \n\nThis is an interesting application paper that extends existing models to source code analysis. The authors did a good job at motivating the applications, describing the proposed models and discussing the experiments. The authors also agree to share all the datasets and source code so that the experiment results can be replicated and compared with by other researchers. \n\nOne major concern is the lack of strong baselines. All reviewers are concerned about this issue. The paper could lead to a good publication in the future if the issues can be addressed. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "# Summary\n\nThis paper presents a BERT-inspired pretraining/finetuning setup for source code tasks. It collects a corpus of\nunlabeled Python files for BERT pretraining, designs or adopts 5 tasks on established smaller-scale Python corpora, and\nadjusts the BERT model to tokenize and encode source code snippets appropriately.\n\n# Strengths\n\n* The idea of applying the pretraining/finetuning paradigm to program analysis tasks makes sense, and has been\n  informally attempted by multiple groups in the community in 2019. This is the first high-quality submission to a\n  top-tier ML conference I've seen on the subject, though.\n* The authors exercised commendable care and diligence in preparing the training data, adopting BERT to source code\n  inputs, and ensuring correctness of the experimental setup. I appreciated all the provided details on tokenization\n  (Section 3.3), deduplication (Sections 3.1-3.2), and task setup (Section 3.5). This should become a technical standard\n  in the community.\n* The paper is written clearly and concisely, and is generally a pleasure to read.\n\n# Weaknesses\n\nI have a gripe with the authors' choice to ignore program structure (e.g. abstract syntax trees) or features (e.g.\ntypes) in their program representation. Without this extra information (easily available from a compiler/interpreter\nAPI) the pipeline is not substantially different from the original NLP pipeline of BERT et al. The main program-related\nrepresentation insight comes in tokenization (Section 3.3) and the definition of \"sentences\". To repeat, I appreciate\nthe effort the authors put in making tokenization appropriate for BERT processing of source code, but this is a drop in\nthe bucket compared to the all the other program-related features the work is leaving off the table. Programs are not\nnatural language.\nThe argument that source code analysis would \"pass on the burden ... to downstream tasks\" (Page 3) is odd. First, most\ndownstream tasks of interest occur in the settings where this analysis is already available: IDEs, code review\nassistants, linters, etc. Second, one often needs program analysis to even define downstream tasks in the first place --\nfor example, determining whether function arguments are swapped required detecting a function call and boundaries of its\narguments, thus parsing the program!\n\nThis work obtains (and nicely analyzes) impressive results obtained by applying CuBERT. However, it does not put the\nresults in context with prior work based on structured program representations. Without this, it is difficult to say\nwhether the improvement comes from pretraining or from the language model simply learning a better \"parsed\"\nrepresentation of an input program from all the unlabeled corpus. If it's the latter, one might argue that supplying the\nmodel with structured program features explicitly might eliminate much of the need for the unlabeled corpus.\nI personally think that there will still be a gap between pretraining and finetuning even with structured program\nfeatures simply due to the sheer volume of available data, which, as the authors showed, is crucial for good\ngeneralization of Transformer. However, this still needs to be shown empirically.\n\nThe \"Function-Docstring Mismatch\" task, as presented, seems too easy. If the distractors (negative examples) are truly\nchosen at random, most of them are going to use obviously different vocabulary from the original function signature (as\nFigure 4 demonstrates). A well designed task would somehow bias the sampling toward subtle distractors such as `get` vs.\n`set` docstrings, but this seems challenging.\nThis also explains why the task is not influenced as much by reduction of training data (Table 3).\n\nThe Next Sentence Prediction pretraining task, as adapted for CuBERT, seems too difficult, in contrast. If the paired\nsentences (i.e. code lines) are chosen at random, the model would lack most of the context required to make a decision\nabout the logical relationship between them, such as which variables are defined and available in context, which\nfunctionality is being implemented, etc. I wonder, can the authors experiment with pretraining CuBERT only with the\nMasked Language Model task? Will it worsen the results substantially or at all?\n\n# Questions\n\nSection 3.2: \"similar files according to the same similarity metric...\"\nWhat are these metrics?\n\nWhat is the fraction of positive/negative examples in the constructed finetuning datasets?\n\nWhat is the motivation for making Variable Misuse and Wrong Operator/Operand into a simple classification tasks instead\nof the original (more useful) correction task?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper proposes a transformer based approach to address a number of problems for machine learning from code. Then, the paper adds BERT-based pretraining to significantly improve on the results. The paper is nicely written and easy to follow, but with relatively thin contributions besides the large amount of experiments.\n\nInitially, I was quite positive on the entire paper, but as I read it in more details, I got less convinced that this is something I want to see at ICLR. First, there are no good baselines. BiLSTM gets quite high accuracy on most of the tasks, which is unexpected because most prior works show that the tasks benefit from some semantic understanding of code. I cannot relate any of the numbers with a previous work. Right now, I even have reduced confidence that the authors do not have bugs in the tasks or the reported numbers. Then, for the Operator and Operand tasks, BiLSTM is also doing impressively well (these tasks were done differently in prior works). Interestingly, things get reversed on the last two tasks. Given that most of the experiments are not the same as in any previous paper, I would strongly appreciate if much more details are given in the appendix. In fact, the appendix right now does not have much useful information besides praising how good job the attention is doing. What would be needed is information on how many samples were generated, how large were they, was any data thrown out? Table 1 is a good start, but it actually raises questions. You split the Python dataset into separate functions like Vasic et al and the number of functions 2x higher (counting the artificial samples, I guess), did you put a limit on how large functions you consider? 250 tokens was the limit of Vasic et al. To which of the tasks in the Vasic et al paper can I relate? Is the BiLSTM on the level of that work or it is substantially worse or better? Also, are the results coming from the paper SOTA or uncomparable to other works?\n\nFive tasks are evaluated, which is impressive. This is one reason I want to see the results published. The problem is also quite important. The experiments that show the effect on reduced labelled data are quite important and interesting - in fact, for many tasks, we can start curating datasets and having model working on small data will be crucial. However, I think the paper needs more work before it is something to present, cite or build upon.\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper describes a BERT-based pre-training for source code related task. By pre-training on BERT-like models on source code and finetuning on a set of 5 tasks, the authors show good performance improvements over non-pretrained models. The authors make a series of ablation studies showing that pre-training is indeed useful.\n\nOverall, I find this work relevant and interesting, albeit somewhat unsurprising. Nevertheless, I see no reason to reject this paper. To make my \"weak accept\" to a \"strong accept\" I would like to see experiments on more tasks, preferably more complex tasks. For example, such tasks could include (a) variable naming (b) method naming (c) docstring prediction/summarization (d) language modeling/autocompletion. I believe it's unclear to the reader if pre-training is also helpful for any of those tasks too and adding such experiments would significantly strengthen the paper.\n\nSome clarifications comments/questions to the authors:\n\n* I would insist that the authors rename the \"Variable Misuse\" task to \"Variable Misuse Localization\". To my understanding the current model points to the misused variable (if any), but does not attempt to suggest a fix. This tackles only a part of the task discussed in Vasic et al. (2019), Allamanis et al (2018) and this might confuse readers who want to compare with those works.\n\n* For the Function-Docstring Mismatch task (Section 3.4):\n    * It's unclear to me which dataset is used. Is it the Py150 dataset or the Barone & Sennrich (2017)?\n    * I believe that the citations [a], [b] would be appropriate here.\n\n* Overall, for the all the tasks except from \"Exception Type\", there is a replicability issue: Since the authors manually mutate the code (e.g. introduce a variable misuse, swap an operand), for anyone to compare directly, they would need access to the mutated samples. I would strongly encourage the authors to provide more details on how they create mutated samples and (eventually) the source code that achieves that.\n\n* For the Variable Misuse, Wrong Binary Operator, Swapped Operand tasks. There are a few things that need to be clarified:\n   * How long is each code snippet? One would expect that the longer the code snippet the harder the task. Do the authors pass a whole function?\n   * What is the proportion of positive/negative examples in each task?\n\n\n\n[a] Cambronero, Jose, et al. \"When Deep Learning Met Code Search.\" arXiv preprint arXiv:1905.03813 (2019).\n[b] Louis, Annie, et al. \"Deep learning to detect redundant method comments.\" arXiv preprint arXiv:1806.04616 (2018)."
        }
    ]
}