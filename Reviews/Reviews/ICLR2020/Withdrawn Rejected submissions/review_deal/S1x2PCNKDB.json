{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper attempts to improve adversarial imitation learning (GAIL) by encouraging the discriminator to focus on task-dependent features.\n\nAn advantage of this paper is that it not only improves upon GAIL, but it is doing so after first demonstrating and analyzing an existing issue. \n\nOn the other hand, the presentation of the paper and breadth of experiments could be significantly improved further than the updated version. It would also be necessary to clarify whether the baseline is vanilla PG or D4PG.\n\nA major point for discussion was the selection of the invariance set. The ablation studies and explanation provided during the rebuttal period towards this point are helpful, but somehow we still do not have the full picture to understand well how this method compares to existing literature.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "UPDATE:\nI acknowledge that Iâ€˜ve read the author responses as well as other reviews. \nI would keep my score unchanged, i.e. at 6 Weak Accept. \n\n\n#####\n\nThis work considers imitation learning, i.e. the problem of learning an unknown reward function from expert demonstrations. The paper proposes Task-Relevant Adversarial Imitation Learning (TRAIL), a modification of Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016) which introduces an additional constraint to the discriminator optimization, namely that the discriminator must not be able to successfully discriminate between expert and agent samples on some predetermined invariant set $\\mathcal{I}$. Technically this constraint is realized with an additional term added to the standard GAIL objective that minimizes the negative (i.e. reversed) discriminator objective whenever discrimination accuracy exceeds 50% (i.e. randomness) on $\\mathcal{I}$. Given that the invariant set $\\mathcal{I}$ includes environment samples not relevant for the task, TRAIL thus aims to prevent discriminator overfitting to task-irrelevant features (e.g. object or agent appearance) and regularizes the discriminator to extract more salient, task-relevant features of expert behavior.\nIn experiments on various robot manipulation tasks (Block lifting/stacking/insertion with and without distraction), the paper shows that TRAIL outperforms Behavioral Cloning (BC), standard GAIL, Deterministic Policy Gradients from Demonstrations (DPGfD) as well as additionally introduced improved GAIL baselines that employ common regularization techniques (early stopping and data augmentation).\n\nI think this work could be accepted since it clearly demonstrates a key issue of GAIL (tendency to overfit to task-irrelevant features) and proposes a simple solution for this issue that empirically proves to be effective. I especially appreciate the comprehensive ablation studies that isolate effects from data augmentation and actor early stopping. The paper overall is well written and easy to follow. \nI must admit, however, that I am not too familiar with this line of research and might be missing some context, e.g. recent competitors.\n\nSome questions that I have:\n(1) Could you elaborate more on how to select the invariant set $\\mathcal{I}$ in practice? The paper proposes random policies or initial frames, but as I understand experiments are only reported for the latter design choice. How would performance differ for different choices? What if the invariant set $\\mathcal{I}$ is misspecified, i.e. includes task-relevant samples? Would TRAIL be robust?\n(2) How is the invariant set $\\mathcal{I}$ chosen for the stack, insertion, and stack banana experiments in Figure 8?\n(3) What if actions would be included in objective (2) again?\n\n\n#####\nMinor comments\n- Is the legend in Figure 14 wrong? Table 1 results and the plots in Figure 14 are inconsistent (e.g. on Lift distracted \"TRAIL\" is best in Figure 14 which should be \"TRAIL - augmentation\" according to Table 1.)\n- Section 5.1: \"Finally, to disentangle the importance of actor early stopping, ...\" >> \"disentangle\" somewhat is a signal word in ML that readers directly associate with representation learning. Maybe just \"show\"/\"infer\"/\"understand\".\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The paper addresses the acute problem of irrelevant discrimination in adversarial imitation learning methods. Ideally, the discriminator in such methods should extract task-dependent features and base its discrimination rule upon them only. However, in practice, the opposite usually occurs. For example, if the state distributions of the expert and agent come from different modalities, e.g., different background light, different POV camera, etc., then almost surely the discriminator will base its classification rule on such differences. In turn, the reward signal will be (constant and) irrelevant. This is a well-known problem, that indeed received little attention in previous work. \nAs a solution, the authors propose to maintain an invariance set representing task-irrelevant states. For example, states of a random policy or states from the beginning of an episode. The authors suggest using the invariance set as a regularization mechanism to the discriminator by punishing it for correctly classifying examples from the invariance set. The idea behind this regularization is to distinguish between expert examples that actually represent the task and expert examples that do not. This is achieved by punishing for the discriminator's accuracy over the invariance set.\nIn addition, the authors present two additional improvements: data augmentation and early actor stopping.\nThe paper.\n\n1. I very much like the problem the authors choose to address. I believe that this is a dominant problem that received little attention so far. The reason is that so far, GAIL style methods used experts and agents that come from the same domains (same simulation), thus \"hiding\" such problems. As experiments scale up to real-world setups, this is no longer the case and such problems arise.\n\n2. Overall, I find the experiments section very confusing. The authors propose a formal improvement (the invariance set) which they apply to their agent, as well as two informal improvements (actor early stopping and data augmentation) which they apply selectively for their agent and the baseline methods. As a result, there exist a large number of variations, according to what improvements are applied to what method. Overall, I find it hard to follow the results and understand the contribution of the different improvements.\n\n3. It is not clear if TRAIL and GAIL are using the same architecture and algorithm. Apparently, the authors could have used GAIL's original policy gradient algorithm with their invariance set modification. If an improvement was evident in such a comparison, then it would have been easier for me to understand the significance of the proposed method. Currently, there are too many differences between the baseline and TRAIL to clearly say that the invariance set made the difference.\n\n4. It is not clear why the constraint in Eq. 2 is placed over the sum of the two terms and not on each term separately since each term is defined over a different state distribution. What would you expect to see different if the constraint was placed on the two terms separately?\n\n5. In the same matter, why do you need the 0.5 factor?\n\n6. The invariance set is referred to extensively before it is first defined. Therefore, the reader is forced to toggle back and forth multiple times in section 4 before understanding equations 2 and 3. Consider giving at least an intuition about the invariance set beforehand to make the reading more fluent.\n\n7. The conclusions section is minimal to non-existing while there are many conclusions to draw from the experiments.\n\n8. Data augmentation was previously studied in the context of adversarial imitation. See for example \"Visual imitation with reinforcement learning using recurrent Siamese networks\" by Berseth et.al.\n\n9. Overall assessment: among the three modifications the authors propose, I would be glad to see if the invariance set modification alone can make a difference. The other two modifications are either not novel (see Berseth et.al.), or highly heuristic (\"...we restart an episode if the discriminator score at the current step exceeds the median score of the episode so far for T_patience consecutive steps...\"). However, it seems that the informal improvements (augmentation and early stopping) play a crucial role in the success of TRAIL (... It drastically improves the baseline GAIL agent, and is necessary to solve any of the harder tasks...\"). It is absolutely fine that the authors stress the importance of the informal improvements but it seems that they are dominating the contribution of the main ingredient.\n\nI also find it problematic that the construction of the invariance set is not straightforward (\"...the selection of the invariance set is a design choice...\"). When added to the way that the data augmentation is carried (sensor dependent), and to the fact the the early stopping mechanism seems dubious for some tasks (I can think of several tasks where early stopping will not work), this method is far from being a \"plug-and-play\" solution. I.e., the paper offers general guidelines on how to solve imitation tasks but not a concise algorithm.\nIn order for me to vote for acceptance, I need the paper to be more concise. In its current state, I see it more as general guidelines for training adversarial imitation algorithms.\n\nHowever, as said in the beginning, I very much believe in the problem that the authors present and encourage them to provide an experiment where the only modification between the baseline and the proposed method is the usage of an invariance set. Success in such an experiment will clearly change my mind.\n\nThere are also some presentation issues, e.g cluttered graphs, that can be addressed."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #3",
            "review": "The paper proposes an extension of the adversarial imitation learning framework where the discriminator is additionally incentivized not to distinguish frames that are different between the expert and the agent in irrelevant ways. The method relies on manually identifying a set of irrelevant frames.\n\nThe paper correctly identifies an important shortcoming of GAIL and proposes a sensible and generic way to overcome it. The experiments are well designed and corroborate the claims made in the paper.\n\nEDIT: I acknowledge reading the other reviews and the author response and stand by my initial assessment.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}