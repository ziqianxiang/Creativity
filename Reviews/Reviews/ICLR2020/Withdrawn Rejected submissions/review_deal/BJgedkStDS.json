{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #4",
            "review": "The authors introduce a mechanism for exiting the inference process early, without doing any more processing, if the network determines that there should be a classification early. The proposed mechanism is: learn separate linear classifiers on the outputs of layers in the convnet and use a decision rule based on agreement of these classifiers (and their confidences) in sequence, sometimes combined with the final output (described in Algorithm 3). \n\nThe paper is poorly written (there are many simple grammar mistakes, the methods are not well motivated, and the methods are described in a confusing way), and more seriously the evaluation of the method is not complete. \n\nDetails:\n\nThe evaluation is not complete. The threat model that the defense is evaluated under is not realistic: it assumes that the attacker does not have knowledge of the model: \"we assume that the attacker has no knowledge about the detection mechanism but has complete knowledge about the baseline DNN (white-box attack with respect to baseline DNN)\" (page 9). There are a number of mechanisms that are known that work well in this assumed threat model already, and other works have indicated why this is not a realistic threat model, see: [0]. As such the evaluation of adversarial robustness does not stand since there are no best effort, dynamic attacks on the defense presented. \n\n[0] https://arxiv.org/abs/1902.06705",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "I like this paper. It is well-written, easy to follow and it has a novel idea. The paper covers previous work of art well and introduces the problem. It explains the algorithm well through figures. \nMy only concern is that the paper claims they are improving speed comparing to previous work, but they never compare their work to those mentioned and they don't talk about why there is no comparison provided with previous work of art. Particularly, the algorithm loses some accuracy while detecting adversarial and it is important to know how other algorithms do in a similar scenario."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The aim of the paper is to reduce the misclassifications of deep neural networks in a way that is energy efficient. Misclassifications in this paper are defined as adversarial and out-of-distribution samples, and natural errors (samples in training and test sets that are misclassified). A main target of the paper is to achieve that while saving energy (number of operations per sample). The proposed approach involves adding Relevant feature based Auxiliary Cells (RACs) after one or more hidden layer to decide, based on the detected features (class-specific), whether or not to end the classification early and declare the sample abnormal or output No Decision. By detecting the abnormal samples early on, the method can save unnecessary subsequent computation operations, and thus saving energy. The added RACs blocks use binary linear classificatiers trained on the relevant features for each class, and at test time they evaluate the activated features to dicide the corresponding class with a confidence score. If the RAC units disagree on their classification, the input sample is deemed abnormal and the classification is ended early. The main trick is where to place the RAC units, as if they are placed after the first layers, the features learned so far are not enough to distinguish the classes, and if they are placed immediately before the last layer, the high-level detected features will be useful but then there would be no chance to save energy.\n\nFirst of all, the paper, especially the introduction, is not well written. The paper requires an unnecessarily high amount of time to grasp how the method works. The method is actually pretty simple. The problem under study is of relevance, as recent workshops on green ML at NIPS and ICLR have indicated. I find the proposed solution, however, quite heuristic. Why does it work? The paper does not shed light on this. \n\nThere is a risk that the use of binary linear classifiers in RACs might end the classification immaturely, as the feature space might not (yet) be linearly separable and might still need more nonlinearities to classify it correctly. This is shown in the experimental results where the deeper nets with large number of classes have larger test error when RACs are used while the save in energy is not worth it (Figure 4). For a method that is proposed to reduce the natural errors, it's not clear how this serves the main target.\n\nThe hyperparameter tuning in the proposed method was dataset-specific, which affects the generality of the method aplicability.\n\nQ.: For which other settings can RACs be put to good use?\n\nMinor:  Note that the description of Figure 2 differs from the legends on the plots.\n"
        }
    ]
}