{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper studies Population-Based Augmentation in the context of knowledge distillation (KD) and proposes a role-wise data augmentation schemes for improved KD. While the reviewers believe that there is some merit in the proposed approach, its incremental nature and inherent complexity require a cleaner exposition and a stronger empirical evaluation on additional data sets. I will hence recommend the rejection of this manuscript in the current state. Nevertheless, applying PBA to KD seems to be an interesting direction and we encourage the authors to add the missing experiments and to carefully incorporate the reviewer feedback to improve the manuscript.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors hypothesise that, in a Knowledge Distillation (KD) setting, the student network may benefit from learning from different training data than the teacher. The motivation comes from the fact that human teachers \"adapt\" the examples given their students, depending on their individual expertise on the subject, personal and cultural biases, and other factors.\n\nMore concretely, the paper proposes to use an evolutionary algorithm for data augmentation (PBA, already published) to train the teacher and student networks. The key aspect of the proposed method is that the teacher and the student use different augmentation schedules. The augmentation schedules improve results on both the teacher and student networks.\nOrthogonally to this, they also propose to combine different KD objective functions and show that this also improves the results over each loss in isolation. \n\nThe datasets used in the experiments are CIFAR-10 and CIFAR-100. In most of the experiments the student has a (much) lower bit-width than the teacher, but they also apply the method with a student network with less parameters. Two different network architectures are used in the experiments: AlexNet and Resnet18, their method shows consistent improvements over the baselines in all cases (although some improvements may be considered marginal).\n\nOverall the paper is well motivated, clearly written and, in the experiments section, they give enough details, and/or cite previous works that contain them, to reproduce the experiments.\n\nMost importantly, the experiments are well designed to test the initial hypothesis: \n1. The authors show that the student (and teacher) trained with the PBA data augmentation achieves a higher accuracy than the baseline method (Table 2). However, this is not enough to confirm/refute the hypothesis itself, since it is known that data augmentation generally helps.\n2. They show that the two augmentation strategies are different, by using the teacher’s augmentation to train the student network (Table 3). The results show that it’s better to use a specific data augmentation schedule for the student network.\n\nHowever, (the main criticism is that) the paper only partially confirms the hypothesis. For instance, it is not clear whether the hypothesis is true for other forms of KD, or it only applies when training a student with lower bit-width. In Section 5.6, they train a student with fewer parameters (less layers) than the teacher, instead of fewer bits/parameter. However, the improvements of their method over the baseline (II-KD) seem marginal there. And most importantly, as pointed out earlier, this alone is not enough to confirm the hypothesis. A similar table to Table 3 should be included in this section.\n\nSecondly, the proposed KD loss, which is just a combination of previously published works, is an orthogonal improvement to the main method (as the authors admit), and it is not related at all to the subject of the study. It is obviously good to introduce more than one contribution in a paper, but this second (and minor) contribution should be well motivated in its own, in order to avoid \"distracting\" the reader from the main contribution.\n\nIn addition, I would suggest to perform statistical tests to give additional robustness to the conclusions drawn from the experiments. They perform experiments on different architectures (AlexNet and Resnet18) and different KD losses, and the conclusions are always consistent with the hypothesis, but it’s not clear whether the improvements are statistically significant or not. In this regard, it would also be appreciated to include results with other datasets, since only CIFAR-10 and CIFAR-100 (which are very similar datasets) were used. \n\nBeyond statistical significance, it’s also not clear how important are these results for researchers working outside the scope of Knowledge Distillation. \n\nFinally, some minor comments:\n\n- L_original in Eq. (9) is not defined. I'm assuming it's the L_{KD}^{soft} loss.\n- Please, check consistency of pronouns: authors refer to the teacher network as “it” (e.g. “[...] teacher distills knowledge to a narrow/shallow student to improve its performance”, page 2), but to the student as “she” (e.g. “[...] to train the student better from her teacher”, page 2).\n- “our methods clearly outperforms [...]” (page 6)  -> “our method clearly outperforms [...]”.\n\nScore: Borderline accept, but I will increase the score if the authors address my concerns and provide better evidence that the hypothesis is confirmed."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose a new method to distill a teacher model into a student model. They demonstrate improvements over existing distillation variants. The results are impressive for low-precision networks. \n\nHowever, I found a few problems with the paper:\n\nIn the author’s methods, there seem to be multiple steps:\n1. Train the teacher with PBA (stage-alpha).\n2. Train the student with PBA, using a subset of the data and then using the teacher to learn the augmentation policy (stage-beta). This is described in Section 4.2, but I found it incredibly confusing to get a clear picture since there are several moving parts here (augmentation, student solo training, distillation with teacher).\n3. Additionally in Section 5.2 (experiments), the authors propose the combined inter/intra distillation loss, which is named ‘II-KD’.\n\nIn Table 3, the accuracy of ResNet18 on CIFAR-100 with II-KD is the same as accuracy of Resnet18 Student after stage-beta in Table 2. Same for AlexNet on CIFAR-100. Which of the tables are wrong?\n\nAlso, in section 5.1, the authors mention that they use the pre-trained teacher as a starting point for the student network. This is not a fair comparison with the ‘Soft Labels’ approach of Hinton et al, where the student network is not initialized from the teacher.\n\nFurther, it is unclear why the augmentation plot in the student in Figure 3(c) differs wildly from the augmentation plot of the teacher. There is no augmentation for the first 50 epochs, and then the probability of all the augmentation operations moves in discrete steps together.\n\nThe novelty in the paper seems to be:\na) Applying PBA to a Distillation setting.\nb) Introducing the ‘II-KD’ loss.\n\nAs mentioned (a) is not explained clearly. (b) is explained in a generic way but the authors do not give an example of the inter/intra feature map comparisons.\n\nOverall, while I feel the results are impressive, the method is complex as it is presented. I would be reluctant to accept the paper without further clarification from the authors."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper takes the idea of Population-Based Augmentation (PBA) and extends it to knowledge distillation (KD). The idea is that the ideal augmentation protocol for training-from-scratch (or in this context teacher training) may not be the best for student networks under a KD loss.\n\nI am borderline about this paper and had to pick one, so I landed on Weak Reject. On one hand, I think it’s a really neat idea to apply PBA in this context, and package it as a strage-alpha/stage-beta training procedure. However, the experimental results seem very incremental and I’m not convinced there is a genuine signal there.\n\nExperiments:\n\nTable 1 offers a great comparison of prior work, as well as the combination of prior work (II-KD). Table 2 tells me that PBA seems incremental both for the teacher and the student. Going from vanilla training to student with II-KD gets you most of the way there, and the primary contribution of this paper just gives you a slight benefit above this. It’s great that a more traditional full-precision comparison was also added in Table 4, but this table also confuses me. First, it was unclear if “vanilla training” referred to the teacher or the student. The number 74.31 is not the same as in Table 2 (74.85), so I assume this means it’s the student? If so, the student is already very close to the teacher, and this is not a great starting point for evaluating KD. The student after stage-beta also outperforms the teacher - something that was mentioned in the related work, but I would like more discussion around it specifically for Table 4. Another thing I was wondering was how important PBA is for the teacher’s ability to be a good teacher. It gives a modest boost in Table 2; what if we skip PBA in the teacher but still do it for the student. This would be interesting to add.\n\nOverall, there aren’t that many experiments. The dataset is never more challenging than CIFAR-100. There are also no error bars, which are particularly important when the improvements are small. As for Figure 3, I don’t know if there is anything intuitive we can glean from this. I may just be variation between experiments, as far as I can tell.\n\nI think this paper can be made stronger by making the experimental evidence broader, as well as the analysis of why this works stronger. Without these improvements, the reader is left wondering if there really is any significant benefit. We have to remember that PBA is not cheap (perhaps much cheaper than AutoAugment, but more expensive than fixed augmentation). For most practitioners, the complication and compute costs of PBA would probably not be worth adding on top of KD, if the benefits are too modest.\n\nMinor:\n\nIn Table 3, it says \"Ours\" for AlexNet and \"II-KD\" for ResNet8. Should the both be the same?"
        }
    ]
}