{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper considers the usage of learnt general value functions (GVFs) as representations for downstream control tasks in autonomous driving. The authors learn GVFs for hand-defined cumulant functions from off-policy data using TD-learning, using estimated importance weights to keep updates unbiased. A predictive representation (forecast) is chosen by the authors, which measures the “lane-centeredness” and “road angle” of the car at various effective horizons. The authors perform an experimental evaluation on TORCS using images, where they demonstrate that control policies learnt using such a predictive representation generalize better than policies which learn end-to-end directly from images.\n\nThe use and discovery of predictive representations for RL, especially in the deep setting, is an interesting and important problem, relatively under-explored in the deep RL setting. The paper presents little technical novelty, however, the success of learning systems to perform control with predictive representations in this autonomous driving setting is of interest to the ICLR community. However, the paper suffers from lack of clarity and in presentation, in exposition and in the experimental section. Furthermore, I believe the experimental evaluation doesn’t shed light on the performance of the predictive representation estimation, and leaves questions open as to whether the representation improves generalization or alleviates training problems. With these concerns in hand, I believe this paper holds promise, but in its current state is not yet ready for publication at ICLR.\n\nTechnical Contribution: \n\nThe use of predictive state representations / forecasts is to my knowledge a relatively under-explored setting in deep RL, so the success of such representations for autonomous driving is of interest. The primary technical contribution is techniques to use a continually learnt value function as the representation for a downstream policy. The adversarial density estimation procedure introduced in the paper is not novel, and has been studied previously extensively in the community, especially in inverse RL and exploration.\n\nPresentation / Clarity: \n\nThe presentation of the paper can be much improved, in detail and choice of language. Figures and their corresponding captions in Section 4 are difficult to parse. Most figures lack a legend, some lack axis labels, and captions are often non-informative. I found the exposition unclear / confusing at times, and would recommend the authors to revise the paper to improve readability in general, with focus on the introduction and experimental section.\n\nExperimental Evaluation: \n\nThe experimental evaluation is missing an analysis of the learnt predictive representation (GVFs). In particular, I would like to see a stronger analysis investigating whether the learnt value functions are accurate, how fast /effectively these representations adapt to changes in the policy, how accurate the estimated density function is (using the adversarial technique), and whether this importance sampling term is necessary in practice for GVF estimation (as it seems that the experiments are run in a more online fashion). An analysis of this form would lend greater support to the argument that the proposed policy evaluation scheme is effective and useful - and that the predictive representation is actually predicting the desired quantities. \n\nNo training curves are indicated anywhere, so I also have remaining questions as to whether the gain in performance presented by the authors is a function of the representation / policy generalizing better, or if the representation simplifies the RL optimization process, and thus enabling higher performance on the testing set. Furthermore, is the policy able to generalize better because of the choice of representation, or because the policy is of simplified form (linear) - would these hold for more expressive policies using this predictive representation? \n\nFinally, I would encourage the authors to include results for the comparison where the current cumulants are predicted from images as mentioned in Chen et. al 2015. \n\nThe experimental setup is well explained in the main text and in the appendix. \n\nSpecific Comments:\n\n1. Citations are incorrectly formatted throughout the paper\n2. In the first paragraph of Section 1.1, the bicycle model is never explained\n3. The paper does not define what a predictive representation is when it is first mentioned in Section 3\n4. The units for the X-axis for Figure 3 are not specified (also Figure 4, Figure 6)\n5. Equation 5 is missing a constant of 2\n6. As the authors deal with PSRs, the original paper introducing PSRs [1] should be cited.\n\n[1] Littman et al, \"Predictive Representations of State\", NeurIPS 2001"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary.\nThe paper presents a predictive control approach where an agent learns to drive conditioned on predicted future road angles and lane centeredness over time. The effectiveness of the proposed approach is evaluated in the TORCS racing simulation environment. An existing DDPG approach is used as a baseline. Though an interesting problem is addressed, its technical and theoretical contribution is weak.\n\nStrengths.\n1. The paper addresses an interesting problem -- building a model that is predictive of future events and outcomes.\n2. The proposed approach is evaluated in the TORCS simulation environment and shows better performance than the baseline, i.e. DDPG.\n\nWeaknesses.\n1. vs. existing predictive control approaches\nThough the motivation behind this work is interesting, approaches have been introduced to address similar problems. Unfortunately, these approaches are neither properly introduced nor compared well. Some work may include:\n\n[1] Ebert, F., Finn, C., Dasari, S., Xie, A., Lee, A., Levine, S., “Visual foresight: Model-based deep reinforcement learning for vision-based robotic control,” arXiv 2018.\n[2] Tamar et al., “Learning from the Hindsight Plan - Episodic MPC Improvement,” ICRA 2017.\n\nIn addition, there is a large volume of references for the model predictive control (MPC). Some work may include:\n\n[3] Rosolia et al, “Learning Model Predictive Control for Iterative Tasks, A Data-Driven Control Framework,” IEEE T Automatic Control, 2018.\n\n2. Needs more rigorous explanations of the model.\nThe proposed model depends on conventional RL building blocks and uses similar MDP notations, but using conventional notations with different definitions could make readers confused. For example, gamma is usually used for a discount factor, but gamma in this paper is used as a continuation function, which is not clearly defined, i.e. why there is m number of gamma functions? \n\n3. vs. Existing deep driving models.\nAs cited, the work by Chen et al. successfully showed that a ConvNet can predict intermediate features (i.e. distances to lanes) and a simple PID controller can control a vehicle in the TORCS simulator. The work by Bojarski et al. predicts a curvature of driving, which captures information about the shape of the road ahead. More recent driving models [4,5] predict multiple waypoints, which may reduce the need to use the proposed future prediction technique.\n\n[4] Bansal et al., “Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst,” RSS 2019.\n[5] Zeng et al., “End-to-end Interpretable Neural Motion Planner,” CVPR 2019.\n\nMinor concerns.\n1. Figures need more work to visualize better (i.e. hard to see texts, missing legends, and missing x and y labels).\n2. Typos"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper combines predictive state representations (PSRs) with DPG and tests the overall performance on a TORC simulated task. Specifically, the authors propose to train a generalized value function (GVF) where the cumulant is how far the car is from the center of the road as well as the angle of the car. To train the GVF, the author propose to perform off-policy learning with importance resampling. To estimate the ratios, the authors propose to learn a discriminator that predicts which policy the actions came from, and show how to use this discriminator to estimate the likelihood ratio. For DPG, a small neural network is used for the Q function, and a linear policy is used. The authors demonstrate that their method outperforms DDPG-from-images on held-out TORC racing tasks, while not quite reaching the performance of DDPG-from-ground-truth state.\n\nUsing PSRs is a promising direction of work, but I found the contribution of this work rather obfuscated. It seems like there are two sources of novelty: (1) the use of a different number of continuation functions and (2) using a discriminator to estimate the importance ratio, but no details were given about these implementations. The paper would be greatly strengthened by reducing the amount of time spent summarizing prior work, and more thoroughly describing these contributions. Studying one of these contributions in more detail, rather than analyzing the performance of the final policies on a simulated task would also help make the hypothesis and insight of this paper more clear. In particular, I felt like these answers were unanswered:\n\n1. What parts of the algorithm are important to the good final performance?\n2. How important is it to use different discount factors?\n3. How does this work relate to prior work on off-policy evaluation?\n4. How important was it for a target policy to *not* be used?\n5. How was the discriminator trained (e.g. hyperparameters)?\n\nThe authors use ground-truth information when training. I am surprised that a supervised learning method trained with the ground-truth information was unable to recover the performance of DDPG-ImageLowDim. Can more details be given in this training procedure? Is it difficult to train a model to predict the current angle given the image? I find this a bit surprising.\n\nAlso, if an OU-process was used for exploration, how were the behavior policy likelihoods estimated using one-step backups? Since an OU-process isn't Markovian, it seems like this would require doing trajectory-level likelihoods, rather than on-step likelihood as suggested by Equation 3.\n\nMinor comments:\n - Why don't the other methods receive the last two actions?\n - The authors should cite [1,2] when referencing PSRs.\n - Please include legends and axis-labels to the Figures.\n - I don't understand the sentence, \"These predictions must be off-policy because if they were on-policy they would tell us no information to inform the agent how much adjustment is needed to make corrective actions to stay in the center of the lane.\" In particular, why wouldn't on-policy predictions be more useful to an agent?\n\n[1] Singh et al. Predictive state representations: A new theory for modeling dynamical systems. AUAI. 2004.\n[2] Littman et al. Predictive representations of state. NeurIPS. 2001."
        }
    ]
}