{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a method called Extractor-Attention Network (EAN) for Chinese text classification, which uses both word-level features and spelling-level features (pinyin to be more exact). The two types of input features are fed into multi-headed attention and/or CNN-based Extractor blocks for further feature extraction, before finally being passed to prediction layers for computing the output. The main motivation of the paper is to leverage both the pronunciation and the form of Chinese text, instead of most previous approaches which only pay attention to word-level or sub-word level features while being unable to handle common polyphony and homophones which occur quite often in certain domains. The paper also demonstrates the capabilities of the proposed EAN model on 5 different datasets, consistently surpassing all compared baseline models. Congratulations on that.\n\nHowever, upon closer inspection, there are possible details that require further clarifications and proofs:\n\n1. Most noticeably, the compared baseline models are not cited clearly, the citation used in the results table doesn't match the citation style of the rest of the paper, and I find it unintuitive to verify the included numbers. When I finally found the paper, I realised that all the compared models' numbers are from 2015, far from the current SoTA (e.g. Sun et al., WASSA'18; Meng et al., NIPS'19). The evaluated datasets also only contain 2-7 classes and lack domain diversity. Whether the EAN's demonstrated improvement over TAN is significant is also unclear.\n\n2. The exact role the proposed Extractor plays is unclear, as I am not sure whether the improvements come mainly from the Extractor/Multi-headed Attention(TAN) or the main motivation: the incorporation of both word-level features and spelling features. The complex construction of the Extractor although shown limited improvements over multi-headed attention blocks (TAN in the paper, which is hardly explained), is not justified. It would be much better if the authors could explain why they decided to design it like this instead of some other way.\n\nDue to these reasons, I strongly believe a more careful literature review, detailed explanation on the Extractor's design, and more comprehensive study on how spelling-features can actually help Chinese text classification, are absolutely instrumental for publication.\n\nMinor comments:\n\n1. Table 5 which compares the parameter size of different models doesn't make any sense, since the compared models' performances are not even mentioned in evaluation.\n\n2. There are numerous citation styling errors and grammar errors in the paper.\n\n3. Equation #1-6 are numbered incorrectly. Notation of H s in Equation 8-9 and A,X in Equation 11 are all incorrect.\n\n4. How were the hyperparameters chosen?\n\n5. Why so few dimensions of word embeddings but so much for spelling characters?\n\n6. Page 6 sec 5 '...only list the best results of their variations with different hyperparameters.' This doesn't look very nice."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed an attention networks named Extractor-attention network where both word and pinyin are considered for Chinese representation. Improved results are showed on several dataset for text classification task. Overall, the author trying to attack an important problem and the proposed model can potentially be extended for other east Asian language (Korean or Japanese). The experiment results are encouraging although the novelty is rather mild. Detail comments are as following.\n\n1. How was the model performance in other tasks, rather than classification. \n2. More model training detail need to be revealed? For example, the tricks used for training transformers. Does the proposed model need these similar tricks?\n3. a section about ablation studies will help the reader better understanding each component of the network structure.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "[Summary]\nThis paper proposes an attention-based model, Extractor-attention network(EAN), for the Chinese text classification task. The proposed model consists of two encoders: word encoder and Pinyin encoder. The main contribution of this paper is extending architecture for Pinyin character encoder. In experiments, the authors tested and reported the results on 5 Chinese text datasets.\n\n[Comments]\nI cannot agree with the contributions claimed in this paper for the following reasons:\n- First, the model proposed in the paper is not intuitive and novel. It seems a structure that simply combines existing models. If the proposed model is not simply combined by existing models, the authors need a clear explanation of how each part of the model is intuitively designed.\n- Second, the description and definition of the task are not clear. (The authors need to add a detailed explanation and a clear definition of tasks, including examples.) Also, it would be better to be clear about what problems exist in this task and what the authors want to tackle. \n- Third, (I don't know much Chinese but in general), it doesn't seem natural to be given a Pinyin character. It is considered desirable to learn to infer the meaning of text through context without a pinyin character. In addition, the additional use of the Pinyin character seems to be very equivalent to multi-language tasks and extending seems very straightforward. (I think the multi-language task is more general and natural setting, so I wonder why the authors restrict the problem to Chinese text classification.)\n- In the experiment, only the test error rate is reported, but it is not enough to explain how well the models perform in practical problems. In addition to the test error rate, it would be better to show how practical improvements have been made in real examples."
        }
    ]
}