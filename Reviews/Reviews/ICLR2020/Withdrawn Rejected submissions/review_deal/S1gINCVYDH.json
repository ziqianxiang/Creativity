{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors, in this paper, study the problem of efficient exploration and exploitation in RL. They propose an algorithm, as referred to as a suite of solutions, based on the posterior sampling approach to tackle mainly continuous state-action space MDPs. Their method learns the model of the environment using deep neural networks while having the last layer trained using Bayesian linear regression. They utilize this model to design a policy for the exploration and exploitation trade-off.\n\n\nA few comments that did not affect my evaluation but might be helpful to the authors: I might suggest to the authors to reconsider the title of their paper. I can guess the authors mean they aim to design a more sample efficient algorithm and by RL in the title, they mean the RL algorithms. Please rephrase it. Also, the term \"again\" implies that the methods in this filed were sample efficient, then became sample inefficient, and then this paper proposes to make them sample efficient. \n\nMinor changes: In the middle of page 3, the authors define V_{mu,i}^M. I might have mistaken, but either \"i\" goes from 1 to tau-1 or V_{mu,tau+1}^M is zero instead of V_{mu,tau}^M to have general MDP setting. Please correct me if I am wrong. \n\nAlso, the sentence on the very same page, \"for every previous step the expected future value is the reward at the current step plus all future rewards.\" might require a bit more attention. I might guess the authors mean the expected reward.... is the value. \n\nIn theorem 1, borrowed from Osband & Van Roy 2014a, I think the term sigma_P is not defined. Also please reconsider calling sigma_R the variance, since in that case, the theorem might not hold. \n\nGenerally, I have a major concern about the presentation of this paper. While the authors spent the first 4 pages on the discussions that are useful, important, and provide intuition, I have a hard time to see the significance and importance of presenting them. Especially, when the authors spent a considerably small portion of their paper on their own contribution and provided very shallow representing of their algorithm. \n\nRegarding the references, I found this paper not satisfyingly covering the space of literature. Besides the issue of coverage, there are quite a few parts that require citation, but no citation is provided. I would be happy to see a citation related to AdamW, citations of the line of work by Ronald Ortner on continuous MDPs, and also literature on Thompson sampling in linear MDPs. I can extend this list if the authors are interested. \n\n\nRegarding the presentation of their method, first of all, I strongly recommend the authors to provide a self-contained, detailed and profound description of their contribution and their algorithm. I also strongly suggest the authors discuss the motivation and then the detail of each component used in their approach. \n\nI appreciate the fact that the authors provided the code used in their paper, but it would be more appreciated if they would have provided more details of their algorithm, in the paper, to the point that the algorithm could be reproduced by a reader to some extent, without the code.\n\nIn order to draw a conclusion on the performance of the proposed method, I would be happy to see a set of significantly more empirical studies. \n\n\nGenerally, I like the approach and appreciate the authors' effort. While I think the current paper has undoubtedly high potential to improve, I do not think the current version is ready to be published in this conference."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper suggests a practical implementation of Posterior Sampling for Reinforcement Learning (PSLR) using deep networks. The PSLR framework assumes that the model of the environment is sampled from a relatively lower dimensional distribution (for example because of some inherent structure they posses), and exploits this to reduce the sampling complexity to learn an optimal policy, so that the number of samples required is bounded by the complexity of the model distribution, and not by the dimensionality of the state or action space.\n\nThe paper is well motivated and well written, and the research direction is indeed interesting. I would like to see the implementation section pushing further the limits of the theory by using a more complex posterior model – rather than a simple linear layer – and by testing on more challenging environments, where explicitly exploiting the structure of the non-linear model is essential to solve the task.\n\nRegarding the implementation, in Section 4.2 the model is parametrized as s' = W z + s, where W is sampled from the posterior and z is the output of a feed-forward network. Could you further comment on which data the feed-forward network is trained on? Does the theoretical framework remain valid if the feed-forward network is trained only on states from the environment on which the PSLR will later be tested? Wouldn't that imply that the prior model distribution is already biased toward the particular environment?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors implement Thompson sampling for finite horizon MDPs in a continuous control setting where they learn the uncertainty over the dynamics using a Bayesian linear regression model on top of a neural network embedding. They use model-based RL to learn the optimal policy for a sample from the posterior. Specifically, they use a variant of SAC on unrolls from the learned model. The optimization is not entirely clear from Algorithm 2--it seems that they backpropagate through the learned model and a ground truth reward function. This is very reminiscent of Stochastic Value Gradients (Heess, 2015) yet they do not cite this paper. Important ingredient to make the model based RL to work is to use an ensemble of policies. The work lacks in theoretical novelty since the Thompson sampling approach is established and well-known. This would be perfectly fine if the experiments demonstrated clear improvements but that is not the case.\n\n1. The paper is too wordy and repetitive--there's no reason to spend 5 pages discussing Thompson sampling and the motivations of Bayesian RL.\n2. As also pointed out by the authors it is not clear if the observed improvements are not due to the ensemble of policies. This seems like it should be easy to check by running the other methods with ensembles of policies too.\n3. More generally, since the hypothesis of the paper is that incorporating the uncertainty is important, the experiments should focus on that, and e.g. validate that the proposed approach to learning the posterior actually learns the correct uncertainty. One possible ablation would be to simply add (annealed) noise to the weights and check if maybe such an unstructured distribution over dynamics would work too.\n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Posterior Sampling: Make Reinforcement Learning Sample Efficient Again\n===============================================================\n\nThis paper proposes an implementation of PSRL via Neural-PSRL: a neural network is trained fit the transition dynamics, with a last-layer linear approximation used to give a posterior distribution.\nThis model can be used in conjunction with a policy gradient algorithm to give an exploratory policy.\nEvaluated on Mujoco benchmarks this leads to state of the art performance.\n\n\nThere are several things to like about this paper:\n- The problem of extending \"efficient exploration\" to Deep RL is an important one, and PSRL seems like one of the most suitable general approaches to base an algorithm on.\n- The proposed algorithm seems reasonable, and seems like it also performs well.\n- The approach to writing and accessible code release should be praised.\n\nHowever, there are some other places where this paper falls down:\n- Despite the focus on \"incorporating prior knowledge\" in the early part of the paper, it seems like the proposed method actually incorporates very little prior knowledge... for example, the \"Eluder dimension\" of the space represented by these neural networks is presumably huge! This seems like a bit of a bait and switch.\n- I don't think the experiments offer a huge amount of *insight* into what is really happening in the algorithm. It would be helpful to show *really good* performance even on a toy domain, so that you can really say that the exploration via PSRL is really working... maybe something like bsuite = https://github.com/deepmind/bsuite and particularly the \"deep sea\" experiments could help?\n- It's a problem that the actual main paper doesn't give a clear and concise description of the algorithm used. I think this is quite important and it would be much better to put the older results on PSRL to the appendix instead.\n\nMinor points:\n- I'm not sure the title is a great idea... it might put off some people, but also I don't think it's really warranted from the results... it seems like Neural-PSRL is a potentially interesting avenue, but hardly making anything great (again?)!\n- You might want to connect / contrast this work vs the line of research around \"Randomized Value Functions\" http://jmlr.org/papers/v20/18-339.html... which is another approach that tries to extend something like PSRL to Deep RL.\n\n\nOverall, I do think this is an interesting paper, and a promising avenue for research.\nHowever there are probably too many places where the paper could be tightened up: clarity of algorithm description, clarity of support/results, separation of actual claims vs claims of \"making great\" ;D."
        }
    ]
}