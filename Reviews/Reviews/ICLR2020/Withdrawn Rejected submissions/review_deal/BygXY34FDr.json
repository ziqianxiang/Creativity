{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "I reviewed this paper for NeurIPS 2019 and the authors seemed to have submitted a very similar manuscript disregarding the comments of the reviewers after being unanimously rejected by all 3 reviewers at NeurIPS 2019. The following is very similar to my NeurIPS review.\n\nThis paper proposes a new method which uses variational information bottleneck to learn state representations and successor features to learn a policy for visual navigation.\n\nIn terms of originality and novelty, the method does not introduce very novel architectures or algorithms, but rather introduces a new combination of known techniques for the task of visual navigation. Successor features have been used in several prior works as mentioned by the authors. This paper just uses them for learning the policy. Variational Information Bottlenecks (VIB) is a known method for learning representations, in this paper VIB is used for Siamese style networks.\n\nThe lack of significant technical novelty is acceptable if the proposed method is empirically shown to perform very well. However, I have concerns about the empirical evaluation of the proposed method. Firstly, in my opinion, the agent should have a stop action. The success criterion should not be just reaching the goal state but using the stop action at the goal state. This is important for the experimental setup in this paper because I believe the authors use an episode length of 500 for environments with a total number of states less than 700. A random walk agent would have a significant chance of hitting the goal state (on that note, random walk baseline is missing). Without the stop action, it is difficult to understand whether the policy just learns to visit as many different states as possible or really learns to find the goal. This is especially important as the success rate is very low (20% for the proposed model, 15% for the baseline).\n\nSecondly, instead of defining their own training and test sets, authors should use training and test sets used by some prior work. This helps in proper comparison, benchmarking and reproducibility. With a new train and test set, it is unclear whether the environments were selected to highlight the strengths of the proposed method or the results generalize to other datasets as well.\n\nThe setup could also benefit from stronger baselines. I would have liked to see a baseline based on successor features. \n\nThe submission is not polished. The title in the PDF is wrong, and authors have provided a non-anonymized GitHub link.\n\nOther minor comments/questions:\n- Why is the goal state repeated four times in the input?\n- The authors mention that training a convolutional autoencoder is prone to over-fitting. This is not obvious to me, could you provide a reference?\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary\n\nThis paper proposes an actor-critic version of the Successor Features (SF) framework that relies on the Universal Successor Features Representations developed by [1,2]. The framework is trained end-to-end, with successor features being learned from a shared Siamese Network architecture, which is in turn regularized by a Deep Variational Information Bottleneck [3]. The architecture is tested on the target-driven visual navigation within the AI2THOR simulator, where it outperforms a Siamese Network baseline [4].\n\nThe algorithmic novelty of the contributions in this paper are marginal and while empirical results indicate a more performant system on the AI2THOR benchmark, these results are not explored and analysed in sufficient depth to meet the bar of publication. \n\nMotivation\n\nThe core of this paper is a Deep RL agent that relies on an actor-critic Successor Features framework. Actor-critic SFs have previously been explored by [1], and while there are minor differences with this framework (specifically in the reward regression loss), I do not see this as a novel contribution. In contrast to [1], the authors learn SF representations using a Siamese network architecture, which in turn is largely taken from [4]. The authors add an IB bottleneck to the Siamese network in the form of [3]. While there is novelty in this application, it is unclear to what extent it helps performance.\n\nEmpirically, the author’s proposed architecture, VUSFA, outperforms [4]. The authors perform a nice ablation study where they show that the A3C-SF framework does not necessarily perform [4]: it is necessary to directly condition the policy on the SFs (this issue only arise in the actor-critic setup if SFs only feed into the critic) and adding an IB bottleneck yields further improvements when the training set is very small. \n\nHowever, these results are obtained by training on a mere 20 goals, representing less than 1.2% of all possible goals, and hence generalization performance do not go beyond a 20% success rate in terms of reaching new goals. It is unclear if this is a constraint of the environment or a choice by the authors. Regardless, the low success rate makes it hard to say anything about how these architectures would behave if trained on a richer distribution of goals. In particular, it seems that this data-scarce regime favors the author’s proposed method, since the IB module is a regularization mechanism. A more diverse training set could change the conclusion of the paper, in particular as it appears allowing the agent to fine-tune renders the IB mechanism redundant in terms of generalisation performance.\n\nAdditional comments\n\n- There absolutely must be a related works section in the main manuscript.\n- The Successor Feature Dependant Policy is not the first architecture to directly condition the policy on SFs; most prior works directly derive the policy from the SFs via the Q function. \n- Section 3.1. You should mention that the Siamese Network approach is closely related to prior work. It appears as a novel contribution in the current manuscript.\n\nMinor comments\n\n- Eq. 3; it is worth noting that SFs incorporate dynamics of the environment - under a given policy (distinct from the goal).\n- Citation of A3C should be moved up to the first time you mention it\n- Section 3.2. Needs to be put in relation to [1], which is very closely related.\n- Section 4. ‘Traditionally, SFs are not directly consulted when determining an action’. This is incorrect - in most prior works, SFs are used to explicitly derive a policy (e.g. [5, 6]).\n- What is the meaning of Eq. 7? If it is denoting a stop-gradient operation, it needs to be reformulated. It currently appears as a conditional independence between the policy and SFs.\n- Notation in the Information Bottleneck part needs to be revised. The distributions q and E are introduced in equation (9) without a definition (I believe q is never defined), and it is unclear to me what J(.)_{min}  means. Is it the same as min J(.)?\n- Eq. 12 is overloading the notation of E as both expectation and encoder, which is rather confusing.\n- How does Eq. 12 relate to entropy regularized actor-critic methods like TRPO and PPO?\n- Section 71. Weather -> whether\n- Title needs to be updated\n- code link should be anonymous\n\nReferences\n\n[1] Ma et. al. Universal Successor Representations for Transfer Reinforcement Learning. ICLR workshop. 2018.\n[2] Ma et. al. Universal Successor Representations for Transfer Reinforcement Learning. arXiv. 2018.\n[3] Alemi et. al. Deep variational information bottleneck. arXiv. 2016.\n[4] Zhu et. al. Target-driven visual navigation in indoor scenes using deep reinforcement learning. ICRA. 2017.\n[5] Dayan. Improving Generalisation for Temporal Difference Learning: The Successor Representation. Neural Computation. 1993.\n[6] Barreto et. al. Successor Features for Transfer in Reinforcement Learning. NeurIPS. 2016.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "Comments and Questions : \n\n\t- The core contribution of the paper is to introduce a new architecture for a policy that is useful for successor features. This new architecture uses a variational information bottleneck approach and can be implemented readily on top of existing successor feature based approaches like USFA. \n\t- USFA are adapted for use in navigation tasks - but the paper seems to introduce two contributions based on transfer learning and stability in performance? It is not clear from the introduction what the exact contribution is. I think the paper needs better clarity and write-up. It is missing a lot of details and the core idea is not discussed in sufficient details to fully understand the contribution of the paper exactly. \n\t- It seems to me that the majority of the contribution comes from the new proposed architecture - but it somehow adapts an information bottleneck based approach? Why is this useful and what does information bottleneck mean in context of a policy for use in successor features?\n\t- I am not convinced by the proposed novelty of the paper - there are a lot of buzz terms like transfer learning and generalizability, and adapting this architecture for navigation tasks? Seems to me more like an adaptation of USFA for a navigation based tasks, and using a variational information bottleneck VIB based architecture only? \n\t- The overall contribution seems to be the useful of VIB in successor features, or more concretely for universal successor features and looking at the context of goal conditioned or latent variable conditioned policies? \n\t- The architecture has some novelty in terms of conditoning the policy based on the abstraction of future states for each action selection. I am not aware of this approach from previous works, but seems like an interesting idea for action selection and learing a policy conditioned on future visitation of states (eq : 7)\n\t- From the proposed set of experimental results, it is not clear whether the proposed method is indeed useful. Experimentally or even theoretically, the approach in the paper does not seem convincing enough. It requires more work and better clarity and presentation of the idea and results, including enough details. I would encourage the authors to present the core idea of the paper more concretely. \n\t- Overall, I do not think this paper is ready for acceptance, and would vote for a rejection. It requires significantly more work. \n"
        }
    ]
}