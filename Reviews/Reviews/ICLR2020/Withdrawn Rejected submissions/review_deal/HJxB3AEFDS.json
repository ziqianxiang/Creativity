{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Authors suggest a novel method for pairwise text classification, hand engineered for a patent inventor match system. The idea is that each pixel is assigned to a character. Then for every bi-gram of a word color the pixels connecting those two pixels with a fixed color (green for one author, red for the other). color the first bigram in blue. Then add these two together. train a cnn on top. They achieve results in the ballpark of ther random forest or rule based methods.\n\nOne immediate question is that why is this a sensible idea?! The use case of convolutions in image processing is based on the fact that a shape happening in one location of the image is the same as if happening in the other part. Essentially image processing should be translation invariant (a ball is a ball no matter where in the image). Where as in here your grid position have specific meanings. So a green line in the top of the image has a very different meaning than a green line in the bottom.\n\nThere is no explanation for why not using a text based classifier as in the models used in GLUE tasks rather than transforming the text into an image. What is wrong with using a text based classifier? Why images?\n\nWhy stacking the two images as different color channels rather than using a siamese network? No justification is provided in the paper. When you stack them together the channels get compared and smashed together with each other at the first  CNN level. Where as typically one uses a CNN to get some level of abstraction and feature extraction. So makes sense to process each image separately for some layers then merge them together when comparing two images.\n\nHow exactly the bigrams are connected is not well defined. It is only mentioned with a straight line. For example, in the \"EN\" image of Fig. 1, why U is colored and not B? There are several shortest paths in each image. \n\nThe numbers reported in the experiments are close to each other. STD of the experiments has to be reported.\n\nTable 2 is not clear why \"Ours\" does not have the dataset name? Other methods are reported on IS or E&S. Which one is Ours reported on?\n\nThe paper does not provide justification for the main idea and several design choices. Experiments are also limited to one patent inventor classification. It would be more of an interest to report on a GLUE text classification task (https://gluebenchmark.com/tasks)"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "The authors consider the problem of names disambiguisation for patent names inventors. They define this problem as a binary classification problem: do 2 names refer to the same person. In this article, they propose to build a image page representation of the 2 strings to compare and to apply an image classifier. This approach is different to standard approaches that  either extract features from the pair (edit distance, cosine distance) and classify the pair or take the two stings as input (siamese networks). \n\nThe authors claim that the benefits of the method is to allow to use image classifiers, but one can argue than methods have been specifically design for text and that is is a disadvantage to discard them all.\n\nOne advantage of the proposed method is that it is independent from word ordering : this is correct , but how would it compare to a simple bag of ngrams ?\nThe proposed method could be interesting but the task does not allow to show in which case it could be of use : the model is tested on only one task for which the state of the art  is alread 99% F1. The authors should use another task or explain why a 0.3% improvement  is significant and important.\n\n\nData : give examples of the data from the different datasets. It is not clear if the 2 considered datasets are merged or separate.\n\nBlocking : is it exact match? what is the impact of the task : names that can not be identified anymore after this step?\n\n4.1 :  what happens if a letter occurs several times ?\n\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "\nThis paper proposes a method to map a pair of textual information into a 2D RGB image. They achieve this by heuristically creating a character map in a 2D matrix and then add color according to the character within the text (e.g. a line connecting bi-gram characters). To compare 2 textual pieces, different colors are used. This novel “text representation” can be fed to 2D convolutional neural networks (image classifiers). The proposed representation is evaluated on a patent inventor disambiguation task, and shown to slightly our perform existing methods.\n\nI would not recommend this paper because the method in this paper is not well motivated and the empirical results do not demonstrate the usefulness of the proposed method. In addition, the quality of writing can be improved in several places.\n\nThe goal to capture string similarity (character overlaps) is well motivated for the name disambiguation task, but it is unfounded on why a 2D mapping would provide additional benefits from the existing features. The paper could be improved by discussing the strength and weaknesses between the existing features and the proposed representation. In the experiment, it is shown that a simple string similarity feature and a random forest (Kim et al., 2016) already achieve 99% in the F1 score. A small increment of 0.09 in the F1 score cannot justify the proposed algorithm. I think the author should select other tasks altogether. Finally, the fact that any random character map gives the same result is quite counter-intuitive and requires more analysis such as ablation on coloring (connecting lines, blue color, …).\n\nAs for the writing, there are several issues. First, the format of the table is incorrect. Some tables are out of the border and the captions are on the top. More importantly, the method to construct the character map is unclear and a significant part of the map is left in the appendix.\n"
        }
    ]
}