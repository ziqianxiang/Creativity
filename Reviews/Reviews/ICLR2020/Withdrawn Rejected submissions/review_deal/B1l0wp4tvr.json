{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper considers the information plane analysis of DNNs. Estimating mutual information is required in such analysis which is difficult task for high dimensional problems. This paper proposes a new \"matrix–based Renyi’s entropy coupled with ´tensor kernels over convolutional layers\" to solve this problem. The methods seems to be related to an existing approach but derived using a different \"starting point\". Overall, the method is able to show improvements in high-dimensional case.\n\nBoth R1 and R3 have been critical of the approach. R3 is not convinced that the method would work for high-dimensional case and also that no simulation studies were provided. In the revised version the authors added a new experiment to show this. R3's another comment makes an interesting point regarding \"the estimated quantities evolve during training, and that may be interesting in itself, but calling the estimated quantities mutual information seems like a leap that's not justified in the paper.\" I could not find an answer in the rebuttal regarding this.\n\nR1 has also commented that the contribution is incremental in light of existing work. The authors mostly agree with this, but insist that the method is derived differently.\n\nOverall, I think this is a reasonable paper with some minor issues. I think this can use another review cycle where the paper can be improved with additional results and to take care of some of the doubts that reviewers' had this time. \n\nFor now, I recommend to reject this paper, but encourage the authors to resubmit at another venue after revision.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper concerns the \"information plane\" view of visualizing and understanding neural net training. Kernel-based estimators of Renyi entropy-like quantities are applied to study moderate-sized DNNs. A straightforward extension to CNNs is presented.\n\nMajor comments:\n\nI found this paper extremely hard to follow. I think a lot of my difficulty was a lack of familiarity with the papers this work was building on, but I also felt the main line of reasoning was unnecessarily opaque. I've tried to indicate where some clarifying wording might help readers such as myself.\n\nIn general I am very skeptical that reasonable entropy-like quantities can be estimated reliably for high-dimensional data using anything along the lines of kernel density estimation, especially based on a single minibatch or small collection of minibatches! The authors provide no experimental evidence that these estimates are even close to being accurate (for example on synthetic datasets where the true entropy / mutual information is known). Clearly the estimated quantities evolve during training, and that may be interesting in itself, but calling the estimated quantities \"mutual information\" seems like a leap that's not justified in the paper.\n\nThroughout the paper, CNNs are referred to as having some special difficulty for entropy estimates because of the channel dimension. This is misleading. For DNNs the activations are vectors, and so a kernel defined over rank 1 tensors is needed. Even without the channel dimension, CNN activations would be rank 2 tensors, and so a kernel would need to be defined over rank 2 tensors. Going from rank 2 to rank 3 doesn't pose any special difficulties. Indeed the most obvious way of defining a kernel over higher rank tensors is to flatten them and use a kernel defined over vectors, which is exactly what the paper does in practice.\n\n\nMinor comments:\n\nIn the introduction, it would be helpful to include some relevant citations after the sentence \"Several works have demonstrated that one may unveil interesting properties... IP\".\n\nIn section 3, the multivariate extension proposed by Yu et al. (2019) seems like an interesting side note (since it was used in a previous attempt to estimate information plane for CNNs), but it doesn't seem central to the paper, and I personally found it unnecessarily confusing to have it presented in the main text. What about moving sections 3.2 and 4.2 to the appendix for clarity?\n\nIn section 3.1, \"over a finite set\" is probably incorrect (\"probability density function\" implies a continuous space for X, as does the integral in (1)).\n\nIn section 3.1 (and the appendix), \"The ones developed for Shannon\" seems imprecise. \"Certain approaches developed for estimating the Shannon entropy\"?\n\nIt's not clear what \"have the same functional form of the statistical quantity\" means. Which statistical quantity? What aspects of the functional form are similar? Please elaborate in the paper.\n\nI think \"marginal distribution\" is incorrect. It's representing a whole probability distribution, not just a marginal distribution. Which marginal would it be in any case?\n\nSection 3.2 states \"The matrix-based... entropy functional is not suitable for estimating... convolutional layer... as the output consists of C feature maps\", but as discussed above, there is no special difficulty caused by the channel dimension. Even if the channel dimension were not present the difficulties would be the same. (Also, it seems like defining a kernel over the rank-3 tensors is an extremely natural / unsurprising thing to try given the set-up so far.)\n\nIn section 4.1, \"can not include tensor data without modifications\" seems misleading for a similar reason. One of the great things about kernels is that they can be defined on lots of objects, including things like rank 3 tensors!\n\nNear (8), it would be very helpful to state explicitly what is done for the joint entropy term in (5). It sounds like this term, which involves the Hadamard product, in practice amounts to summing the Euclidean distances between x's and between y's, and it might be helpful to the new reader to point this out. (It also highlights that the method is easy to implement in practice).\n\nThe discussion in section 4.2 is only valid for the RBF kernel, but the first paragraph of that section makes it sound like it is true more generally.\n\nAt the bottom of section 4.2, if the proposed approach is equivalent to the multivariate approach, then how can one suffer from numerical instability while the other doesn't? Also, numerical stability makes it sound like an artifact of floating point arithmetic, whereas I think the point that's being made is more mathematical? Please clarify in the paper.\n\nIn \"enabling training of complex neural networks\", shouldn't \"training\" be \"analysis\"?\n\nIn section 5, under \"increasing DNN size\", I wasn't clear on the meaning of \"deterministic\" in \"neither .... is deterministic for our proposed estimator\". Random variables can be deterministic or not, but how can a mutual information be deterministic?\n\nUnder \"effect of early stopping\", isn't looking at the test set entropies (as is done elsewhere in the paper) much more relevant to overfitting than considering different \"patience\" values?\n\nIn the bibliography, two different papers are \"Yu et al. (2019)\".\n\nIn appendix A, \"the same functional form of the statistical quantity\", \"marginal\", etc don't seem quite correct, as mentioned above. Also the first equation should not have a comma between f(X) and g(Y) (which if I understand correctly are being multiplied)."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "=== SUMMARY ===\n\nThe paper...\n- suggests an estimation method of mutual information that is less susceptible to numerical problems when many neurons are involved\n- suggests an estimation method tailored to convolutional neural networks. \n- analyses whether H(X) ≈ I(T;X) resp. H(Y) ≈ I(T;Y) is true in high dimensions for their suggested estimator.\n- analyses how common regularization techniques affect the learning dynamics. They show that early stopping prevents the learning process from entering the compression phase\n\n\n=== RELATED WORK ===\n\nPrior work is clearly presented and well-treated. The introduction and related work give a good overview of prior work on information plane theory, touching on the important contributions from Tishby & Zaslavsky [2015], Schwartz Ziv & Tishby [2017], Saxe et. al [2018], and Noshad et. al [2019], among others. In Section 3, Renyi’s \\alpha-order entropy is outlined, and the multi-variate extension proposed by Yu et al. [2019] is described.\n\n\n=== APPROACH === \nMultiple papers are put into perspective, and the findings of these papers serve as a foundation for the suggested method\n- Renyi’s Entropy is presented as a generalization of different entropy measures (among them Shannon Entropy). Computing Renyi’s Entropy however necessitates to get a high-dimensional density estimation over the data distribution\n- Giraldo et. al. suggested a kernel-based method to estimate Renyi’s Entropy solely on data without estimating the (intractable) density. An estimation for the mutual information can also be found in said paper\n- Yu et al. suggested a generalization of b) that can handle C variables instead of only two. As this computation includes taking the product over C values that lie within (0,1/N) the result tends to 0.\n\nAn RBF-based kernel is defined for a fixed conv. layer by computing the Frobenius norm w.r.t two 3D-output tensors of the conv. layer X_i, X_j. A derived matrix A is defined similarly to Giraldo et al.\n\nThis way, C individual kernels are defined, which are combined according to Yu et al.\n\n\n=== EXPERIMENTS ===\n\nComparison to Previous Approaches\n- Very detailed and clear structure of experiment. \n\nIncreasing DNN size\n- The experiment shows that there is both a fitting and a compression phase\n- H(X) ≈ I(T;X) resp. H(Y) ≈ I(T;Y) was disproven\n\nEffect of Early Stopping\n- Early Stopping prevents the learning process from entering the compression phase. The authors conjecture that the compression phase is related to overfitting -- it would be interesting to see some evidence to support this.\n\nData Processing Inequality\n- Enough evidence is provided to sustain that the data processing equality also holds for the suggested estimator\n\n\n=== OTHER COMMENTS ===\n\nThe paper would benefit from providing more rationale on why the suggested estimation method is numerically stable (as opposed to Yu et al.), maybe by supplying another proof.\n\nTechnical mistakes\nSection 4.1. Introduces X_i as a 3D tensor of dimensions C x H x W. Eq (7) mentions the Frobenius norm which is defined over matrices. We assume that X_i should be rather defined as a 2D tensor of dimensions H x W for a fixed layer index after reading Section 4.2.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors try to resolve the problem of estimating mutual information between high-dimensional layers in neural network. Specifically, the authors try to tackle the convolutional neural networks, by propose a novel tensor-kernel based estimator. They use the proposed estimator to discover the similar trends on the information plane as the previous works.\n\nIn general, I think the introduction of the tensor kernels for mutual information estimation is the key contribution of this paper. However, I think this contribution is a little bit incremental, compared to the multivariate matrix-based version introduced by Yu et.al. In formula (7), the authors use the Frobenius distance of two tensors as the input of the kernal, which is equivalent to vectorize the tensor and use Euclidean distance. This approach does not capture the special structure of tensor and seems incremental. Also, the phenomenon of \"turning point\" on information plane for neural network has been challenged since its first publication. Despite its high citation, the deep meaning of this phenomenon has not been clearly studied. So an incremental change on computing such a questionable phenomenon make the contribution of this paper not very strong.\n\nSo I would like to give a weak reject. "
        }
    ]
}