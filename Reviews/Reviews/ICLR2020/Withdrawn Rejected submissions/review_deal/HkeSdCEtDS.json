{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper proposes an alternating dialog model based on transformers and GPT-2, that model each conversation side separately and aim to eliminate human supervision. Results on two dialog corpora are either better than or comparable to state-of-the-art. Two of the reviewers raise concerns about the novel contributions of the paper, and did not change their scores after authors' rebuttal. Furthermore, one reviewer raises concerns about the lack of detailed experiments aiming to explain where the improvements come from. Hence,  I suggest rejecting the paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a pre-trained language model architecture specifically used for task-oriented dialogue systems. The basic idea is to alternate between the likelihood of two parties in a dialogue. This simple yet effective approach yield significant improvements in baseline dialogue system datasets in terms of both BLEU score and accuracies. Experiments are done throughly by comparing to BERT and GPT-2, which reflected the cutting edge research in pre-trained language model. This paper opened up potential new domains that can inspire a wide range of work and it fits very well into the ICLR community. "
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper is concerned with reducing the amount of manual dialog state or dialog act labeling in task-oriented dialog, following the lead of Eric and Manning (2017) who did not require any such explicit annotation. The authors note that Eric and Manning’s approach didn’t do well on more recent dialog datasets in comparison to e.g. Sequicity (Lei et al., 2018), which trains an explicit belief state. The first main contribution of the submission is to exploit GPT-2 to outperform Sequicitiy and other techniques, doing so without explicit dialog state/act supervision. The second contribution is an alternating parameterization of the model that distinguishes between agent and user utterances. Experimental results show that the method of the paper (ARDM) is either superior or on par with methods that exploit hand-labeled dialog states/acts.\n\nOriginality/impact:\n\nI think the approach of the paper is well motivated, though quite simple. The idea of improving on a generation task with GPT-2 is a tried one, and its effectiveness isn’t really surprising. The alternating parameterization is interesting but has been done before in (e.g.) Zhao and Kawahara (2019) and (Zhang et al., 2019) [1]. Overall, I think the paper doesn’t bring much novelty, but results are promising, and this work represents a step towards making task-oriented dialog system less reliant and hand coded linguistic information. It could have an impact for the dialog community.\n\nEmpirical contribution:\n\nMy main concern with the paper is a lack of ablation, which makes it hard to understand where the improvement comes from. More specifically:\n\n* It is not known how much the alternating nature of ARDM contributes the overall gains compared to (1) other details of the model; (2) pre-training with GPT-2. One way to ablate ARDM’s alternating mechanism from other characteristics of the model would be for example to tie the parameters of the user and the agent. \n\n* Table 1 compares GPT-2 with ARMD, so we don’t know if the improvement comes from either fine tuning on CamRest676, from ARDM’s alternation between speakers, or from other characteristics of ARDM. Note: The text characterizes GPT-2 as “a pre-trained large-scale language model GPT-2”, so I presume “GPT-2” here means without fine-tuning.\n\n* Results of Table 3 are perplexing and more analyzes would be needed to understand what is really happening. Considering that both TransferTransfo and ARDM are based on transformer and GPT-2, and are apparently fine-tuned on the same data, I find it strange the gap between the two is so big on perplexity (10.1 vs TransferTransfo’s 19.9) while TransferTransfo is actually superior on BLEU. I understand BLEU is problematic for dialog evaluation as it treats references (gold responses) as the only reasonable responses (Liu et al., 2016), but that makes perplexity as an evaluation metric problematic for exactly the same reason. In fact, (Schwenk et al., 2006)[2], (Luong et al., 2016)[3], and others have shown that BLEU and perplexity are highly correlated, which isn’t surprising as both penalize generated responses to the extent that they lexically differ from the gold standard. So, these results look a bit strange/suspicious to me, and I think deserve further investigation. Sample outputs of both TransferTransfo and ARDM might help better understand the problem (Why showing only ARDM’s? When showing generation examples, I think it is standard practice to show outputs of multiple systems, and not just for the system of the paper). \n\nMinor comments:\n\n- [Manning and Eric, 2017] should be [Eric and Manning, 2017]\n- ARDM with “recurrent” maybe isn’t a good name as it may imply it is RNN instead of Transformer based.\n- Section 3.1: I suggest you explain the “memory mechanism” more formally and in more details, e.g., M_{t-1} doesn’t seem to be used anywhere.\n- “assume there is only one layer in Transformer”: Is it for the sake of the presentation only, I assume?\n- Tables 1-3: perhaps add confidence intervals as some of the differences are quite small, and so are some of the test sets.\n- Table 3 doesn’t seem to be referenced anywhere.\n- Table 4: I would recommend showing ARDM and TransferTransfo side by side.\n\n[1]: https://arxiv.org/abs/1903.05759\n[2]: https://www.aclweb.org/anthology/P06-2093/ (e.g., Fig. 5 shows the high correlation between BLEU and perplexity)\n[3]: https://arxiv.org/abs/1410.8206\n\nQuestions:\n\n(1) In 4.3.1, are capacities of the two models the same? If ARDM has (roughly) double the number of parameters due to its modeling of two speakers, what about comparing TransferTransfo at same model capacity to make the comparison more meaningful?\n\n(2) I am confused how you (the authors) compute belief state success F1 for GPT-2, as the text says there is only pre-training with this model, which I think implies there is no fine-tuning on the belief state identification task. Could you provide more details?"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper explores methods to incorporate a pretrained language model into a dialog system. The authors propose Alternating Recurrent Dialog Model (ARDM) where the pretrained language model is used to initialize both the user LM and the system LM. The authors also present a memory module to augment this dialog system where each memory slot contains a <key, value> pair derived from hidden states for each dialog turn.\n\nI think the writing of the model section could be improved.\nA few clarification questions for the authors:\n- How is the hidden state for the dialog turn t (i.e., h_t) derived from hidden states of tokens in turn t? Supposed there are N tokens in turn t, which Transformer hidden state is used as h_t? \n- How is h_t used to compute p(w_i | w_{<i}, u_{<t}, s_{<t})?\nIf I understand correctly, the authors use t to index dialog turns, but then they seem to use the same symbol to denote token indices in the same section. So the exact model, although appears to be simple, is very confusing to me.\n\nIn terms of experiment results, the authors show that their approach improves over the basic GPT-2 and is competitive with baseline methods that rely on more supervision.A few clarification questions regarding the experiments:\n- Did the authors tune the GPT-2 model on each dataset, similar to ARDM as well? Or are the GPT-2 results shown in Table 1 after fine-tuning?\n\nIn summary, this paper is a plug-and-play extension of the GPT-2 pretrained model that could be explained more clearly."
        }
    ]
}