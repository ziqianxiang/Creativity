{
    "Decision": {
        "decision": "Reject",
        "comment": "The paper proposes a solution based on self-attention RNN to addressing the missing value in spatiotemporal data. \n\nI myself read through the paper, followed by a discussion with the reviewers. We agree that the model is reasonable, and the results are promising. However, there is still some room for improvement:\n1. The self-attention mechanism is not new. The specific way proposed in the paper is an interesting tweak of existing models, but not brand new per se. Most importantly, it is unclear if the proposed way is the optimal one and where the performance improvement comes from. As the reviewer suggested, more thorough empirical analysis should be performed for deeper insights of the model. \n\n2. The datasets were adopted from existing work, but most of them do not have such complex models as the one proposed in the paper. Therefore, the suggestion for bigger datasets is valid. \n\nGiven the considerations above, we agree that while the paper has a lot of good materials, the current version is not ready yet. Addressing the issues above could lead to a good publication in the future. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes a Transformer-based model with cross-dimensional self-attention for multivariate time series imputation and forecasting. The authors consider time series with observations collected at different locations (L), for different measurements (M), and at different timestamps (T). The authors describe 4 different self-attention mechanisms based on how the three dimensions, and it turns out the proposed Decomposed approach achieves the best performance and has moderate model complexity among the four. Experiments on several traffic and air quality datasets show the superiority of the proposed model.\n\nOverall the problem and the proposed model are well motivated. Handling time series data with missing values is quite important, and the authors design the novel cross-dimensional attention mechanism which is reasonable and performs well. Especially, the authors compare several recent RNN-based models. The proposed method outperforms these strong baselines in imputation and long-term forecasting tasks.\n\nHowever, I do have a few questions and concerns.\n\nIt would be quite helpful to see the training time and model size comparisons with baselines, which is to validate the claim in the introduction that `replacing the conventional RNN-based models to speed up training`.\n\nThe proposed model treats all three dimensions equally, with direct attention of every two variables, and is independent with the order. Though effective and mathematically clean, I am not sure the temporal/spatial smoothness and dependencies in time series are properly modeled in this way -- as time series is not the same as embedded sequences in NLP. This may explain why the performance on short-term forecasting is relatively unsatisfying.\n\nIt seems that the proposed model is designed for missing completely at random (implied by the statement `... due to unexpected sensor damages or communication errors` from the introduction, and the experimental settings on adding missing values). Many missing variables in time series may be missing at random or even not at random.\n\nAbout the experiments:\nTwo datasets of the three mentioned in the main paper have M=1, which degrade the proposed model from 3-dimensional to 2-dimensional.\nWhy forecasting experiment is conducted on METR-LA, while using the imputation for forecasting is conducted on a different dataset and without comparing other forecasting baselines?\nWhat are the metrics used in Tables 6, 7, 9? Several metrics are used (RMSE, MSE, MAPE, MAE, MRE) while results on different datasets are shown in different but not all metrics. Is there any reason to cherry-pick metrics for different experiments?\nIn Table 4, the proposed method's results in RMSE is consistently better than shown in MAE compared with other baselines. Any explanations would be useful.\n\nThe overall idea is relatively easy to follow, while some detailed descriptions should be added or clarified.\nWhen taking health-care data as an example of geo-tagged time series, could you explain or provide references?\nFigure 2 only demonstrates 3 attention mechanisms, and the Shared should also be included.\nS(i,j) is used in Section 3.1 without explicit formal definition.\nPlease clarify the sentence about \\sigma below Equation (4).\nPlease refer to the section number explicitly in Supp if used. (E.g., on Pages 5 and 7.)\nOn which dataset and what settings are the results in Table 1 computed? The numbers are helpful, but it would be better if the results are computed based on the hyperparameters (e.g., T, L, M, d_V, etc).\n\nMinor typos:\nPage 3, Paragraph of RNN-based data imputation methods: `...indistinguishable. so that...`\nPage 3, Section 3.1, `..where Then, ...`\nPage 14, A' is used to denote the reshaped tensor, while \\tilde is used in the main paper.\nPage 16, `During testing,`"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper empirically studies the effectiveness of transformer models for time series data imputation.  In particular, the paper studies the effect of generalized forms of self-attention that can attend across dimensions of the input.\n\nGeneralizing self attention to work across dimensions of a multi dimensional time series is a good idea, and the experiments in the paper seem to support its effectiveness.  The paper does provide some ablation results to compare their three forms of modified self attention, which is good.  I believe the primary contribution of the paper is as an empirical study into the effectiveness of generalized self attention in time series datasets.\n\nHowever, I have to vote to reject the paper.\n\nMy primary issue with the paper is its tone.  While generalized forms of self attention are a good idea, the paper strongly emphasizes that it is a novel idea.  In particular, image models that use self attention regularly attend too multiple dimensions.  Consider for instance arXiv:1802.05751, arXiv:1904.09793, arXiv:1712.09763, arXiv:1805.08318.\n\nThere is also several existing generalized self attention discussions: arXiv:1812.01243 or arXiv:1805.00912\n\nThe idea of extending self attention to look at multiple dimensions is fairly obvious.  If the paper changed its tone from purporting to construct a novel method of self-attention (which I do not believe it does), to being an empirical study of the utility of self attention models for doing time series imputation I would be much more willing to accept it, though as a purely empirical study the bar would be high on the standard of the experiments, the reported experiments are on rather small datasets.\n\nAlso, the paper could use an edit for grammar."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The classic transformer network is designed to capture the dependencies along one dimension. Applying the transformer to spatiotemporal data creates the challenge of modeling the attention in a computationally efficient way (time, location, variable). This paper investigates different ways of implementing 3D attention and extensively evaluates the performance of them. \n\nThe proposed problem is a practical problem and I think the main contribution of this paper is valuable. However, there is always a trade-off between having access to the entire history and the ability to be used in streaming settings. The need for the processing of the entire history of time series increases the latency of the algorithm. Also, the authors should report the actual run-time of the algorithms on the real data (beyond Table 1 and compared to the baselines).\n\nThe main criticism of the experiments is that the datasets are very small. For example, NYC-Traffic has only 186 time series which is considered to be of the toy-scale. Also, if you have run the experiments and not copied the reported data, make sure to indicate that in the paper. Citing a paper in a table usually means the numbers are reported from the paper.\n\nThe ways that the authors decompose the attention tensor reminds me of the works in different tensor decomposition algorithms in spatiotemporal data [1, 2].\n\nThere are minor typos in the paper too. For example, see the first in-line equation in Section 3.1.\n\n[1] Kolda, T. G., & Bader, B. W. (2009). Tensor decompositions and applications. SIAM Review, 51(3), 455-500.\n[2] Bahadori, M. T., Yu, Q. R., & Liu, Y. (2014). Fast multivariate spatiotemporal analysis via low-rank tensor learning. In NeurIPS.\n"
        }
    ]
}