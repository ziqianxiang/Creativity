{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "In this paper, the authors propose to disentangle the object-information from a pre-trained CNNs for vision tasks. An explainer is introduced to extract the object-related information from the noisy feature maps of a CNN and the disentangling can be achieved in an unsupervised manner. The proposed method is original, at least to my limited knowledge of related work.\n\nHowever, I have several concerns about this paper.\n1.\tThe most important one is about novelty. From my understanding, the proposed method just applies the well-developed interpretable CNN (Zhang et al., 2018c) in the distillation stage. It enables the proposed method uses a much smaller explainer to extract the object-related information from the feature map without affecting the performance. The difference between the proposed method and interpretable CNN should be verified clearly.\n2.\tThe proposed method lack of motivation. In the second paragraph of Sec. 1, the authors claim that human annotation may contain subjective bias without giving an accurate definition of interpretability. A more detailed definition and illustration of the subjective bias are helpful to verify the authors’ claim. Besides, in Table 1, the comparison is not clear, making its priority compared to existing methods not clearly presented. The argument like “high discrimination power” and “potential broad applicability” are vague and lack of support.\n3.\tThe experimental results are not convincing. First, the proposed method is only evaluated on the classification tasks, and the performance on larger datasets such as ImageNet is not presented. Since the proposed method is used to interpret pre-trained CNNs, it is necessary to evaluate the proposed method on larger datasets. Besides, it would be great that the proposed method can be applied to other tasks, such as detection and segmentation. Conducting the proposed method on multiple backbones does not indicate that this method can generalize to other tasks. Finally, the proposed method is not fairly compared. I think the interpretable CNN is an important baseline of this paper and it should be compared directly.\n4.\tThe authors claim that “People usually have to trade-off between the network interpretability and the performance in real applications” in Sec 1 as a drawback of previous methods. However, the proposed method also makes a trade-off as indicated in Sec. 3.3. The authors should make the paper logically consistent.\n\nBesides, there are several minor issues about this paper. For example, several arguments in the introduction, such as “become a new demand for CNNs”, need further support, such as citations. In Fig. 1, the images used here for different methods are different. The priority of the proposed method is not clear whether it is because the baselines adopt harder examples or the proposed method performs better. Some typos should be addressed, such as the double “equation” in the second paragraph of Sec. 3.3.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper introduces a method to “diagnose”  a pretrained neural network, which is able to roughly quantify neural activations corresponding to object parts without part annotation.  An autoencoder is trained to reconstruct the features of intermediate layers of the pretrained neural network, where two tracks are introduced: an interpretable track contains disentangled filters for parts and an ordinary track produces the residual information (i.e. part-irrelevant features). The outputs of both tracks are added with a learnable weight $p$ to reconstruct the features of the original network. $p$ is claimed to measure the radio of parts’ information in the features. \n\nPros:\n\nThe paper is clearly written overall with helpful schematic illustrations\n\nThe work is a good attempt to quantitatively analyze the interpretablity of several common CNNs.\n\nCons:\n \n1) Lack of novelty: The autoencoder as the main contribution is greatly based on previous works about  interpretable neural networks.  For example, mask layer is proposed by (Zhang et al. 2018c) and the loss of filter interpretability Loss_f is proposed by  (Zhang et al. 2019). \n\n2) Lack of comparisons with previous methods: The claim that the proposed method has a higher discrimination power than interpretable models is not supported or verified by experiments. Since the work is highly relevant to (Zhang et al., 2018c) and (Zhang et al., 2019), it’s disappointing not to show any comparison with them in the experiment section. Since the location instability and classification error, representing the interpretability and discrimination power respectively, can be calculated by both the proposed method and the interpretable model, I will expect the detailed comparison w.r.t these two terms.   \n\n3) $p$ is the ratio of information through the interpretable track, and thus I can consider p in the interpretable models is implicitly set to 1.  It would be more convincing to show the proposed method is superior to the interpretable model when $p$ is set to 1. \n\n4) Since there is a tradeoff between the model interpretability and classification error,  it would be better to plot a curve by tuning the corresponding hyper-parameters and provide some analyses. \n\nIn summary, although this paper studies an interesting problem of interpreting pretrained neural networks , the technical novelty is limited, and comparisons with existing methods and sufficient ablation studies are missing."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposes an unsupervised method to disentangle and quantify the knowledge of object parts used for inference by CNN. In order to achieve that, they propose to introduce an explainer network, which learns to quantitatively disentangle object-part features from intermediate layers and use the part features to reconstruct CNN features. Their main contribution is providing a simple yet effective strategy to diagnose pre-trained neural networks, which trains an extra explain network without any annotations of object parts or textures for supervision. They perform both qualitative and quantitative experiments to conclude the superior performance of the proposed strategy in boosting feature interpretability. \n\nOverall, the method proposed in this paper could make general and practical contribution, with several caveat for some clarifications. Given some responses, the work would appear more complete, and I would be willing to increase the score.\n\n1. It would have been more persuasive if comparisons on interpretability with other methods were more explicitly shown.\n\nMinor Comments:\nPage 1, In the second Line on the second paragraph in Introduction part, there seems to be a typo of “object objects”, which in my understanding should be “object parts”.\n"
        }
    ]
}