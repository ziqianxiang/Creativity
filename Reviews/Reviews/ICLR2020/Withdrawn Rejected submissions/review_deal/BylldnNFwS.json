{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper studies the decision boundaries of a certain class of\nneural networks (piecewise linear, non-linear activation functions)\nusing tropical geometry, a subfield of algebraic geometry that leverages piece-wise linear structures. \nBuilding on earlier work, such piecewise linear networks are shown to be represented as\na tropical rational function. This characterisation is used to explain different phenomena of neural\nnetwork training, such as the 'lottery ticket hypothesis', network\npruning, and adversarial attacks.\n\nThis paper received mixed reviews, owing to its very specialized area. Whereas R1 championed the submission \nfor its technical novelty, the other reviewers felt the current exposition is too inaccessible and some application areas are not properly addressed. The AC shares these concerns, recommends rejection and strongly encourages the authors to address the reviewers concerns in the next iteration. \n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "This paper proposed a framework based on a mathematical tool of tropical geometry to characterize the decision boundary of neural networks. The analysis is applied to network pruning, lottery ticket hypothesis and adversarial attacks.\n\nI have some questions:\n\nQ1: What benefit does introducing tropical geometry brings in terms of theoretical analysis? Does using tropical geometry give us the theoretical results that traditional analysis can not give us? If so, what is it? I am trying to understand why the authors use this tool. The authors should be explicit in their motivation so that the readers are clear about the contribution of this paper. More specifically, from my perspective, tropical semiring, tropical polynomials and tropical rational functions all can be represented with the standard mathematical tools. Here they are just redefining several concepts.\n\nQ2: In “Experiments on Tropical Pruning”, the authors mentioned “we compare our tropical pruning approach against Class Blind (CB), Class Uniform (CU), and Class Distribution (CD) methods Han et al. (2015)”. What is Class Blind, Class Uniform and Class Distribution? There seems to be an error here “Figure 5 shows the pruning comparison between our tropical approach ...”, i think Figure 5 should be Figure 4. \n\nQ3: In the adversarial attack part, is the authors proposing a new attack method? If so, then the authors should report the test accuracy under attack. Also, the experimental results should not be restricted to MNIST dataset. I am also not sure about the attack settings here, the authors said “Instead of designing a sample noise η such that (x0 + η) belongs to a new decision region, one can instead fix x0 and perturb the network parameters to move the decision boundaries in a way that x0 appears in a new classification region.”. Why use this setting? Are there any intuitions? Since this is different from traditional adversarial attack terminology, the authors should stop using adversarial attacks as in “tropical adversarial attacks” because it is really misleading.\n\n\n\n===================================================================\nThanks the authors for the response. I still have two questions:\n\nQ1: The authors say that this theory provides a deeper understanding to Lottery Ticket Hypothesis (LTH). Then another paper “Rethinking the Value of Network Pruning” [1] suggests something different than LTH. [1] suggests that we do not need the initialization of large networks to train the pruned network from scratch to achieve high accuracy. Since the authors claim that their theory is related to LTH, then what would the proposed theory say about [1]?\n\nQ2: Since you redesign the task of adversarial attacks, I am still not convinced why this setting is interesting? The reason why people are interested in adversarial attacks is because it could happen during test time. What is the application of this setting? Why this new setting is important and worth studying? They are not clear to me. \nAlso, as i wrote in my initial review, “the authors should stop using “adversarial attacks” as in “tropical adversarial attacks” because it is really misleading.”. I hope the authors can address this concern, or otherwise new readers may also find this part difficult to understand.\n\n[1] Rethinking the Value of Network Pruning. Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, Trevor Darrell. ICLR 2019.\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a method to characterize the decision boundary of deep neural networks. The authors take advantage of a new perspective on deep neural network i.e., tropical geometry. They present and prove a theorem connecting the decision boundary of deep neural networks to tropical hyperspheres.  Then, they use the theoretical results of the theorem for two applications i.e., network pruning and adversarial examples generation.  For the former, they show that using the introduced decision boundary characterization using the tropical geometry, one can dimish the number of parameters in a model with insignificant loss in performance. For the latter, they introduce a new method for adversarial examples generation by altering the decision boundary. \n\nOverall, the technical and theoretical contribution of the paper regarding the relation of decision boundary to the tropical geometry is significant and could be useful for further investigation of the decision boundary of deep neural networks. \nHowever, there are several caveats in this paper that need further clarification.\n\n1) This paper needs to be placed properly among several important missing references on the decision boundary of deep neural networks [1][2]. In particular, using introduced tropical geometry perspective, how we can obtain the complexity of the decision boundary of a deep neural network?\n\n2)  The second part of Theorem 2 should be explained straightforwardly and clearly as it plays an important role in the subsequent results and applications. \n\n3) In the tropical network pruning section, the authors mention that \"since fully connected layers in deep neural networks tend to have much higher memory complexity than convolutional layers, we restrict our focus to pruning fully connected layers\". However, convolutional layers of investigated architectures (i.e., VGG16 and AlexNet) have a large number of parameters as well. So, wouldn't it be necessary to investigate pruning convolutional layers as well if diminishing the number of parameters is the main purpose? Moreover, the size of the last fully connected layer is simply determined by the preceding convolutional architecture, which in fact extracts the salient features (at least for well-known image datasets), while the last fully connected layer just flattens the extracted features to be fed into the subsequent classifier. Hence, it would be interesting and, in my opinion, necessary to investigate pruning convolutional layers as well. \nFurthermore, the authors mention that a pruned subnetwork has a similar decision boundary to the original network. What does it mean exactly by \"similar\"? What are the measures capturing this similarity, if any? Also, the authors imply that two networks performing similarly (in terms of the accuracy) on a particular dataset have similar decision boundaries. I am skeptical if this the case and it needs to be validated concretely. \n\n4) In adversarial examples generation, typically for a pre-trained deep neural network model one is interested in generating examples that are misclassified by the model while they resemble real instances. In this setting, we keep the model and thus its decision boundary intact. In this paper, nevertheless, aiming at generating adversarial examples, the decision boundary and thus the (pre-trained) model is altered. By chaining the decision boundary, however, the model's decisions for original real samples might change as well. Therefore, it is not clear to the reviewer how the introduced method is comparable to the well-established adversarial example generation setting.  \n\n5) Two previous papers investigated the decision boundary of the deep neural networks in the presence of adversarial examples [3][4]. Please discuss how the introduced method in this paper is placed among these methods. \n\nMinor comments:\nIn the abstract, \"We utilize this geometric characterization to shed light and new perspective on three tasks\" --> unclear, needs to be revised.\n\nProposition 1, \"the zonotope formed be the line segments\" --> \"the zonotope formed by the line segments\"\n\nPage 7. Results. \"For VGG16, we perform similarly on both SVHN and CIFAR10 CIFAR100.\" --> \"For VGG16, we perform similarly on both SVHN and CIFAR10.\"\n\n\n\n\nReferences:\n[1] @article{li2018decision,\n  title={On the decision boundary of deep neural networks},\n  author={Li, Yu and Richtarik, Peter and Ding, Lizhong and Gao, Xin},\n  journal={arXiv preprint arXiv:1808.05385},\n  year={2018}\n}\n[2] @article{beise2018decision,\n  title={On decision regions of narrow deep neural networks},\n  author={Beise, Hans-Peter and Da Cruz, Steve Dias and Schr{\\\"o}der, Udo},\n  journal={arXiv preprint arXiv:1807.01194},\n  year={2018}\n}\n\n[3] @article{khoury2018geometry,\n  title={On the geometry of adversarial examples},\n  author={Khoury, Marc and Hadfield-Menell, Dylan},\n  journal={arXiv preprint arXiv:1811.00525},\n  year={2018}\n}\n\n[4] @inproceedings{\nhe2018decision,\ntitle={Decision Boundary Analysis of Adversarial Examples},\nauthor={Warren He and Bo Li and Dawn Song},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=BkpiPMbA-},\n}\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "1. Summary of the paper\n\nThis paper describes the decision boundaries of a certain class of\nneural networks (piecewise linear, non-linear activation functions)\nthrough the lens of tropical geometry.\nAn earlier result by Zhang et al. (2018) is extended to multi-class\nclassification problems (technically, only the result for a binary\nclassification is given in the main text, though).\n\nSimilar to this earlier work, the network is shown to be represented as\na tropical rational function. The dual subdivision of this function is\nshown to be represented as the convex hull of two zonotopes.\n\nThis characterisation is used to explain different phenomena of neural\nnetwork training, viz. the 'lottery ticket hypothesis', network\npruning, and adversarial attacks.\n\n2. Summary of the review\n\nThis is a highly interesting paper with a very relevant subject. I think\nthat the perspective of tropical geometry leads to valuable insights. My\nbackground is *not* in tropical geometry, so this paper required several\npasses to fully grasp.\n\nI like the novel insights that this paper creates; it is very interesting\nto observe known phenomena via tropical geometry. I appreciate the\nthorough description of all concepts in this paper. This is to some\nextent both 'boon and bane': on the one hand, the paper contains a lot\nof information and concepts that need to be understood; on the other\nhand, the experiments go *wide* but not *deep*. I suggest to accept the\npaper, but to fully endorse it, I would recommend to work on the\nfollowing issues:\n\n- Clarity & exposition: In some places, the paper could build intuition\n  for non-experts (such as myself) better. This is closely tied to the\n  second point.\n\n- Focus: I would maybe pick *one* or *two* of the experimental areas and\n  use the remaining space to explain all concepts in more detail, build\n  some intuition, and provide a more in-depth setup. Nothing has to\n  removed of course; it could still be put in the appendix.\n\nThis paper has the potential to be a very strong insightful contribution\nto to our community; the authors are to be commended!\n\n3. Detailed comments (clarity)\n\nThe paper describes its concepts well and has a high information\ndensity. At times, there is the risk that readers are provided with too\nmuch information in the main text, leaving the necessary intuition\nsomewhat lacking (unless they are already experts in the subject matter,\nin which case a lot of the information can be skipped).\n\nI realise that writing a paper based on methods that are not yet\nwell-established is no small feat; the authors are to be commended for\nthat!\n\nHere are some suggestions from someone with a background in differential\ntopology:\n\n- The introduction and contributions are somewhat repetitive; I would\n  suggest merging the 'Contributions.' paragraph with the one preceding\n  it\n\n- Even though it *should* be a well-known definition, I would briefly\n  explain that the semiring lacks an additive inverse\n\n- Add an explanation of the tropical quotient to Definition 1; I find\n  the current phrasing of Definition 3 to be confusing at first glance\n\n- What are *upper faces*? Faces with a specific coordinate fixed?\n\n- A definition of $\\pi$ is required. It is my understanding that $\\pi$\n  is a projection function that 'drops' the last coordinate. Is this\n  correct? If so, it should be briefly mentioned on p. 3; else, the\n  discussion about the bias-free case on p. 4 cannot be understood.\n\n- Theorem 2 lacks an 'intuitive' formulation; the results are stated in\n  a terse mathematical fashion, but it would be helpful (also in light\n  of the subsequent discussion) to briefly comment on their *meaning*.\n\n  For example, the first result could be restated as 'the decision\n  boundary is a subset of a tropical hypersurface of the polynomial\n  $R(x)$'.\n\n  The paragraph 'Theorem 2 bridges the gap...' could maybe also be moved\n  to _precede_ the theorem statement.\n\n- The paper refers to $\\mathcal{T}(R(x))$ as a super-set, but in my\n  understanding, it is a *level set* because Definition 4 uses an\n  equality, not an inequality. Am I misunderstanding this? One potential\n  misinterpretation of my part could be that superset refers to the fact\n  that $\\mathcal{B} \\subseteq \\mathcal{T}(R(x))$; so not a superset\n  in the sense of level set analysis, but rather a superset in terms of\n  set theory. If this is the case, maybe rephrase the sentence above to\n  something like 'boundaries $\\mathcal{B}$ through their superset\n  $\\mathcal{T}(R(x))$ according to the first statement of Theorem 2'.\n\n- What does the colour map in Figure 2 depict? The number of iterations?\n  Moreover, I find the polytope though to understand at first glance;\n  how is it related to the decision boundaries that are shown in the\n  leftmost figure?\n\n- I would suggest to place Figure 1 after stating Theorem 2, since it is\n  only referenced later on. Furthermore, the red structures are somewhat\n  confusing. According to Theorem 2, the decision boundary is a subset\n  of the hypersurface, right? What is the relation of the red structures\n  in the convex hull visualisation? The caption states that they are\n  normals, but as far as I can tell, this has not been formalised\n  anywhere in the paper (it is used later on, though).\n\n- To what extent is the existence of the functions described by Theorem 2\n  unique? On p. 5, in Section 4, the paper alludes to *not* using the\n  functional form of the network directly because it does not seem to be\n  unique. I would like this to be explained a in more details, as\n  I found the justification of why the dual subdivision is used quite\n  hard to follow.\n\n- In Section 4, how many experiments of the sort were performed? I find\n  this a highly instructive view so I would love to see more experiments\n  of this sort. Do these claims hold over multiple repetitions and for\n  (slightly) larger architectures as well?\n\n  Please also see my comments on Figure 2 above.\n\n- The claim that orientations are preserved should be formalised.\n  I immediately understand the intuition behind this concept, but\n  if possible, I would like a quantification of this. Might it be\n  possible to *measure* changes in orientation with respect to an\n  original polytope? If so, it should be possible to provide more\n  experiments about these effects and summarise them accordingly.\n  Maybe it would also be interesting to investigate whether other\n  initialisations can be compared in terms of their orientations?\n\n- In Section 5, I would give a brief link to the appendix for the\n  definition of a Minkowski sum.\n\n- Section 5 has (in contrast to the other sections) a lot of details\n  containing the experimental setup, but it is missing a description of\n  the competitor methods. Adding to what I wrote above, I feel that the\n  paper should rather pick *one* area in which experiments are\n  performed; the pruning (together with the lottery ticket hypothesis\n  explanation, which could be seen as a motivating example) strikes me\n  as a good candidate for this.\n\n  I really like this concept of tropical pruning, by the way---it is an\n  elegant, principled description!\n\n- The plots in Figure 4 should summarise multiple pruning runs, if\n  possible. Why not show a standard deviation as well? Given the\n  stochasticity of training, I would think this highly necessary.\n\n- In Section 6, I find the comment on normals generating a superset to\n  the decision boundaries hard to understand.\n\n-  The perspective of perturbing the network such that the decision\n   boundaries change is really interesting, but I am missing\n   a 'take-away message' or a discussion of the insights. Currently,\n   this section seems more like a feasibility study: it appears to be\n   possible to use the tropical description to find new parameters that\n   misclassify a given input. I would propose a discussion of the\n   implications of these findings. \n\n- Concerning future extensions of this work, are there some promising\n  results or directions for CNNs or GCNs? If so, it would strengthen\n  the conclusion if they were mentioned.\n\n4. Minor style issues\n\nThe paper is well-written. I found some minor style issues:\n\n- 'piece-wise' --> 'piecewise' (occur multiple times)\n- 'recently demonstrated' --> 'demonstrated'\n- 'Thereafter, tropical hypersurfaces divide' --> 'Tropical hypersurfaces divide'\n- 'If set $B$ --> 'Letting B'\n- I am not sure if I would call adversarial attacks a 'nuisance'; maybe rather a 'problem'?\n- Use '\\operatorname' or '\\mathrm' to typeset the loss in Eq. 5\n\n5. Update after rebuttal\n\nThe authors addressed all my important comments; their efforts in rewriting and revising the paper in such a short time period are to be commended. I am very happy to raise my score.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}