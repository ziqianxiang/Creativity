{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents an approach to improve DNN robustness to adversarial attacks. The main novelty relies on defining vulnerability in the latent feature space. The algorithm trains with an additional term to promote similar features between an input and its adversarial counterpart.\n\nI like the paper and the idea behind. Here are some comments that would be nice to address.\nMethod:\n- Efficiency-wise, how is this implemented?  If I understand correctly, the training process is less efficient as the features need to be stored and compared, right?\n- Any guarantees to make sure the vulnerability does not reduce the capacity of the model?\n\n- I like the idea of moving the robustness to hidden layers. The paper proposes a metric (the regularizer) to get features closer. While that sounds interesting (I assume resulting features would be agnostic to the input), the evaluation shows that the metric does not correlate with the robustness. \n- I would rephrase all around the objectives for training the model J(\\theta...). \n- I find difficult to follow around Eq. 7\nExperiments:\n\n- The way it is written, is very confusing: ANP leads to 88% reduction in memory while maintaining similar robustness... what about clean accuracy? If clean accuracy is not valid, the entire system is irrelevant.\n- I am not sure how to read the numbers. I can understand some measures are relevant (accuracy to attacks) but at the expense of a huge clean accuracy drop (69% to 57%). I rather would tune the proposal to achieve similar accuracy and then be able to compare just the robustness. \n- In the experimental section, I am missing the size of the resulting network. As there are neurons set to zero, that value is relevant. \n- The contribution of VS seems relevant over AT but marginal on ANP. Please, elaborate. \n- Lack of correlation between vulnerability and robustness in the experiments. Please, elaborate. \n\n\nOther comments:\n\n- I think the overall organization needs improvement. The experimental section has plenty of numbers repeated in different tables and that consumes space and is confusing. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposed an adversarial training method that aims to obtaining sparse and robust networks at the same time. First, the vulnerability is defined at the latent feature space and measured as the expected difference between clean latent feature vectors and adversarial latent feature vectors across all layers. Then this vulnerability measure is incorporated as a regularization and the network is trained in an adversarial training setting. A Bayesian sparsification algorithm is used to prune the network at the same time. The method makes sense to me. However, my main concern is the evaluation. See below for more details.\n\nIt seems no other defense algorithm except AT has been compared with ANP and its variants. Since this paper, to my understanding, is an adversarial defense algorithm, it should be compared to other SOTA defense algorithms.\n\nHow does this method work for different adversarial attacks? ANP is trained under L_inf black and white box attacks (Papernot et al., 2016), and test under the same attacks. What if trained attacks are different to the attacks at the test time. Will the robustness is generalizable across different attacks?\n\nThe 3rd loss in Eq.6: why is the mask only optimized on the loss on the adversarial examples? Why not with the clean example loss?\n\nEq. 1: is that an L1 or L2 norm?\n\nDefinitions 1 and 2 are almost identical with only 1 or 2 different words. Can you combine them to make it more concise?\n\nIn summary, without the thorough evaluations as suggested above, it’s hard to see if the proposed method is competitive or not.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "Adversarial neural pruning\n\nThis paper proposes “adversarial neural pruning” method and vulnerability suppression loss to defend against adversarial attacks. For adversarial neural pruning, the authors train a pruning mask in an adversarial way that benefits both the clean accuracy and the adversarial robustness. The authors also propose a new loss function “vulnerability loss” that measures the robustness of intermediate features in neural networks. By using both techniques, the authors demonstrate through experiments that it is an effective method to defend against adversarial attacks.\n\nI have the following questions:\nQ1: Don’t understand definition 1 and definition 2?\nI don’t quite understand the meaning of definition 1 and 2. It seems useless for the presentation of the paper to me. Can the authors illustrate why you define these two notions here? Is there any usage in this paper for defining (\\epsilon, \\delta)-robust and (\\epsilon, \\delta)-vulnerable?\nQ2: How do you compute the vulnerability of a network?\nThe authors define the vulnerability of a feature in equation (1). However, I can not find how such vulnerability measure is computed exactly. Can the authors specify how they compute this  vulnerability measure?\nQ3: In page 4 “We emphasize that our proposed method can be extended to any existing or new sparsification method in a similar way”\nThis claim is not carefully discussed in this paper. Can your approach be applied to [1]? \nQ4: Title does not properly cover the content of this paper?\nThe author proposed two approaches: adversarial neural pruning and vulnerability loss. However, the title seems to only cover one of them. It is misleading since vulnerability loss seems also to be a major part of the proposed method. Therefore, either the title needs to be changed or the structure of this paper should be altered to highlight adversarial neural pruning approach.\n\n[1] Learning both Weights and Connections for Efficient Neural Networks. Song Han, Jeff Pool, John Tran, William J. Dally, NIPS 2015."
        }
    ]
}