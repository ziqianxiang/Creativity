{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper tries to improve the limitations of previous variational bounds for mutual information estimation, where it uses generalized functions from nonextensive statistical mechanics to get generalized better bounds for the logarithm and the exponential in the partition function. Experiments on simple synthetic data are performed to show the effectiveness, but these experiments are too simple, more thorough experiments on real data sets should be conducted.\n\nThere are some confusions in the writing.\n\nIn Equation (4), the last term is a Monte Carlo sample average by K samples to approximate the partition function Z, so approximate symbol should be used for last term instead of = symbol. Similarly in Equation (5), the last inequality is not really an inequality since this is a Monte Carlo sample average by K samples to approximate the terms in  the previous inequality. This is also true for equation  or inequality of (6), (18)-(20). In fact this is a problem that appears in Poor and Oord's papers. \n\nOn page 2, second line K is the batch size, then the first line after equation (4), K is the number of samples. Please make it consistent. \n\nOn page 3, the first and second paragraph, it is stated \"I_{NWJ} estimator performs well in high dimensional high dependence scenarios but exhibits high variance due to the exponential function in the partition function which amplifies the critic errors. One way to reduce the variance is to make the critic depend on multiple samples and use a Monte Carlo estimator for the partition function.\" Please explain why I_{NWJ} estimator exhibits high variance, is it observed by experiments, or it's been proved mathematically? Similarly why making the critic depend on multiple samples and use a Monte Carlo estimator for the partition function can reduce the variance. I see these kind of statements often appear on papers, I know Poore's paper provide experimental evidence of the validness, but I don't see any mathematical proof.\n\nThe way to cite the references is not right, for example, McAllester & Statos (2018), the correct one is (McAllester & Statos 2018)."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper try to establish novel variational lower bounds for mutual information. Previous variational lower bounds for mutual information either have a large variance (NWJ) or have an upper bound (ICE). The author address the problem by introducing a parameter q and defining q-algebra. The previous variational lower bounds are special case when q approaches to 1. Some experiments were conducted to show that the proposed variational lower bounds have smaller variance and can achieve high values.\n\nIn general, this paper is well-written. The motivation, definition, solution and experiments are well organized to support the claims. However, there are some questions about the paper.\n\n(1) The authors derived the variational lower bound based on q-algebra without any proof. For example, in (13), they simply claimed that D_KL{q} (p(x|y) || \\hat{p}(x|y)) >= 0. Before (16), they simply claimed that log_q Z_q(y) \\leq Z_q(y)/a(y) + \\log_q a(y) - 1, for any x, q, a(y) > 0. This inequalities are well-known without q algebra, but not well-known with q algebra. I think proofs should be included in the appendix.\n\n(2) Some key details of the experiments are missing. For example, what is the value of q in Figure 2? What is the dimension of X in Figure 2? How will Figure 2 change if the value of q and the dimension of X change? Especially, the value of q is the most important parameter which should be well studied in the experiment."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes to estimate Tsallis-q mutual information, rather than plain mutual information, with variational estimators (as have been popular in the past few years). This avoids some of the problems of normal Shannon mutual information estimators, especially that many of these estimators are upper-bounded by $\\log(K)$; the Tsallis version allows for $\\log_q(K)$.\n\nIt seems like a good idea to consider versions of mutual information-like quantities other than classical Shannon mutual information, since (a) most machine learning uses of the MI don't really depend on the particular Shannon mutual information, and (b) it is difficult to estimate. The Tsallis mutual information is certainly a reasonable one to consider.\n\nI have some doubts, though, about the derivations here:$\\DeclareMathOperator{\\E}{\\mathbb{E}}$\n\n- First, in (13), the second line seems to attempt to use $\\E[A \\oplus_q B] = \\E[A] \\oplus_q \\E[B]$. But this is *not true* for $q \\ne 1$. Per (22), it should be \n\n\\begin{align}\n\\E[A \\oplus_q B]\n   &= \\E[A + B + (1-q) A B]\n\\\\&= \\E[A] + \\E[B] + (1-q) \\E[A B]  \\tag{*}\n\\\\&= \\E[A] + \\E[B] + (1-q) \\E[A] \\E[B] - (1-q) \\E[A] \\E[B] + (1-q) \\E[A B]\n\\\\&= \\E[A] \\oplus_q E[B] + (1-q) \\operatorname{Cov}(A, B).\n\\end{align}\n\nBecause we can have q either more or less than 1, clearly the first line of (13) cannot always be $\\ge$ the second line of (13) when A and B have some covariance, which would be necessary for this argument as-is to work. I don't think we can go directly from (*) to show it is greater than $\\E[A]$, for the same reason. So I think a core step of the derivation is simply wrong.\n\nSimilarly, it is not at all clear to me that one can take (19) and then simply post-hoc use different values of $q$ in different components as you do here: the argument used the equality of $q$s. I certainly agree that there is no reason $q_1$ and $q_2$ need to be the same, but you need to ensure that your derivation actually works with these different values of $q$. Otherwise (20) is just a fairly arbitrary formula with no particular relation to $I_q$.\n\nIncidentally, (15) through (20) use $I(X, Y)$ where they should use $I_q$. The equations otherwise imply a relationship of $I_q$ to $I$ that is simply not true.\n\nThis conflation is similar in your experiments, where it seems that you've perhaps picked values of $q$ so that the $I_q$ line approximately agrees with the true $I$ line. But you should compute, or estimate with a thorough Monte Carlo simulation, the true value of $I_q$ and plot that as well. Does that actually agree with $I$ so well here? How sensitive are the results to the choices of $q$? (I see Figure 5 in the appendix does some slight study of this latter issue, but it should have some reference in the text, and you should consider more than one alternative setting of these two parameters.)\n\nMoreover:\n\n- How representative of machine learning problems is this toy example? Certainly we want estimators that can handle high-dimensional Gaussians, but can this estimator deal with very high-dimensional, very structured data that MI estimators are currently being asked to handle (and not doing a great job of)? It's not clear that this experiment gives us much insight into those problems.\n\n- What properties of mutual information does Tsallis entropy satisfy? What does it not, and are they ones we care about in machine learning? It would be useful to have a brief overview in this paper.\n\n\nSome other small comments:\n\n- In (4), as well as (14), you say that an expectation w.r.t. $p(x)$ is exactly equal to a sample average. Unless you're meaning to denote $p(x)$ as an empirical distribution (which would be quite confusing in context), this is not true; I think you should simply change these equal signs to $\\approx$.\n\n- What is the $\\forall x$ doing in the $\\log Z$ bound just above (5)?\n\n- (5) uses an undefined index $j$; you should probably introduce some notation for the shuffled (x, y) pairs.\n\n- The discussion after (6) should probably cite McAllester and Stratos.\n\n- Below (6), you use the physics-style < > notation for expectations one single time in the paper; this should be $\\E$ for consistency.\n\n- It would probably be better to denote the softmax operator $\\sigma$ as $\\sigma_f$, to make explicit that it depends on a function $f$.\n\n- Does I_alpha really train two critics? I thought it simply trained a single critic (with a loss function based on two terms).\n\n- The $\\log Z \\le Z/a + \\log a - 1$ bound is obvious from a Taylor expansion, but where does the $\\log_q$ version come from? You should either prove it in the appendix or give a citation.\n\n\nOverall: given the apparent errors in derivation, my current rating is a Reject. If these can be straightforwardly resolved, and particularly if my other comments can be addressed, I could be convinced to upgrade it â€“ I do think the fundamental idea seems good, it just needs to be executed correctly."
        }
    ]
}