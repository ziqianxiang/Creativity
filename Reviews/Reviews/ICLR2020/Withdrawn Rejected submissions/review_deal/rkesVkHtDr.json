{
    "Decision": {
        "decision": "Reject",
        "comment": "Summary: This paper casts the problem of step-size tuning in the Runge-Kutta method as a meta learning problem. The paper gives a review of the existing approaches to step size control in RK method. Deriving knowledge from these approaches the paper reasons about appropriate features and loss functions to use in the meta learning update. The paper shows that the proposed approach is able to generalize sufficiently enough to obtain better performance than a baseline. \n\n\nThe paper was lacking in advocates for its merits, and needs better comparisons with other baselines before it is ready to be published.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "The paper proposes to learn the step size for a Runge-Kutta numerical integrator for solving ordinary differential equations initial value problems. The authors frame the stepsize control problem as a learning problem, based on different performance measures, on ODE dependent inputs and on a LSTM for predicting the next step coefficient. Experiments are performed on 3 ODEs by training and testing in different contexts.\nThe problem of designing adaptive controllers for ODE numerical schemes is interesting and is probably a new application for ML. The paper makes an effort to introduce the necessary background and for reviewing some classical adaptive controller techniques. The description of the method is relatively clear, but could however be largely improved in order to make it more accessible to the audience.  Many of the arguments and proposed ideas come without justification, some definitions should be made more precise. The construction of the training and test sets should be better explained. The experiments show that the proposed approach leads to fewer evaluations but larger mean errors.  The graphics also show that the local error is smaller for the proposed method than for the baselines which is in contradiction with the global error behavior. This should be clarified â€“ the relations between the two error types should be made clear.  The baseline is not defined in the text so that it is difficult to judge the performance. Why not comparing to several adaptive baselines?\n\nIn conclusion, this is an interesting topic, the paper proposes new ideas. A more careful writing and especially a better comparison with sota baselines would greatly improve the paper. \n\n\n------ post rebuttal -----\nThanks for the answers. I still think that the ideas are interesting but that the experiments  do not demonstrate enough of the proposed method. I will keep my score.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "Updated review: Thanks to the authors for their response to my comments. I believe the strong point of this paper is the novel idea, however, I find the justification for that idea incomplete as author's seems to suggest that the proposed method is probably computationally more expensive (which is opposite to the original motivation of the paper).\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nSummary: This paper casts the problem of step-size tuning in the Runge-Kutta method as a meta learning problem. The paper gives a review of the existing approaches to step size control in RK method. Deriving knowledge from these approaches the paper reasons about appropriate features and loss functions to use in the meta learning update. The paper shows that the proposed approach is able to generalize sufficiently enough to obtain better performance than a baseline. \n\nI think this paper, in general, is clear and well-written. I believe the idea of the paper is interesting too. \n\nThe paper argues that the main challenge of solving the step size control problem for the RK method is balancing the computation vs accuracy trade-off. Existing methods tackle this problem in different ways and this paper proposes to solve it via meta-learning. However, the paper does not mention how and why meta-learning is expected to tackle this challenge?\nSo a couple of comments on what set of problems do we expect meta-learning to better tackle this trade-off than the existing methods would have been useful. I am wondering if it is even possible to say something about this in principle? \n\nThe paper argues that the idea behind using meta-learning is to learn behaviour from a given class of problems and then generalize to new unseen problems (from the same or different classes). \nHow do we know that these problems are even from same distributions? \nWon't the proposed approach fail spectacularly when the problems are not from the same distribution? It would have been nice if the paper made this distinction even if empirically. \n\nIn the experiments section, I could not find/understand what exactly is the baseline the paper is comparing to. \n\nI was more interested in a study that compared the performance of MLRK as the number of instances of the training problems are varied. \nThis again makes me come back to the original point of computational cost vs accuracy. What is the computational cost of collecting data on 30000 instances of problems? Should we not worry about this cost?\nAlso, what is the computational cost of the proposed approach and why are we not comparing it to existing approaches/baseline?\n\nminor comments: \nwhat is tol? it tol the same tolerance as lambda.\n \n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "## Summary ##\n\nThe authors present a method for learning a step-size adaptation strategy for Runge-Kutta methods. The principal contributions of the paper are:\n\n1. They define a loss function that better captures global performance of the controller, rather than just local behavior.\n2. They propose a set of input features to a step size controller. It includes the intermediate Runge-Kutta evaluation values, which allow the controller to approximate the derivatives of the Jacobian function $g$.\n3. They describe a recurrent architecture for adapting step size over time.\n\nRunge-Kutta methods are a workhorse of ordinary differential equations and choosing step size is one of the central challenges involved in their application. Better methods for step size selection would definitely be of broad interest. As the authors point out, existing methods often consist of hand-tuned heuristics---a feature that often suggests machine learning could provide significant improvements.\n\nWhile the premise of the paper is very promising, I don't think it is ready to be accepted to ICLR at this time. Most significantly, the experimental results are not particularly compelling. I believe the authors should refine their method, aim for better experimental results and resubmit. I have included more specific comments below.\n\n## Specific Comments ##\n\n1. I think the paper would benefit from a clearer description of the RK step-size selection problem. For instance, for a p^th order RK solver, at each time step,\n\n   * Inputs: t, y(t), g  # Also possibly intermediate values from previous time steps.\n   * Select a step size h(t)\n   * Evaluate g at p different points based on h(t).\n   * Use these evaluations to compute a value of y_(t + h_t)\n\n   For those that aren't familiar with these methods (at ICLR there will be many!) I think this would help explain where the authors' method (and the other methods your describe) fits into the larger algorithm.\n\n2. I think a short explanation of error estimation would be helpful in addition to the reference to Butcher. This estimation is critical to step size adaptation. In particular I think this would be clearer if the authors expanded the paragraph at the bottom of page 4, where they describe error in a polynomial in (t_n, h) whose coefficients are derivatives of g.\n\n3.  The authors' proposed loss function (Eq. 5) includes the true value of y at t_n, y(t_n). They acknowledge that this may make it computationally prohibitive, but I think this point warrants further discussion. Does this mean that their loss function can only be used on problems for which we have a closed form solution (such as harmonic oscillators)? I noticed that in their van der Pol experiments, the authors switch to a more standard loss (Eq. 6). Is this because Eq. 5 is intractable in this example? Is there a reasonable approximation to Eq. 5 that could be used when a closed form solution is not known?\n\n4. There are a number of issues with the experiments that I think could use clarification or improvement:\n\n  (a) The authors compare to a 'baseline' but I don't believe this baseline is defined anywhere. Is it one of the adaptation methods described in Section 2?\n\n  (b) In Table 1, the baseline method achieves lower error, while MLRK uses fewer steps. It is difficult to assess if this is an improvement since this this cost-accuracy tradeoff is at the heart of the Lagrangian formulation. Ideally, shouldn't we be able to adjust 'tol' to trace out some Pareto frontier for the cost-accuracy tradeoff? In this case, wouldn't we hope for MLRK to be able to achieve better accuracy given the same computational budget?\n\n  (c) In the van der Pol experiments, the authors switch to local L1 loss (Eq. 6). Was the intention to experiment with local L1 loss and van der Pol provided an interesting class of examples? Or was the reasoning that Eq. 5 is intractable for van der Pol so they had to use local L1 loss? If Eq. 5 can still be evaluated on these experiments, this might be a more convincing comparison.\n\n  (d) The number of steps required was provided for the harmonic oscillator experiments, but not the van der Pol ones. This would be helpful for comparing the methods.\n\n  (e) What is the computational overhead of running an RNN alongside your solver? Although it doesn't tell the whole story, it would be informative to report wall clock time along with the number of steps required for each method."
        }
    ]
}