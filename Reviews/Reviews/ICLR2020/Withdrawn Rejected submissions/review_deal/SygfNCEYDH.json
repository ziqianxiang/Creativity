{
    "Decision": {
        "decision": "Reject",
        "comment": "Thanks for your detailed feedback to the reviewers, which clarified us a lot in many respects.\nThis paper potentially discusses an interesting problem, and the concern raised by Review #2 was addressed in the revised paper.\nHowever,  given the  high competition at ICLR2020, this paper is unfortunately below the bar.\nWe hope that the reviewers' comments are useful for improving the paper for potential future publication.\n\nThe ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors propose a formulation to align entities and relation across different knowledge-basis. Each entity (or relation) is associated with a distribution over the entities (or relations) in the other KG through an exponential kernel-density-like-estimate. \n\n\nThe main contribution seems to be the use of GAN to generate good negative triplets instead of the traditional search-for-hard-negatives in constrained problems. This is a novel contribution and I see the value of this line of work.\n\nThe reason why I am tending not to accept this work is because it very similar(/same) to existing work like KBGAN (https://www.aclweb.org/anthology/N18-1133/). Most of the key ideas have already been covered there, and I would like to see a comparison of this work with that before acceptance.\n\n\nnits: \nsome column names in the table are inconsistent - hit vs h, mrr vs mr.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose an unsupervised knowledge graph alignment framework, KAGAN, using adversarial training. Specifically, they utilize a triple discriminator to discriminate the aligned source triples and target triples, and a reward function to minimize the divergence between source triples and target triples. Also, they propose to leverage the lower bound of the mean KL divergence (mutual information) to resolve the mode collapse problem. The proposed method can be incorporated with a supervised method to be a weakly-supervised approach. Even though there are a family of unsupervised approaches for domain alignment, this paper is the first to solve the knowledge graph alignment problem in an unsupervised/weakly supervised way.\n\nStrength:\n1.\tThe paper addresses a critical knowledge graph alignment problem using GAN based on triplets, not usually entity alignment in literature, but also considers the relation alignment in knowledge graph.\n2.\tThe paper tries to solve the problem in an unsupervised way, and shows the on-par performance with weakly supervised methods in FB15k dataset.\n3.\tThe paper considers mode collapse problem and tries to solve the problem via mutual information rather than mean KL divergence, also gives the theoretical proof.\n4.\tDetailed experimental analysis and ablation studies show the effectiveness of the proposed method on small datasets.\n5.\tThe paper is well-written and easy to follow.\n\n\nI have two concerns as follows:\n1.\tThe authors conduct experiments on multiple small KG datasets such as FB15K and WK15K. But the reviewer finds that the baseline papers authors mentioned also have experiments on larger datasets like WK120K (Chen et al., 2017a), WK60k, DBP-WD or DBP YG (Sun et al., 2018a). It is essential to conduct the experiment on larger datasets to verify the effectiveness of the proposed method.\n2.\tIs it necessary to construct a reward function to update the alignment function using REINFORCE algorithm? For instance, current distribution matching methods can define the discrepancy between the two distributions (such as target distribution and aligned source distribution). It can directly optimize the loss in an end-to-end differentiable way instead of a reinforcement learning way. It can avoid the sampling and provide a more stable optimization process.\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The authors aim at learning alignments between knowledge graphs. For this they use a discriminator that functions as an adversary to the parameterized triple alignment function leading to adversarial training. Furthermore, they regularize the training by maximizing a neural mutual information estimator. This requires training another approximating neural network along the way.\nSeveral experiments seem to indicate that their approach improves over competing methods in the unsupervised and weakly-supervised setting."
        }
    ]
}