{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a new decision based attack algorithm. Given a targeted example and original example, the algorithm gradually check samples on this path with some local noise randomly added. The authors show that their approach can get better L_0 perturbation on MNIST and CIFAR-10. \n\nAlthough the algorithm is novel, there are several weaknesses in both algorithm design and experimental evaluation. \n\nExperimental comparisons: \n1. From the tables, the authors show improved L0 perturbation but with much worse L2 perturbation, which was the focus in both Boundary and OPT attack. Therefore it's not very convincing that the proposed method can improve the quality of adversarial example on standard L2 or even Linfty, L1 perturbations. As mentioned in the first paragraph of section 3.2, the authors mentioned that their method can also be applied to L2 perturbation, so a more fair comparison would be trying L2 version of Fussing-based attack and comparing it with Boundary and OPT attacks. \n\nFrom Figure 2 we can clearly observe perturbations, and comparing with the quality we got using L2 perturbation based algorithms (Boundary and OPT attack), I feel the proposed algorithm leads to human-perceptible perturbations. \n\n2. I am quite concern about whether AdvFuzzer is sensitive to the target image, since it only samples points on the path toward one target image, but the goal of either targeted or untargeted attack is not to perturb toward a particular target image, but to find the minimum perturbation that changes label. So, how do you select the target image? Is the algorithm sensitive to this selection?\n\n3. No experiments on ImageNet (note that both Boundary and OPT attacks have been tested on ImageNet). \n\nRegarding the algorithm : \n4. The algorithm might have huge memory requirement since the set Sadv keeps growing. What's the max size of Sadv set in your experiments? "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2545",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes a fuzzing technique to mutate images in a black-box manner for realizing targeted and un-targeted attacks. The authors evaluate the effectiveness of their technique on MNIST and CIFAR-10 dataset.\n\n+The paper is generally well-written and easy to follow. Fuzzing is a well-explored area in Software Engineering and this paper shows the potential of fuzzing for adversarial attacks. \n\n- My main concern of this paper is the results (Section 4). Though for the untargeted attack on  MNIST dataset the proposed technique performs well, its performance is not that satisfactory, especially for CIFAR-10. For an untargeted attack (Table-1) though L0 is significantly low compared to baseline attacks, L2 is increased. This is even worse for Targeted attacks (Table 2)----both reported L0 and L2 loss are greater than C&W L0 attack. \n\n- What is the justification of using Fuzzing 300 and 500 settings, as their number of queries almost match with Opt Attack? \n\n- The evaluation tasks for BlackBox attacks should be performed on black-box scenarios such as google cloud API and Amazon API settings. Instead, the authors use simple white-box settings like mnist, cifar10.\n\n-\t“Note that due to the randomness of our fuzzing attacks, the results could vary in a certain range from run to run.” --- How much is this variation?\n\n- The paper exceeds 8 page page-limit.\n\n- In the methodology section, it is not clear to me how many pixels are mutated per step? It apparently looks like one pixel is selected per step without any guidance. This is almost like a simple brute force approach.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "This paper employed fuzz testing approach to generate black-box adversarial examples. In general, the fuzz testing is new to the adversarial attack area to the best of reviewer's knowledge. One difference with the existing black-box attack generation methods is that AdvFuzzer walks from a source image to a guidance image rather than starting from a targeted image. However, I still have concerns about this submission. \n\n1) Figure 1 is not clear. Please highlight AdvFuzzer and LocalFuzzer and clarify the meaning of the color and symbols.\n\n2) The algorithm presentation is poor. It makes the current technical contributions unclear. What is the challenge when applying fuzz testing to adversarial example generation? And what are technical contributions induced by this new challenge?\n\n3) Some experiments are unclear to me. \n3.1) The lack of ImageNet results is a weak point. \n3.2) What is the computational complexity of the proposed fuzzing-based approach? Having a comparison on computation time might be helpful.\n3.3) The second row of Figure 2 seemingly implies that fuzzing attacks lead to much higher L_inf distortion. Right? Is there any comparison under L_inf  metric? This observation also holds in Table 1 and 2 (small L_0 but large L_2). Thus, I wonder if the reduced query number is at the cost of a significant increase of L_inf norm. For a fair comparison, it might need to add L_inf constraint such that all methods have similar pixel-level perturbation tolerance. E.g., one related work on black-box soft-label/hard-label attack https://arxiv.org/pdf/1907.11684.pdf\n\nThus, my initial rating is weak reject. \n\n############### Post-feedback ############\nThanks for the detailed response. My concerns on 1) and 2) have been addressed. However, my concern on ImageNet results remains. \n\nIt is difficult to get an idea on how well the proposed algorithm performs in the absence of baseline in Table 7  \" Average, Median, and Standard Deviation Results for Untargeted Attacks on ImageNet using Fuzzing attacks\". \n\n\nThus, I keep my score intact. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}