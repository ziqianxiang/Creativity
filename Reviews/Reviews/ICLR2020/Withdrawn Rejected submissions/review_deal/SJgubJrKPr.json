{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposed to augment the existing action set with the ‘macro’ actions that consist of sequence of primitive actions. The framework is based on genetic algorithm (GA) to propose and modify the ‘macro’ actions, where the fitness function in GA is simply the performance of underlying RL algorithm on the newly constructed action set. Experiments on some benchmarks from Atari and VisDoom show some promising results. \n\nOverall the technique proposed is simple and straightforward, which should be easy to understand and implement for the community. In my opinion, the augmented action set forces some form of exploration, and thus would achieve better performance especially in the situations where reward is sparse. There are still some concerns I have:\n\n1. Some claims are too strong. For example, the paper criticizes the previous works with human supervision. However, the GA used here also requires a lot of human heuristics: the design of mutation/augmentation/selection/evaluation, etc. \n\n2. The current specific method only works with discrete actions. How would the method be adapted to continuous case? Actually I’m also concerned about using such mutation based method for exploration, so I’m not fully convinced that this could be generalized to continuous case.\n\n3. It seems that Q+ and Q* are always growing inside the while loop of Algorithm 1. Why there’s no deletion of the actions? Also what would be the trade-off between the size of action space and the performance? I think there must be a limit where afterwards RL becomes harder with large action space.\n\n4. Regarding the experiment, I think the main competitors would be the other exploration based methods, like count based, intrinsic reward based, etc. I’m not asking for comparing against everything, but something like \n1) separate the effort of curiosity with proposed method in Figure 4 (i.e., add a proposed method only baseline); \n2) In fig 2 and 3, add baselines that using A2C/PPO + exploration, e.g., curiosity, count, etc. \nI would expect a stronger statement with these ablation studies.  \n\nMinor:\nThe paper could be more compact. For example, the Algorithm A1 contains only one operation. For better readability, there’s no need to put it into appendix. \n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a generic algorithm for constructing macro actions for deep reinforcement learning. The main idea is to append a macro action to the primitive action space once a time and evaluates whether the augmented action spaces lead to promising performance. The algorithm is tested with two deep RL methods, PPO and A2C. The experiments were conducted in two environments, Atari 2600 and ViZDoom. The benefit of the algorithm is also demonstrated by the transferability of the constructed macro actions on a different DRL algorithm.\n\nThe overall idea of this paper is generally clear to me. But I have concerns about the technical details and writing of the paper.\n\n1. How the “Evaluation” is done is not clear to me (line 12 of the algorithm). Even though this paper focuses on the construction of macro actions, leaving the DRL algorithm as kind of a black box, I still think how the integration and evaluation are done plays a huge role in this kind of problem. Therefore, I think an important technical perspective is missing in this paper.\n\n2. The baselines are way too easy. I am not quite familiar with the literature but I sort of can tell the random, repeated, and naively handcrafted macro actions are way too easy as baselines. What this paper is doing is to have an automatic explore (augment) and evaluate the procedure for generating the macro actions. So the baselines should be at least somewhat similar flavor. \n\n3. The motivation mentioned in the introduction includes avoiding biased macros and also promoting more diverse macro actions. However, it is not clear to me how the proposed method is improving these two perspectives. I think part of the reason is that the notion of ``less biased” or ``diverse” actions are more something good to have or heuristics. Also, the append or alter operators are not exploring the whole combinatorial space.\n\n4. I think it is possible to have a more extended discussion of the related work in this paper. Also, the approach that is taken in this paper -- studying the construction of macro actions independently  -- could be better motivated. At this point, I am actually not sure whether it is appropriate, maybe due to limited discussion of the previous work (only one paragraph in the intro). \n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper describes a framework for macro-action design in deep reinforcement learning. Macro actions consist of open loop sequences of atomic actions. The macro actions are generated by a genetic algorithm that can extend or modify the sequences.  Fitness evaluation of the actions is achieved by training a reinforcement learning policy using an extended action set that includes the newly generated macros. The RL methods and the genetic algorithm do not seem to have been modified in any significant way for integrated application to reinforcement  learning problems. Training of macro actions happens offline before applying RL.\n\nI recommend rejection for this paper. The paper does not seem to introduce any major novelties. It mainly seems to be a straightforward application of a genetic algorithm. The experiments do not offer any major new insights.\n\nDetailed comments:\n\n*The paper claims a number of contributions which result in a framework for macro action construction. This framework seems to be a relatively straightforward application of GAs without large modifications needed for use in RL tasks. I have a hard time seeing how these contributions add to the state-of-the-art:\n\n-The proposed framework  is an SMDP with open-loop macro actions to which any deep RL algorithm can be applied. None of these elements are new.\n\n-Evaluation of macro actions consists of looking at the average reward of a policy  learnt with the macro actions added to the action set\n\n-Action set augmentation just adds macro-actions to the existing action set (just as is done in most temporal abstraction methods)\n\n-The macro action construction method seems to be a standard black, box GA using append-to-sequence and change-element-in-sequence as search operators.\n\n*The lack of proper discounting of macro actions seems rather hacky. One would expect that the next state is discounted by gamma^<macro action steps> rather than simple discounting.\n\n*Training multiple Deep RL algorithms to evaluate multiple generations of action sets seems incredibly expensive. It would be good to indicate the total amount of samples / time used to perform this optimisation.\n\n*The experimental section does not offer much insight beyond the fact that the macro actions can improve performance. Since these are simple open-loop action sequences it seems hardly surprising that they can be used with DRL methods or that they are not specific to a DRL method.\n\n\nMinor comments:\n-I don’t see the need for specifically specifying ‘deep’ reinforcement learning as part of the framework. All of the -definitions seem to apply to general reinforcement learning and are not specific to deep approaches.\n\n-Is section 3 title intentionally ‘generic algorithm’ or was that meant to be ‘genetic’?\n"
        }
    ]
}