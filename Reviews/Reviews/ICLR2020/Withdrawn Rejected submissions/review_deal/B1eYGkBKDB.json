{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposes a quantization strategy for Transformer models and demonstrate results on a neural machine translation benchmark. Although the results look promising, the experiments are a bit sparse in terms of details. In my opinion, an empirical performance-focused paper like this really needs to provide a lot of details. Specific recommendations:\n\n- Table 1 compares BLEU scores across different papers. But is is not clear exactly whether they are comparable. In particular, what is the model size for the model in each paper? It would be insightful to show more details such as model size, compression rate, inference time, etc. \n\n- Table 2: the compression rate is shown as x4. But it would be good to show the actual model size as well. Some parameters, such as the bias terms, are not quantized, correct? \n\n- If you are also arguing that quantization gives inference time improvements, it would be good to show timing numbers too. I would recommend dedicate significant portion of the paper to this. \n\n- The ablation study is good. For empirical papers like this, these are the kind of analyses that will be particularly exciting to the reader in my opinion. So more analysis on different variants and practical strategies will be helpful. \n\n- I would cut all the pruning results (e.g. Sec 4.4). I don't see how that is related to quantization, which is the main contribution of the paper. The pruning results seem to be tacked on and make the paper a bit incoherent. \n\n- Finally, it is not very clear how does the proposed quantization method different from previous methods. The previous methods should be described with respect to your approach so it is easy to understand what is novel. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "\n\nThis paper proposes a method for reducing the required memory space by a quantization technique.\nThis paper specifically focuses on reducing it for the Transformer architecture. \nSeveral similar methods that shared the same motivation have already been proposed in the literature.\nMoreover, the main idea of the proposed method is the uniform quantization method, which has described in the previous study.\nTherefore, unfortunately, the technical novelty seems to be very marginal and narrow.\nIt is hard to say that this paper is innovative or has enough contribution and influence on the community.\nPlease clearly elaborate if the authors think that the proposed method has certain technical novelty, and I missed to recognize it.\n\n\n1,\nHowever, I am a bit surprised by the results of the proposed method; it successfully achieved the on-par result (or even improving) to the standard (non-quantized) models on the well-studied WMT14 EN-FR and WMT14 EN-DE translation tasks.\nAccording to Tables 1 and 2, it reduced the required memory space 1/4 from the original while maintaining any performance degradation, or even it slightly improves the performance.\nIt is good to see such a simple approach offers a better result.\nI think that the experimental results may offer a new insight into the network quantization technique. \n\n2,\nThe paper evaluates the proposed method only on WMT14 En-De and En-Fr benchmark datasets. The effectiveness of the proposed method would be more convincing if it was evaluated on different datasets and tasks as well, e.g. wikitext-103 for language modeling.\n\n3,\nNot much insight and discussion are provided on why the proposed method works better than other baselines. For example, the ablation study offers very interesting observations, but the authors only state the fact, and not providing any hypotheses for further analyzing the reason of working well for the proposed method.\n\n4,\nOne additional thing that I concern about this paper is the reproducibility of the experiments.\nI could not find a detailed explanation of how the authors obtained the reported results written in the tables.\nIf the results were obtained from a single run for each method with the single random initialization, then it may be a bit unreliable.\nIt is because we often observe more than 1point BLEU difference in the same model configuration except for a random initialization.\nTo clear such suspicion, the authors should report the average BLEU of the several runs or release their code for reproducing their experiments in future validation.\n\n\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "This paper proposes an 8-bit quantization method to quantize the Transformer, which is a popular machine translation model. Uniform min-max quantization is used for computational expensive operations during the inference. The authors also propose to bucket the weights before quantization to reduce quantization error. Experiments are performed on WMT translation tasks.\n\nThe paper is overall easy to follow. One of my main concerns is about the novelty since both the min-max quantization and bucketing the weights before quantization are not new. Another concern is that one important goal of quantization is to speed up inference, however, no inference time is reported in the paper. Can the authors provide the inference time results on some typical cpu/gpus?\n\nFrom Table 4, the proposed quantization has much larger BLEU gain than the base transformer after 100k training steps, than that after full training (Table 2). Does this mean the proposed quantization converges slower at the later stage of training? How many training steps are run to reach the result of the proposed method in Table 1? Can the authors compare the training curve of the base transformer and the quantized one?\n\nFrom Table 4, the earlier the quantization starts, the better the final performance. What about the performance of training from scratch? Will this give better performance?\n\n------------post rebuttal comment-----------\nI thank the authors for their detailed response. However, the concerns on the novelty of the quantization technique and the inference efficiency of the proposed method still remain. Thus I keep my score unchanged.\n-------------------------------------------------------",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}