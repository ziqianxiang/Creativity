{
    "Decision": "",
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The authors propose a method for weight quantization in neural networks, RPR (Random Partition Relaxation) which extends previous approaches for weight quantization. The difference to previous approaches is that RPR iteratively selects a random fraction (the constrained weights) of continuous weights (called relaxed weights) to quantize during optimization and increases this fraction over time until all weights are constrained, while maintaining the ability to use SGD as the main workhorse for optimization.\nThe authors demonstrate cases for constrained weights to binary and tertiary cases while keeping continuous weights for input and output layers.\nDuring experiments the authors show that (a) their method is competitive on most benchmarks with simpler methods but can be beaten in some conditions and (b) RPR yields strong results for the GoogleNet architecture.\nThe paper overall is written concisely and well organized, though it is light on technical formalism and generalizable technical insights.\n\nComments:\n1. How is training evaluated here? Do the authors repeat training K times (over i.e. random seeds)? The results on testing contain no error-bars to evaluate if the experiments were lucky or are reliable. This is particularly important to show the value of an iterative optimization scheme as chosen here since 'lucky' runs can dominate results. I would find this a necessary requirement here to trust the results and to understand how much variance the iterative optimization and the masking process introduce to the training.\n\n2. How robust is the method to the scheme for setting FF (fraction of constrained weights) vs learning rate? How much damage would strange settings do to the model? Is there a reliable heuristic for picking the scheme? There is not nearly enough information on this.\n\nReferences:\nIn ICLR last year, Frankle et al introduced the Lottery ticket hypothesis. In this paper, progressive masking and retraining of NNs is studied as a mechanism for sparsification and masking is heuristically performed by quantizing weights close to 0 (by some threshold) to the value 0.\nIt would be worthwhile for the authors to connect their idea to these observations and to include a discussion of those results, as it seems to indicate that quantization/sparsification/masking over iterations of SGD training share useful dynamics here.\n\nDecision:\nAs the paper stands I find it technically and experimentally not mature enough. The process is certainly interesting, but seems neither empirically nor theoretically studied well enough in the current form."
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The authors propose Random Partition Relaxation (RPR) to quantize the network parameters to binary or ternary value. The key idea of the proposed method is to randomly keep small fractions of full-precision weights during the training of the quantized network. In this way, small fractions of the full-precision weights can compensate for the accuracy loss from the quantization. Experimental results on ImageNet show that the proposed RPR achieves comparable performance. However, the novelty and contribution of this paper are limited. My details comments are as follows.\n\nStrengths:\n1.\tThe authors transform the quantization to a mixed-integer non-linear program (MINLP) problem.\n\n2.\tThe authors propose to solve the MINLP problem through alternating optimization. Specifically, the authors propose Random Partition Relaxation (RPR) to randomly relaxing the quantization constraint on small fractions of the weights.\n\n3.\tExperimental results on ResNet-18 and ResNet-50 show the effectiveness of the proposed method.\n\nQuestions and points needed to be improved:\n1.\tThe novelty and contribution of this paper are limited. The idea of the proposed method is the same as INQ [1]. Both methods split the network into two disjoint groups and only quantize one group of weights. Base on INQ, the authors only make a small improvement. Therefore, the proposed method is incremental and the contribution is limited.\n\n2.\tThe definition of the quantization function is not clear. How to quantize the network parameters into binary or ternary value? \n\n3.\tThe effect of the initialization in Section 3.2 is unclear. No ablative experiments are conducted to show the effectiveness of the initialization.\n\n4.\tThe comparisons in Table 1 are unreasonable. INQ [1] quantizes all layer of the network. However, the proposed RPR does not quantize the first and last layers in the network, which reduces the error brought by quantization. \n\n5.\tThe comparisons in Table 1 are unfair. The performance of the baseline model (Top-1 Error: 69.76%) is better than the competitors (. Hence, directly report the accuracy of the quantized model is unreasonable. More results regarding the accuracy gap are required.\n\n6.\tThe results in Table 1 are not convincing. For ResNet-18 and ResNet-50, ADMM[2] achieves better performance than the proposed RPR. However, for GoogLeNet, the performance of ADMM is worse than the proposed RPR. More explanations on these phenomena are required.\n\n7.\tDoes the initial value of freezing fraction (FF) affect the performance of the proposed mothed? More ablative studies on this term are required.\n\n8.\tSome mentioned methods lack references, such as the straight-through estimator (STE).\n\nMinor issues:\n1.\tIn page 1, “compute cost” should be “computation cost”.\n\nReferences:\n[1]\tAojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights. In Proc. ICLR, 2017.\n[2]\tCong Leng, Zesheng Dou, Hao Li, Shenghuo Zhu, and Rong Jin. Extremely low bit neural network: Squeeze the last bit out with ADMM. In Proc. AAAI, pp. 3466–3473, 2018. ISBN 9781577358008.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Authors propose RPR - a way to randomly partition and quantize weights and train the remaining parameters followed by relaxation in alternate cycles to train quantized models. The authors show the results on latest network architectures on ImageNet dataset - ResNet18, ResNet-50 and GoogLeNet and the results either compare to other quantization methodologies and in the case of GoogLeNet they surpass the state of the art ADMM method by 1.7% top-1 accuracy\n\nThe current experimental section does not have any runtime or model size numbers, it would be great if authors could talk about that and maybe add that information to get a more apples to apples comparison on not just accuracy but other factors.\n\nAlso, while Figure1 is a useful visualization of how weights change in the RPR method what is missing in the paper is a more technical/mathematical explanation of the procedure as well as detailed discussion on the hyper-parameters and how things change if we choose a different ratio for the partitioning. \n\nAs it is, the paper currently is easy to read, seems like a good idea and has good results but I feel it is important to add more detail in both methodology and in experimental results especially have more ablations and understand what is truly happening to the model."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary\nThe paper proposes a new training scheme of optimizing a ternary neural network. In the proposed method, during the training phase, a randomly selected portion is kept with full precision and the rest of the weights are quantized. The two steps are conducted iteratively while in each iteration, the portion of the full precision weights, termed as freezing fraction (FF), is decreased until 100%.\nPros:\n•\tThe proposed method is interesting. Adding a small portion of full precision weights during the quantization could potentially improve the model performance.\nCons:\n•\tThe paper is not well written.\no\tIntroduction does not introduce the proposed method and the contributions. \no\tMany statements in the paper are not supported by any evidence.\n•\tIn the paper, it states that “Complex network compression schemes cannot be applied at this point as decompression is often a lengthy process requiring a lot of energy”. However, this is not supported by any evidence. Indeed, many compression methods do not need to be reconstructed during inference time.\n•\tIn the method section, the paper is claiming that performing GD for one full epoch before advancing to next iteration can speed up the convergence. However, this is not supported by any experiments or theoretical proof.\n•\tIt is not clear about the benefits brought by using FF to quantize the network iteratively. The implementation of the brute force search methods may also improve the model performance since the experiments are all conducted on pertained models. \n•\tIt would be interesting to see some results on 8 bit quantization or any other bit width that the quantization cause no performance drop. My concern is that, since the proposed method using brute force method, larger bit width may potentially take longer time for the optimization based on Eqn. (4). Hence, it would be clear to have some experiments on the optimization process.\n•\tThe improvements brought by the proposed methods are not significant based on the  given experiments result.\n"
        }
    ]
}