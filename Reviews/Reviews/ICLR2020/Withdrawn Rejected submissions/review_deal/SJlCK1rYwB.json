{
    "Decision": "",
    "Reviews": [
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This manuscript introduces a general framework to deal with bias(es) in the datasets. The authors propose a bias-only model designed to capture biases in the data. The authors claim that performance of the bias only branch influences the overall cross-entropy loss which leads to learning bias-free models. \nThe paper is on a very interesting topic, but it lacks proper methodological motivation, discussion, and backing up of the claims. Furthermore, the experiments only show accuracy results, while one expects to see a set of experiments demonstrating how the bias was actually removed from the target learned model. \n\nThere are some major concerns on this paper: \n-\tThe proposed methods all operate on the last loss component on how to combine the loss of the bias branch and the one coming from the actual model. The authors hypothetically mention that with this combination the model will learn to remove the bias-contaminated features from the model side because the bias branch already has them. This is very hypothetical and there is no guarantee to it. There is no regularization to make the model branch avoid the bias-contaminated features. There is absolutely no theoretical nor any experimental studies on this issue in the paper. Models are pretty much prone to cheating. They will cheat if they can. If there is a bias in the dataset, the model will capture it and cheat, even if it leads to redundant information coming from the bias side. \n-\tThe second important shortcoming of the paper is lack of proper experiments to back up the claims in the paper. Experiments only show simple accuracy results with our without bias. But the authors keep claiming that their model learns to remove the bias. If that is the case, correlation analysis between the features learned from the model branch and bias terms/variables is missing. Also, there is no direct comparison on how each of the three proposed losses differ from each other in removing the effects of the bias. \n-\tThe authors show in the experiments that their proposed model is better than the hypothesis-only model. This can only be attributed to the fact that info from the premise and in general the added meta-data help the learning framework to learn better features as it is regularized better (and constrained more). This does not mean that the bias is removed from them. There is no discussion or experiments on this issue. \n\nMinor:\n-\tThe proposed models are all similar in terms of modifying the final loss function of L_C, how the outputs of B and M models are ensembled. It may not be a good idea to keep claiming that several different methods are proposed in this paper for dealing with bias. Nature of all methods is the same.  \n-\tFigure 1 that is the introduced in the introduction and referred to ever after is not clear and self-contained. One cannot understand it until they read until the end of the method section. \n-\tThere is no theoretical analysis or discussion on why the proposed loss functions should be able to remove bias!\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "The paper essentially explores variants of reweighing training samples based on how much they are captured by a bias model (i.e. hypothesis only in NLI). The most effective method, product-of-experts, involves ensemble based training of a bias only model and a full model (i.e. for SNLI, bias = hypothesis only, full = hypothesis+premise).  At test time, the bias model is removed and only the full model is applied. The ideas are simple and largely effective, given that a bias model can be formulated / is known ahead of time. \n\nThe paper has significant overlap with Clark et al. @ EMNLP 2019 ( https://arxiv.org/abs/1909.03683 ) and He et al. (  https://arxiv.org/abs/1908.10763. )  I am unsure what the ICLR policy is on concurrent submission, but this work does expand somewhat on that work, experimentally. If this ICLR submission doesn't fall within the grace period I would likely reduce my score. \n\nThat being said, this work does have a set of experiments different than those produced in concurrent work, showing results on SNLI hard and FEVER. Furthermore, the paper presents a debiased focal loss, which, while not as effective as ensemble based training, is an interesting baseline to consider.  \n\nOverall, the work is good and in an interesting direction, caveat being that several other concurrent works @ EMNLP exist exploring exactly the same idea. \n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper presents techniques  to reduce dataset-specific biases learnt by a model during training.  The overall idea behind all the proposed techniques is to give low weights to \"biased samples\" in the loss function compared to unbiased samples so that the mistakes on unbiased samples contribute more to the loss and thus forces the base model to learn robust or true boundaries. To identify biased examples, authors propose to simultaneously train a biased- model using \"bias-specific\" samples or features. Heuristics to generate such samples or features have been proposed for specific types of problems including fact verification, textual entailment, and syntactic bias. In their experiments, authors used two large-scale NLI datasets and FEVER dataset, and evaluated the proposed techniques' performance on their challenging unbiased evaluation datasets proposed very recently.\n\nMy major concerns about this work are limited applicability of the proposed techniques to certain NLP problems and lack of technical novelty, and therefore I give a weak-reject rating.\n\nStrengths: \n1) The paper is well-written and easy to follow. \n2) Proposed methodologies outperform latest baselines from NLP domain on the unbiased versions of above datasets.\n\nComments: \n1) To my understanding ,the performance of the techniques proposed in this paper rely heavily on the quality of biased features or samples. Such samples are identified based on the performance of bias-only model. The notion of bias-only model  (e.g. claim-only trained model) follows naturally  for the problems and datasets covered in this paper. However, that might be quite challenging to do in a general ML problem setting, which restricts the applicability of this work to specific problems.\n\n2) Technical contribution and novelty of the paper is limited. The proposed techniques are simple variations of existing techniques in the literature. Further, the authors provided multiple techniques to remove dataset biases, but didn't provide any insights on why a particular technique perform better than the others. For example, one of the proposed techniques based on Ruby Variations (RV) consistently underperform the other proposed approach \"Product of Experts\" (POE). Unless authors can provide a technical insight on why POE is better than RV, RV does not seem to add any value to the paper.\n\n3)  How do the proposed techniques compare with the general techniques of improving generalization error? E.g. bagging, regularization, dropout, adding noise in the data, etc. Similarly, to improve performance on harder examples, boosting algorithms are known to be powerful. There is no discussion on why such techniques might not be suitable for the problems discussed in this paper. I am curious as to why such methods are not applicable in the discussed problem settings. \n"
        }
    ]
}