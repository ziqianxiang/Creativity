{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "*Summary of contributions:*\n\nThis paper introduces a simulation framework for robotics learning called Lyceum. They argue it is 10-20x faster than popular existing frameworks.\n\nThey successfully motivate their work with two arguments. First, they claim that sim2real is a successful strategy, but limited by computational resources required to run sufficient numbers of simulation steps. Second, they claim that real-time model predictive control is limited by the failure of existing frameworks to meet latency requirements.\n\nTheir work is well-placed in the context of the robotics learning frameworks such as MuJoCo, OpenAI gym, and dm_control. Lyceum is an alternative to high-level abstractions (such as OpenAI gym and dm_control) that exist on top of physics simulators like MuJoCo. One advantage of the existing high-level abstractions is that there are a number of algorithms already implemented on top of them. Lyceum does already implement several of the most popular algorithms (PPO, natural policy gradient), and supports the ability to implement other algorithms, but not all the common algorithms are implemented on Lyceum yet.\n\nThey give a description of how the Julia programming language is well-suited for this task.\n\nLastly, they show two experiments: Their first set of experiments (Figure 3) aim to show that sampling from Lyceum parallelizes linearly across up to 16 cores, whereas OpenAI Gym and dm_control do not parallelize linearly. Their second set of experiments (Figure 1) aim to show that Lyceum speeds up wall clock time for end-to-end training by 10-20x.\n\n*Decision:*\n\nOverall, this is a well-written paper and the ecosystem itself sounds like an interesting contribution. If this paper were just an announcement of a new simulation framework, I’d be excited to try out the framework. However, to warrant an ICLR paper I think it needs two major changes: (1) more rigorous experiments and (2) for the code to be available during the review process.\n\nI’d encourage the authors to use the review feedback to improve the paper - I’d be happy to give an “accept” score if this feedback is (very) thoroughly addressed.\n\n*Important comments/questions (these do impact the score):*\n1) Regarding experiments in Figure 1 (the end-to-end performance + speed experiments):\n\na) It should be more clear what this figure is saying. First I think you need to establish that your model achieves the same (or comparable) performance in the same number of time steps.\n\nIt would make sense to use a fixed benchmark for when a game is considered “solved” and then report the scalar values for (1) number of simulation steps to reach that threshold and (2) the wall clock time to reach that threshold, for both OpenAI Baselines and your implementation.\n\nI’m eyeballing off your graphs, but it looks like Lyceum might take 10x less wall clock time to run the same number of simulation steps, but it doesn’t look like you’ve achieved a 10x speedup in the time it takes to achieve a certain score. (Swimmer v-2 maybe gets a 10-ish-x speedup to achieve a score of ~75, but Hopper-v2 looks to get a 2-4x speedup of time to achieve a score of ~2000, and Humanoid-v2 doesn’t ever even reach the same performance as OpenAI baseline.)\n\nb) OpenAI Baselines includes two PPO implementations, aptly named PPO1 and PPO2. Specify which implementation you used.\n\nc) Your PPO implementation gets significantly worse performance on Humanoid-v2 than the OpenAI Baselines implementation. There is one sentence on page 9 explaining why this might be the case (“reward and observation scaling, value function gradient clipping, orthonormal paramter initialization, and more”), but the rigorous thing to do would be to improve your PPO implementation to match the OpenAI Baselines performance.\n\nIn particular, because this is the most complex of the three environments you experiment on, the low performance significantly weakens your claims. You are using this experiment to make an argument about the speedup of end-to-end training, but then you qualify the worse performance by saying it doesn’t matter because your sample efficiency is faster and that’s what matters. If your argument is based primarily on the sample efficiency, then there is little value added by this experiment because your other experiment is about the sample efficiency.\n\nd) This experiment would be much stronger if you showed performance results on multiple algorithms (not just PPO), and achieve comparable performance to a high-quality baseline (such as OpenAI Baselines), and use more than one complex environment (such as Humanoid).\n\n2) Regarding experiments in Figure 3 (the sampling efficiency + parallelization experiments):\n\na) Your main argument is that Lyceum achieves a linear speedup in sampling when parallelizing across up to 16 cores, and that OpenAI Gym and dm_control do not. I am convinced that Lyceum does achieve this speedup, but I’m not totally convinced that your parallel implementation of OpenAI Gym and dm_control are optimal, which would weaken the comparison.\n\nYou used the multiprocessing library in Python, but then you say your profiling indicates that it’s an IO-bound task (in which case we wouldn’t expect \na linear speedup using multiprocessing). In addition, you say that Github issues suggest using multiprocessing to parallelize OpenAI gym, but the most relevant Github issue I found (https://github.com/openai/retro/issues/42) suggests using a vectorized environment like SubprocVecEnv to create a separate subprocess for each environment.\n\nb) Smaller note: in Figure 3, it’s confusing to say “ours” when you’re comparing Lyceum to your multiprocessing implementation of gym. Both are technically “yours”, so distinguish them by saying “Lyceum” and “gym + multiprocessing”\n\n3) Exactly which environments are supported (currently) in Lyceum?\n\n4) When will the code, tutorials and demos be available? The link only gives a “Coming Soon!” page. If this were a paper making an algorithmic contribution, I’d be less inclined to reject the paper solely because code wasn’t available yet. However, the contribution of this paper IS its codebase, so the code really should be available during the review process.\n\n*Other comments to improve the paper (I think these should be addressed, but didn’t impact the score):*\nFigure 1: the x- and y-axes should have units.\nFigure 2: the caption is confusing. there is a reference to (top) but the figure is one row containing four images. Rewrite this caption so it is clear what each image is.\nPage 9: paramter -> parameter\n\n*Suggestions (it's up to you if you want to address this):*\nPage 5-6: The full AbstractEnv interface is given, but I think there is some other format that would be a better use of space. One option would be to include an appendix with the full interface, written in the style of documentation (instead of using comments above the signature for each function)."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper introduces a computational ecosystem for robot learning built on Julia. The authors claim to achieve faster learning for various RL algorithms and support real-time model predictive control. \n\nOverall, the paper is well written and the main contribution and the main structure of the ecosystem is clearly described. That this supports parallel computation can be of immense use for the training time is often a caveat in RL algorithms and parallel computation is crucial. The benchmark results reported in Fig. 3 show that sampling throughputs are better in the proposed ecosystems across reported environments and this ecosystem outperforms Gym and DMC in parallel scaling of sampling throughputs. \n\nWhile I am inclined to accept the paper as I believe that it has the potential to offer an origin and efficient ecosystem to work well in real-time MPC and RL algorithms, I think the paper lacks certain details.\n\n1. Fig. 1 shows that the although the time taken by Lyceum is less than that by Gym, the performance of the latter is much better in Humanoid. Although time is a crucial parameter in training, it is the performance that is usually of utmost importance. A more detailed explanation /intuition than given is needed for one to understand the limitation of the proposed ecosystem, which may in turn reflect on possible success or failures in other environments (not reported in the paper).\n\n2. There is hardly any code given in the paper! While I understand that this is beyond the scope due to the limited space, I checked the link given in the abstract and it was not working. It is very hard to draw a critical and analytical conclusion without having a look into the details of implementation.  "
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper introduces a new software system for robot learning. The emphasis of the paper is on scalability. The authors rightly identify that many of today's advances in robot learning are out of reach of many groups due to the sheer scale of the computing infrastructure needed to generate results. The key novelty of the new approach is in switching to the Julia programming language which offers the easy of a high level programming language with the speed of a native language. The authors offer bindings to influential robotic libraries and demo some existing RL algo's in their library.\n\nI have a few concerns with accepting this paper in a conference:\n- The authors don't go into a lot of depth on why existing robotics results require so much resources. The examples they give (lots of cores, lots of GPU's) suggest that not the \"glue\" code is the main bottleneck but rather the float-compute heavy parts. I think the paper would be a lot stronger if the authors could do a detailed analysis of existing RL experiments and highlight which bits of the computation they are focussed on speeding up.\n- Related, the experiment the authors show don't address the problems which they quote in their motivation. A strong paper would motivate the need for a new ecosystem from a set of problems and then show that their new software solves those problems.\n- In the experiment section, I believe the authors should also think more carefully about comparing two pieces of software which are leading to a different result: figure 1 clearly shows that the results of OpenAI baseline and their approach lead to very different outcomes. In the description the authors hypothesize this is likely due to very different implementation (i.e. OpenAI baseline doing gradient clipping and many other things). If so, I don't trust the experiment performance results as such.\n\nThere is an interesting discussion to be had when a paper introducing a new piece of software warrants publication in the conference. I can think of a few reasons\n- A library has been validated through significant adoption (e.g. Tensorflow/PyTorch).\n- A library introduces a whole new paradigm/algorithm to solve an existing ML problem (e.g. low precision backprop).\n- A piece of software solves a practical problem (academic/industry) which democratizes an area (e.g. Dactyl on a moderately sized cluster).\nIn this particular submission, I don't see a key breakthrough of the sort above. The paper is well written and a good description of the new library. At this point, this paper feels more like a whitepaper about the new library than a scientific paper which claims to solve a problem in a new way.\n\nMinor:\nTypo in first sentence of conclusion 'intoducing'"
        }
    ]
}