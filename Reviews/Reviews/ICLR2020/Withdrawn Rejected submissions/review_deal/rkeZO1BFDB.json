{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper introduces a new dataset for vision-language navigation, which they frame as cross-lingual understanding benchmark. They evaluate some pre-existing methods adapted to the cross-lingual VLN problem as baselines.\n\nI personally do no think that this task will help the community make further progress either in cross-lingual understanding or in multimodal learning compared to already existing benchmarks. This is a very specific task, which merges two important problems of natural language understanding but with little motivation as to why the combination of the two makes this problem even more interesting. The cross-lingual understanding and multimodal problem are problems that are already difficult enough and unsolved, that at this point, I do not see the value of combining both as done in this paper.\n\nThe paper also lacks novelty in the methods proposed, as they apply previous approaches from Ganin et al. in their specific setting, which has been used in other bimodal setting such as unsupervised machine translation. In summary, this paper is well written but does not provide enough technical novelty or an interesting enough problem to solve to be accepted at ICLR."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "\n\nPAPER SUMMARY\nVision-and-language navigation (VLN) requires an agent to take actions in realistic environments with an understanding of human language. Recently proposed dataset, Room-to-Room (R2R), is captured from real indoor environments but only English instructions are provided.  In this paper, it proposes a task of XL-VLN (Cross-Lingual Vision-and-Language Navigation). A navigational dataset with the Chinese language is collected to support learn instructions of multiple languages.  Based on this dataset, this paper studies two different learning setups (i.e., zero-shot learning and transfer learning) while the meta-learner and domain adaption methods are used. It shows that all these methods improve the baseline neural agent models.\n\n\nPROS\n1. Multiple Languages in VLN datasets.\nI really agree that studying a single-language dataset would limit the view of VLN research.  Moreover, mature VLN systems would be deployed to multiple countries with different languages in real life. I am glad to see that a new dataset with another language and its preliminary results are provided in this paper.\n\n2. Zero-shot learning setup. \nI appreciate the methods in solving zero-shot learning, which develops symmetric encoders for the two languages. Instead of using a translator to get additional training/test data, the instructions of two languages are feed into the model \nSimultaneously. This approach in enabling zero-shot learning of multiple languages looks novel to me. \n\n2. Well-written.\nThe paper is well-written that I easily get the idea of the paper.\n\n\nCONS\n1. Chinese is the only language.\nAlthough the task is appealing, the novelty of this dataset is moderate since only one extra language is collected. Thus, zero-shot/knowledge-transfer methods developed for this dataset might overfit the specific language pair (i.e., Chinese-English). Only one extra language also disallows the existence of \"validation set\", where the methods are developed/tuned on a specific language pair and tested on an unseen language pair. Overall, the dataset gives a chance to study the VLN task beyond English, but it is far below a \"cross-lingual\" VLN dataset as it claims in the title.\n\n2. The contribution of each component is not solid proved.\nI calculated the contribution (to the unseen results) of each component based on the numbers in the paper:\n\nUnseen                           Accu  SPL\n---------------------------------------------\nBaseline:                        23.2   18.0\n+ Two Language:         +1.0    +1.4\n+ Meta Leaner:             +1.2    +0.6\n+ Txt2img:                     -0.6     +0.5\n+ Domain Adversarial: ???      ??? (I did not find the exact numbers in the paper)\n\nIt seems that each component contributes around 1% to the final model and the overall improvement is around 2%. Based on previous results on English R2R,  around 2% improvement w.r.t the baseline might not be significant enough to show the validity. To show the significance, I prefer to see the stddev of the models of different runs and a bootstrapping test (https://en.wikipedia.org/wiki/Bootstrapping_(statistics)). For now, I am not sure whether the proposed methods \n\n3. Might need more experiments.\nIn Table 1, train w/ MT is trained on ZH MT and test on ZH. An alternative approach is naturally applicable here: trained on EN and test on EN MT. However, I did not locate this result. By the way, the results in Table 4 and Table 5 are indispensable to me, which show the actual contribution of each component. Is there any reason to put them in the Appendix instead of Table 1 (i.e., ablation studies)?\nThe models seem only takes the egocentric view as input where most of the current VLN systems are built on the panoramic views. The paper would be better if the experiments on panoramic views are conducted.\n\n\nTECHNICAL CONSIDERATIONS\n1.  In Eqn. (6), the paper mentions that the ReLU could be used as an activation before L_2 norm. It seems that the loss would have a plain minimum 0 if the outputs of  (W h) are all (or mostly) negative. Thus, the ReLU activation seems not to be effective from intuition. Is there any explanation or any references that take this formula? \n\n2. The adversarial domain adaptation in Sec. 4.3 might need some clarification. Suppose the NMT system translates the source domain S to MT target domain T' and target domain T to MT target domain S'.  As mentioned at the beginning of Sec. 4.3., this method tries to align the distribution of S and T' in training, while domains S' and T are used in testing. Aligning S and T' does not naturally guarantee the match of S' and T. Thus, I prefer to see more explanation here.  \n\n3.  What is the difference between the meta-leaner in this paper and the gated-mechanism? Specifically, the gated mechanism uses a learned gate to control the information. It could be scalar (as in this paper or in attention mechanism) or a vector (as in LSTM). To me, meta-learner seems to be too broad and misleading. \n\n4. The notation of the dataset \\{ (E, p_1, x_{1:N}, G \\}^|D'| is a little bit confused to me. Is there any reference material that use the superscript to annotate the cardinality? \n\n\nTYPO\n1. In Eqn. (5), it might be softmax( FC( h_t ) ) instead of softmax( h_t ).\n\n\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "The paper accomplishes several things: they present the first cross-lingual VLN dataset (by collecting new Chinese annotations), they conduct analysis between English and Chinese corpus, they introduce a zero-shot task of cross-lingual vision-language navigation, they propose a visually grounded alignment module, and show the performance of an adversarial domain adaption loss. \n\nAlthough I'm not a domain expert in this area, this seems to be a nice contribution. I like several things about this paper:\n-I think the idea of using images as a pivot for crosslingual grounding is really neat. I'm not a domain expert in computer vision, but this seems to be a really elegant \n-the model performs well, reaching nearly the same level of performance with crosslingual vision-mediated training and training with one language's annotations.\n\nI'm still curious about:\n-is the Chinese attention more accurate because the Chinese utterances were shorter on average?\n-how much do you think the transfer was enabled by the fact that English and Chinese are both analytic languages that are relatively syntactically rigid, particularly in the context of instructions?\n\nThings that need more work:\n-explain this module a bit more:  txt2img module\n\nSmall comments:\n-I assume you used *Mandarin* Chinese; please specify this, as there are many Chinese languages (Mandarin itself has many dialects). You should also state that the instructions are simplified (not traditional)\n-Fig. 2 could have less space between the figures and larger text. The legends in A and B are way too small. It would be prettier too if you remove unnecessary axes and gridlines in C and D. One of the tag labels in D is missing. (I suggest color scheme from \"sns.cubehelix_palette\" that allows for good greyscale and colorblind reading). \n-have you tried a genuine bilingual situation where instructions can be given in either language? That seems to be a natural extension.\n-I'm not sure I understand the relationship between your notation D' and D (is D' the union of all D?)\n-This sentence surprised me: \"nouns and verbs, which often refer to landmarks and actions respectively, are more frequent in Chinese dataset (32.9% and 29.0%) than in English one (24.3% and 13.7%)\": why do you think that is? Do Chinese speakers prefer nouns more? \n-Table 1 should have Stdevs\n-Fix phrasing on: the caption for fig. 6: \"a succeeded instruction\", \"the meta-learner trusts more on the human-annotated Chinese instruction which is of better quality\", \"While the attention on English is more uniform and less accurate than on Chinese.\"\n-typo p 11. \"meta-leaner\"-->\"meta-learner\"\n-please report how much you paid your workers, how many there were, etc. (can be in an appendix)"
        }
    ]
}