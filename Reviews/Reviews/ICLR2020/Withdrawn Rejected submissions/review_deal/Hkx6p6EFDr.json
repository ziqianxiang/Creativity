{
    "Decision": {
        "decision": "Reject",
        "comment": "This paper defines a parameter-tying scheme for a general feed-forward network with the equivalence properties of relational data. Most reviewers raised a few concerns around the experiments, baselines, datasets used and motivation. A few pointed out that the paper is hard to read - for a person without heavy database theory literature, which includes most of ICLR readers. While this paper may read well for the folks in the domain, authors should consider revising the paper to be more inclusive so that it can be read more widely. The motivation of the problem was also another point that many reviewers have mentioned (perhaps related to the language issues above) that some noted that you may not always want equivariance in relational DB and other noted that the paper did not sufficiently demonstrate the advantage of the proposed methods. Reviewers also univocally commented on experiments - many voiced the lack of baselines (not even any simple one). Authors wrote back to defend that there is no similar method and even simple tensor factorization isn’t applicable. That makes me wonder - is there really no single simple method you can compare with? If nobody had solution for this problem, is this a problem worth solving? Reviewers also encouraged to use larger (beyond Kaggle dataset) real-world datasets to strengthen the paper. All the points raised by reviewers suggests that this paper can benefit from another round of nontrivial editing before it’s ready for the show.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper introduces a parameter tying scheme to express permutation equivariance in entity-relationship networks that are ubiquitous in relational databases. Results on the generality of the tying scheme are presented, as well as some experimental validation.\n\nThe paper extends several previously-proposed lines of work in statistical relational learning, including the work of Hartford et al. (2018) to multiple relations. In contrast to this earlier paper, which builds its argument step by step with great clarity (helped by illustrations), the current paper would prove hard to read to an ICLR audience, since most of its language and methods borrow more from the database theory literature. Since the proposed tying scheme is quite close to that of Hartford et al. (2018), one way to present the paper could be to construct the argument in such a way as to emphasize the difference with the previously-proposed scheme, thereby far better outlining the current contribution.\n\nIn addition, the experimental validation leaves much to be desired. In the synthetic data experiments (e.g. Figure 3), there is no guidance as to how to judge the quality of the resulting embedding. Also, it would be helpful to contrast the performance of the proposed approach against alternatives on a wider variety of datasets, such as MovieLens, Flixster, Netflix. (See the methodology followed by Hartford et al., 2018).\n\nAll in all, even though the proposed approach could usefully extend the state of the literature, the paper in its current form would need additional work before recommending acceptance at ICLR.\n"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper defines a parameter-tying scheme for a general feed-forward network which respects the equivalence properties of a vector encoding of relational (ie database) data. As long as this parameter-tying is observed, the resultant feed-forward network is guaranteed to respect the equivariant properties of this data, meaning entity indexes can be permuted, and the output produced by the layer will be permuted in the correct manner. This results in a maximally expressive feed forward layer for this kind of data.\n\nThis is a very good paper and should be accepted. The approach is convincing, and provides a clear way forward for representing an important class of data. The paper is surprisingly well-written and comprehensible given the complexity of the math. I learned a lot from reading this paper.\n\nThe experiments are somewhat rudimentary, but they are perfectly adequate for a paper whose contribution is primarily methodological and theoretical.\n\nOne suggestion I have is that some of the details from the Appendix should be moved into the paper proper, most notably the details of the experiments in Appendix H."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #5",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper proposes Equivariant Entity-Relationship Networks, the class of parameter-sharing neural networks derived from the entity-relationship model.\n\nStrengths of the paper:\n1. The paper is well-written and well-structured.\n2. Representative examples, e.g., the Entity-Relationship diagram in Figure 1, are used to demonstrate the proposed algorithms.\n3. Detailed proofs for some equations are provided for better understanding the proposed equivariant entity-relationship networks.\n\nWeaknesses of the papers:\n1. No effective baselines are used for comparisons in the experiments. Are there state-of-the-art algorithms that have been proposed by other researchers to be used as baselines in the experiments?\n2. No effective real-world datasets are used in the experiments. The authors only take synthesized toy dataset in their experiments. Are there other real-world datasets to be used in the experiments?\n3. In terms of missing record prediction, why do the authors embed, e.g., the COURSE, in this way but not the other ways? What are the motivations of embedding like this? Are there other embedding techniques, e.g., Matrix Factorization and Skip-gram frameworks like that in Word2VEC, can be used for your purposes?\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "The authors extend recent work in equivariant set encoding to the setting of entity-relation data.  Similar to the cited previous work, they encode sets of objects (in this case the tuples of a relational database) with a permutation invariant function. They use a parameter tieing scheme to enforce this invariance.\n\nI don’t find the paper to be particularly well motivated. Relational DBs are not necessarily a setting where you would always want equivariance. While the ordering of tuples do not matter, relations are often asymmetrical. Further, the idea of concatenating all tuples of a relational DB to be passed through a particular feed forward layer is infeasible for all but the smallest datasets. Real world databases such as knowledge bases contains millions to billions of entries. Scaling issues aside, the experiments do not actually show that this method outperforms any reasonable baselines such as a simple tensor factorization. \n\nI also found it particularly hard to follow the descriptions of the methods. A few specific points:\n- The text and notation in the beginning of section 2 could be a lot clearer. I had to read these paragraphs multiple times to pick out precisely what you were trying to say. Maybe have the set of relations be [R] like your other sets rather than a different script R. \n- In the next paragraph you say “A particular relation R is a multiset if it contains multiple copies of the same entity” but you previously defined R to be a set of instances and not entities. I think you need to be more consistent with your terminology since the differences between type level entities and instances is important for your definitions and as you noted in your first footnote, these terms are often used very differently.\n- This explanation could be helped a lot by improving the figure. The caption and images are both very dense but still requires a lot of coreference. For example, labeling the entities and relations with their ids in figure 1a directly would make it much easier to mentally map to what you are explaining in the caption. Also figure 1c is not at all clear.\n\nLastly, the methodology seems quite incremental over the previous work. A lot of the context and background that was sent to the appendix should be included in the main paper, particularly the relation to related work\n\n\nedits:\ndouble ‘the’ in abstract “linear complexity in the the data “",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #2",
            "review": "The paper handles the permutation invariance of the entity-relationship data entries by tying the weights of linear parameterization together. Heavy math was used to describe the proposed idea, and experiments were performed on simple problems.\n\nThe main idea of the paper seems to be equation (3) and the associated method of tying weights in the linear layer. This is actually quite simple, and does not require the numerous complicated definitions in section 2 and 3 to reach the same conclusion. Using math is good, but using more than necessary simply slows down the dissemination of ideas.\n\nIn both theory and experiment, the paper does not demonstrate the advantage of the weight tying method compared to an untied baseline. This weakens its necessity and motivation.\n\nThis paper also has very weak relationship with deep neural networks, while the word 'deep learning' was mentioned in both introduction and conclusion.\n\nIn general, it is a weakly motivated and math-excessive paper. Based on these considerations, I recommend rejection.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}