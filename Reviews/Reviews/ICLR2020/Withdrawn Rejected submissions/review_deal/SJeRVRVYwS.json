{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper proposes a novel dataset, which summarizes a scientific paper into a corresponding press release (i.e., Science Daily), for textual summarization task. The proposed transfer learning setup improves an automatic evaluation metric (i.e., ROUGE) as well as a newly proposed automatic metric for checking the factuality of the summarization model. \n\nI really appreciate the dataset itself and the way they introduce the problem (e.g., factuality, language style difference). The dataset would be a valuable dataset for abstractive summarization. However, the way that they validate the hypotheses is not convincing, and most of the empirical validation is not reasonable to me. Please find more details below. \n\nOverall, the paper is easy to follow except for the RA evaluation part. In terms of presentation, it would be better to follow by adding some real examples of the Science Daily text. Readers may be interested in seeing how the press release is different from other summaries. \n\n\nMy major concern is the lack of scientific/linguistic consideration of the task. First, it would be better to highlight the major difference of the task compared to existing textual summarization and tackle some of them in your model design. For example, the text used in press releases will be much easier words for public readers as described in the paper. Then, how does your model address this issue? If the summarizing the papers to press release is different from summarizing the papers to abstract or title of the paper, then how does your model address this? For the issue of factuality (which I don’t get convinced much though), which parts of transfer learning do you think it is tackled by? Given the ROUGE and RA scores, I don’t find any evidence of how factualities are improved by the model you proposed.\n\nSecond, textual summarization is not always “neural” textual summarization. If your focus is only the “neural” textual summarization, please clarify how the “neural” component makes it different from general textual summarization. Otherwise, I highly encourage authors to do more literature survey on textual summarization, scientific paper summarization, textual abstraction, and more. Summarization of scientific papers has been actively studied for many years, in various ACL venues (e.g., ACL, NAACL, EACL, EMNLP). Some works focus on the content of papers, while others focus on the meta-information of papers such as citation networks. Here are some of them:\n\nCoherent citation-based summarization of scientific papers. ACL 2011\nA supervised approach to extractive summarisation of scientific papers. CoNLL 2017\nScisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. AAAI 2019.\n\nIf you treat textual summarization as a machine learning problem, it is fine. But, please understand the nature of the problem first and then design your model accordingly. \n\nAnother weakness of this work is the lack of novelty in the proposed methods. The only contribution made in the proposed model is adding the separation tokens to distinguish the source and target, and two different datasets. First, I don’t think the hypothesis made by authors is correct: encoders and decoders will focus on the tokens they specified and distinguish them accordingly. This may or may not be true but there is no evidence provided in the experiment. Second, another hypothesis in the PART model does not make sense at all: dividing press releases into three equal parts (i.e., highlights, body, conclusion). This is the commonly believed writing scheme in the press release, but how the equally divided pieces of text could be corresponding to each category? It would be nice to annotate some of the press release text and validate your hypothesis. \n\n\nIn Tables 3 and 4, why do think PART outperforms the BASE (or AB)? Is that because of the separation tokens? If so, it would be better to add another simple baseline to do the exact operation in a different way. For example, making two objectives (i.e., multi-task learning) for both the press release and paper abstract but sharing the internal representations of encoder and decoders might be one way. Basic separation of a long document by tokens is not multi-task learning. \n\nIn RA evaluation, the description of the current human evaluation on summarization is moved to Section D in Appendix. But, why? For me, that is the most important motivation and background of the RA method. But, once I read the Appendix, I still don’t get convinced of the necessity of RA evaluation and the reason why the existing human evaluation is not feasible for your task. Similarly, there is no qualitative analysis (i.e., human evaluation) on the output summaries. Also, please consider moving a few output text into the main paper for better understanding. By the way, you use the acronym RA in the title of the section but the full name is firstly introduced at the end of the whole section. \n\nThe scores of RA in Tables 5 and 6 and the partial agreement of RA scores with ROUGE scores are not convincing at all to support the conjecture “RA could be a good measure for the generalizability of transfer learning experiments in summarization”. \n\nThe points discussed in the Discussion section sound reasonable and helpful but any shreds of evidence are not given, especially about the conceptual/logical accuracy.\n\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "1: Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper presents experiments on neural summarization of text, using two types of sequence-to-sequence models, on a corpus that has been created from both an existing data-set, and a newly created data-set. The authors propose a new metric to measure the \"factuality\" of the generated summaries, which the proposed approach improves.\n\nI find the paper very confusing to read, and the experiments unconvincing. My main issues are:\n- The description of \"Our SD dataset\" is concerning: \"We do not perform explicit pre-processing, and thus the papers do not follow any standard format ...\" - this makes me wonder if the data can be trusted, please describe this more carefully. Can you make the data available so that other researchers can reproduce your results?\n- You claim to achieve \"sizable improvements over a strong baseline\": I have no idea how well a strong baseline on this dataset should perform because there are no reference points of your implementation, data preparation, and choice of hyper parameters on other well-understood datasets or tasks, as far as I can tell.\n- I find the description of the experiments confusing: you claim to use multi-task training, transfer learning, as well as co-training - but from what I can tell, the two methods are essentially the \"GROVER\" method with extra text markers, and an auto-regressive application of a seq2seq model (which is auto-regressive itself in the case of the STORY model)\n- There is a concern about novelty: \"unlike these techniques, our task here is automating science journalism\" - this in itself is probably not a contribution, however the \"RA\" metric could be an important and novel contribution\n- I must admit I am not sure I understand the RA measure from your description. Could you at least describe what the range and ideal value for RA is? Is it 0-100 with 100 being best? Because it is a fraction that you multiply by 100 to get \"points\" or scores? Where do you get the p_hat from in the evaluation? Do you measure them \"empirically\" by counting? Or do you get them from the trained model? \n- It is good to show example, in this case I think it would be better to show shorter excerpts and more conditions, and explain exactly what the reader is supposed to see from each example.\n"
        },
        {
            "rating": "1: Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1093",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "<Strengths> \n+ This paper proposes a novel task named automatic science journalism to produce a press release from a scientific paper. Its specialty lies in that it requires summarization of a long text to another long text with extreme paraphrasing. \n+ It is a clever idea to couple a scientific paper with its corresponding news article to build a new large-scale dataset. \n+ The paper reads very well. \n\n<Weakness>\n1. This paper has no technical novelty. \n- It simply applies two existing models to new corpora, including FCONV as a vanilla CONV seq2seq model and STORY as  a SOTA neural story generation model.\n\n2. Experiments are done in a preliminary way. \n- Although this paper ambitiously proposes the automatic science journalism as a new challenging summarization task, what is actually done is nothing but primitive experiments about basic transfer learning. For example, the paper simply measures the ROUGE scores between three different configurations of the dataset: BASE, AB and PART.\n- The proposed evaluation metric is reasonable defined based on classification probability. However, it is far from saying that it can measure the actual target factuality, which is abstract and hard to be gauged even by human. \n\n<Minor comments>\n- The survey for neural text summarization is too short and cursory in Section 2. While it is an active research area in NLP, only three papers are referred. \n\n\n<Conclusion>\nAlthough this paper has a strong potential to be a good piece of work in terms of a new application and novel datasets. However, it is still largely incomplete and there are big gaps between what is aimed to do and what is actually done; as a result, it may not be yet ready for publication and have much to be done, in my opinion. \n\n"
        }
    ]
}