{
    "Decision": "",
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper addresses empirical study of generalization behavior in gradient-based meta-learning. To this end, authors evaluated: (1) flatness of minima in terms of the spectral norm of Hessian matrix; (2) coherence of adaptation trajectories; (3) the average inner product between meta-test gradient vectors. Finally a regularized MAML is proposed, adding a penalty on angles between inner loop updates. Experiments are shown, measuring three quantities mentioned above. \n\n---Strength---\n- Empirical analysis of various properties of the objective landscape in gradient-based meta-learning is interesting and new.\n\n---Weakness---\n- Recent theoretical work on meta-generalization bound or convergence properties of MAML is available. For instance,\n  [1] M. Khodak (2019), \"Provable guarantees for gradient-based meta-learning,\" ICML.\n  [2] N. Golmant (2018), \"On the convergence of MAML,\" NeurIPS.\n- While empirical study could be interesting, but not much insight was not provided. \n- A regularized MAML is proposed but its effectiveness is not well studied yet.\n\nTo sum up, the paper provides a few interesting empirical results but it is not clear what benefits are gained from these results. "
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary:\nThis paper explores generalization of gradient based meta learning algorithms (MAML in this case). They first look to flatness of minimum as a measure of meta-generalization and find it is not correlated. They then look to similarity of adaption trajectories (3 different signals tested) and find all of these correlate to varying degrees. They then show a meta-regularization strategy designed.\nMotivation:\nI am unclear as to the motivation for all of the similarity metrics. Why do we expect meta-generalization to be correlated with these? If my understanding is correct, these are computed between 2 different tasks. As a result I would not expect there to be any correlation yet there is?! This is puzzling and intriguing but I don't feel this paper illuminates why this is.\nThe general flow of the paper presents a large number of things which imo are distracting. For example the discussion of flatness seems out of place in the context of these trajectory metrics (which occupies most of the paper). There are figures with more than one shot experiments -- e.g.  8ce which doesn't seem to be related to any of the points you are making. Improving the cohesiveness of the work will greatly improve this work.\nExperiments:\nMost of this paper is backed up by experimental evidence. Additionally most of the trends rely on the reader matching two curves -- e.g. figure 7 where the reader is expected to see similarities. As it stands now, these relations are certainly readable but could likely be presented in a much more concise way -- e.g. like figure 8e.\nSecond, considering how important meta-generalization is to this work there are no curves showing training performance anywhere in the main text (though there are a few in the appendix). Showing that a generalization gap exists seems quite important. Infact, showing this gap vs your measurements might be interesting as this seems to be something we care about.\nShowing the fact that a penalty targeting these metrics and that meta-test performance increases as a result is a strong piece of evidence in favor of this work. Expanding on this outside of the toy setting of omniglot would be highly valuable imo. \nRating:\nAs it stands now, I am borderline leaning towards reject. I am willing to change my score however. Improvements in motivation for why these generalization metrics make sense should be considered. I also think the paper can be unified to tell a more compelling story. E.g. distilling the points being made in figures and clarifying the exposition.\n \nOther Suggestions:\nConsider being more careful with the use of generalization. I believe in many places -- e.g. in contributions -- you mean meta-generalization. These two concepts are not the same but are related and this confused me while reading. For example, the flatness section discusses flatness of the minimum post adoption. This is a test of generalization in my mind. Sensitivity of the meta-parameters (learned initialization) involves computing flatness at the test solution as well but is not the same thing and will be computed differently.\nPlease include what the error bars are over in all plots.\nFigure 8 nit: the a, b, c, d labels are extremely small and in an odd place.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "Summary: This paper empirically analyzed the generalization properties in gradient based meta-learning. The authors provided an experimental evidence against correlation between generalization and flat minima in the context of meta-learning. Then they showed the generalization is correlated with the coherence between the adaptation trajectories in the parameter space. Finally they proposed a new regularizer for MAML.\n\nMain comments:\n\n1. This paper aims at understanding the generalization in gradient based meta learning, which is the key to understand learn to learn problem. However, the significance of this paper is not clear, because there are several recent papers starting analyzing the theoretical aspects in gradient-based meta-learning (at least 3 months before ICLR deadline), for example:\n\n(a) Khodak, Mikhail, Maria-Florina Balcan, and Ameet Talwalkar. \"Provable guarantees for gradient-based meta-learning.\" ICML 2019. \nThis paper proved the convergence and transfer-risk in the first order gradient based meta-learning (e.g Reptile) for convex and non-convex loss. Similar to the paper, it also introduced and demonstrated the **importance of the similarity in the parametric space**(e.g Section 2.2) in meta-learning.\n\n(b) Finn, Chelsea, et al. \"Online meta-learning.\" ICML 2019.\nThis paper proved the regret in gradient based meta-learning and empirically validated in many real world problems.\n\n(c) Denevi, Giulia, et al. \"Learning-to-Learn Stochastic Gradient Descent with Biased Regularization.\" ICML 2019.\nThis paper proved the statistical advantage for adopting the meta initialization (reducing the variance in the statistical learning).\n\n(d) Khodak, Mikhail, Maria Florina-Balcan, and Ameet Talwalkar. \"Adaptive Gradient-Based Meta-Learning Methods.\" arXiv preprint arXiv:1906.02717 (2019).\nThis paper extended the theoretical conclusion in (1) into the non-stationary task-environment.\n\nUnfortunately, the paper does not discuss or even mention these related works. I would like to see more theoretical support.  Particularly in section 5.3.1., they proposed a simple regression problem for fitting sine function under l2 loss, implemented by two layer-MLP, I think it is feasible to derive some theoretical results in this paradigm (e,g. Denevi, Giulia, et al [2018]).\n\n2. As for the technical part, the against correlation between generalization and flat minima in the context of MAML should be theoretically verified, at least the author should propose some theoretical/analytical insights. It is not convincing that the paper only empirically verified on the Omniglot and Mini-ImageNet under cross-entropy loss. It will be much better to design more diverse empirical tests (e.g different network architectures and different tasks like RL /NLP and different loss). It is hard to arrive such an important conclusion by testing only two datasets.\n\n\nMinor comments:\n1. Fig. 3 seems over training (support loss keeps stable after several training epoch) and it is natural the target accuracy dropped. why not using the early-stopping strategy?\n2. The regularizer for MAML seems interesting but only tested in Omniglot dataset, I would like to see more empirical results. \n\nOverall, the main claim in the paper “against correlation between generalization and flat minima” is not convincing. I suggest the author think more theoretical supports and discuss the related works, to justify this important conclusion.\n\nReference: \nReptile: On First-Order Meta-Learning Algorithms.  Alex Nichol et.al 2018\nLearning To Learn Around A Common Mean.           Denevi, Giulia, et al  NeurIPS,  2018"
        }
    ]
}