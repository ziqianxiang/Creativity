{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "All three reviewers agree that the paper provide an interesting study on the ability of generative adversarial networks to model geometric transformations and a simple practical approach to how such ability can be improved. Acceptance as a poster is recommended.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "The paper explores and experiments on extrapolating attributes of images produced by GANs by manipulating their representations in latent space. Attribute manipulation is done by predicting latent space walks (linear or non-linear) and is learned in a self-supervised way by using augmented outputs of a pretrained GAN as target images.\nAuthors experimentally show dependence of range of possible attribute manipulations on the diversity of the dataset in terms of that attribute as well as propose techniques to improve it.\n\nSuggested concepts are explained in a clear way with extensive experiments confirming the findings. Techniques proposed for improving \"steerability\" of GANs are backed up by both qualitative and quantitative analysis, although missing experiments on more sophisticated datasets than MNIST.\nOverall,  I recommend to accept this paper.\n\nSeveral questions I would like the authors to address to make some details more clear and the paper more complete:\n1. Why the color distribution of generated images is evaluated on a sampled subset of pixels, not full images? (\"Quantifying steerability\" section.)\n2. On Figure 6, which classes are outlying on transformation limitation / data variability plots (bottom-right corner) and how it may be explained?\n3. While StyleGAN can not preserve geometry of objects for shift in location-based attributes, when walks are learned in the W space, have you experimented on manipulating those attributes with z space? What are the results?\n\nOther minor flaws include\n1. Pictures in Fig. 2 are mixed up between G(z) and G(z + \\alpha w)\n2. In Fig. 2 edit(G(z, \\alpha)) -> edit(G(z), \\alpha))\n3. In eq. (2) f^n(z) -> G(f^n(z))\n4. In eq. (6) +\\alpha^* -> -\\alpha^* ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper propose to study the generalization properties of GANs through interpolation. They first propose to learn a linear (and non-linear) interpolation in the latent space for a specific type of image transformation for example zoom, translation, rotation, luminance, etc... They show that linear interpolation in GANs can produce really realistic images along the path and enable to control and transform generated images to some extent. They then propose to measure to what extent the generated images can be transformed without \"breaking\".  Finally they show that the quality of the interpolation can be improved by learning the interpolation and generator jointly.\n\nI'm in favour of accepting this paper. The paper is well written and organized. The experiments and observations are very interesting and really illustrate the generalization capacity of GANs.\n\nMain argument:\n- I think those observations are very valuable to the community and are a good way to get insight into the capabilities of GANs. This also give interesting informations about the different bias present and learnt in the dataset. This could also lead to very nice applications.\n- The interpolation with StyleGAN and BigGAN seem to give qualitatively very different results. It would have been very interesting to study the quality of interpolations on more models and datasets, and compare their generalization capabilities as well as the bias present in the different datasets.\n- Does training the generator and interpolation jointly improve the quality of the generator in general ? It would have been nice to run this method on more complicated dataset like CIFAR10 and see if this method increase the overall FID score.\n\n\nMinor comments:\n- In appendix A.2 the authors explain how the range of $\\alpha$ is set for the different experiments. However it's not clear how is this range used in practice ? Do you sample uniformly $\\alpha$ in this range to train the linear interpolation ? Also how many steps are required to learn the linear interpolation ? How much the does it influence the quality of the interpolation ?\n- There is a typo in equation 6\n- In figure 6: What does the right figure represent ? especially what are the different colours ?"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This work explores the extent to which the natural image manifold is captured by generative adversarial networks (GANs) by performing walks in the latent space of pretrained models. To perform these walks, a transformation vector is learned by minimizing the distance between transformed images and the corresponding images generated from transformed latent vectors. It is found that when traversing the latent space of the GAN along the direction of the transformation vector, that the corresponding generated images initially exhibit the desired transform (such as zooming or changing X position), but soon reach a limit where further changes in the latent vector do not result in changes to the image. It is observed that this behaviour is likely due to bias in the dataset which the GAN is trained on, and that by exploring the limits of the generator, biases which exist in the original dataset can be revealed. In order to increase the extents to which images can be transformed, it is shown that GANs can be trained with an augmented dataset and using a loss function that encourages transformations to lie along linear paths.\n\nOverall, I would tend towards accepting this paper. Improving the amount of control that we have over generative models is desirable for image synthesis, and this paper does a great job of demonstrating the extent to which these models can be manipulated in terms of mimicking basic transforms. Figures are very clean and informative, and experimental results are extensive. I don't have much else to say about this paper, as I did not find anything in it that concerned me, and the paper answered all of my questions."
        }
    ]
}