{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper implements a novel architecture for inferring loop invariants in verification (though the paper bridges to compilers).  The idea is novel and the paper is well executed.  It is not the usual topic for ICLR, but not presents an important application of deep learning done well, and it has interesting implications for program synthesis.  Therefore, I recommend acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "Summary:\n\nThis paper introduces a novel way to find loop invariants for a\nprogram. The loop invariants are expressed as SMT formulas. A\ncontinuous relaxation of an SMT solver is introduces mapping\nevery SMT formula onto a truth value \\in [0, 1]. This relaxation\ncalled a continuous semantic mapping is decided such that every\ntrue formula has a higher value than every false formula. This\nallows an invariant to be learned.\n\nNovelty and Significance:\n\nThis work is interesting and although authors do seem to be\noutside of the community, I firmly believe is appropriate for\nICLR. If the claims made by the paper are true they constitute\na significant contribution to the field of program synthesis\nand program analysis.\n\nTechnical Quality:\n\nThe evaluation was fairly thorough, but the paper can be\nstrengthened massively with a few small changes and additions.\nIt might be more helpful if there was a sense of how many problems\nnone of the systems can do and how complicated of a program can\nthis system extract a loop invariant from. Why were was it these\nparticular 12 that work? Are there examples that don't?\n\nI don't know why all this time is spent on t-norms when behavior\non them is fairly similar and the simplest norm works best.\n\nLots of details are missing in this paper. How much training data\nwas generated? How long did it take? Does training data need to\nbe generated for each example? If so is that included in the\nruntime for Figure 3?\n\nThe paper talks about neural architecture, but all I see is\neffectively a curve-fitting task for some template. This feels\ndifferent from the code2inv paper where a program can be fed\ninto the system and the pretrained model emits the invariant.\n\nClarity:\n\nNot enough of this paper concentrates on the novel aspects of the\napproach. Section 5 discusses template generation, but not in\nenough detail that I would be able to replicate this work. I\ncould also not find enough details in the Appendix.\n\nI don't know why vital page space is spent on defining completeness\nand soundness.\n\n\nPossibly related work as relaxations to SAT/SMT solvers do exist in the literature.\n\nGuiding High-Performance SAT Solvers with Unsat-Core Predictions\nhttps://arxiv.org/abs/1903.04671\n\nLearning to Solve SMT Formulas\nhttps://papers.nips.cc/paper/8233-learning-to-solve-smt-formulas.pdf\n\nNotes:\n\nYou cite Si et al. for LoopInvGen when you should be citing Pathi\nand Millstein in the third paragraph on page 2.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The domain is loop invariant detection, in the static program analysis space. Loop invariants hold before, during, and after loop execution, and can be useful for compiler optimizations and/or correctness checking. \nThe paper explains Basic Fuzzy Logic and then uses it to introduce Continuous Satisfiability Modulo Theories, including proposed continuous mappings for inequalities (>, >=), negations, equalities, and requirements of t-norms to be useful for the relaxated optimization proposed. This Continuous Logic Network is then optimized to provide invariant proposals to Z3, an SMT solver. The whole system is used to solve the entire Code2Inv benchmark set, in substantially faster time and with fewer proposals to Z3 than comparable previous approaches. Ablations are provided which study the t-norm used (3 options considered), and another which uses heuristics only with no training/optimization to make static proposals to Z3. \n\nOn the whole, I like the presentation and the thinking here, and think it will be interesting to folks in the field, possibly spurring on further thinking in compilers, program synthesis, constrained optimization, etc, so recommend accepting. \n\nRelaxed representations of satisfiability problems seems like something people have thought about in OR for some time, so I wonder if there is a missing part of the literature survey. A cursory glance turns up https://openreview.net/forum?id=BJxgz2R9t7 \n\nInterestingly, the heuristics do quite well, which calls into question how hard the dataset is, and how competitive the preceding works really were. Since these heuristics seem to be an important contributor to this approach, I think they deserve further discussion in the appendix, and/or source code should be released.\n\n9 problems from the dataset are rejected as invalid. Please identify these in an appendix, and provide the counterexamples.\n\nThe dataset used here is quite small, and it seems like only ~30 of the problems are \"hard\" in requiring beyond-heuristic complexity. Couldn't the SyGuS tools be used to generate a much larger test set?\n\nIn fig 2, the model (x) doesn't match the template/invariant \\/."
        }
    ]
}