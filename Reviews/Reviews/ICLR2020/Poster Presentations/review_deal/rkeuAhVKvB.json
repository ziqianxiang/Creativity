{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The paper presents a graph neural network model inspired by the consciousness prior of Bengio (2017) and implements it by means of two GNN models: the inattentive and the attentive GNN, respectively IGNN and AGNN. The reviewers think \n- The idea of learning an input-dependent subgraph using GNN seems new. \n- The proposed way to reduce the complexity by restricting the attention horizon sounds interesting. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #1",
            "review": "# Update after the rebuttal.\nThanks for reflecting some of my comments in the revision. The presentation seems to be improved, and the additional ablation study seems to address my concerns. \n\n# Summary\nThis paper proposes a new neural network architecture for sequential reasoning task. The idea is to have two graph neural networks (GNNs), where one performs input-invariant global message passing, while the other performs input-dependent message passing locally. The input-dependent GNN employs a flow-style attention mechanism. The results on several knowledge completion datasets show that the proposed method outperforms the state-of-the-art methods.\n\n# Originality\n- The idea of learning an input-dependent subgraph using GNN seems new. \n- The proposed way to reduce the complexity by restricting the attention horizon sounds interesting and seems necessary for scaling up. \n\n# Quality\n- The overall architecture looks like a fairly complicated combination of neural networks (two GNNs with attentive mechanism). However, it is not entirely clear how much each component contributes to the performance. The experiment only shows the benefit of having IGNN.\n- The effect of the proposed complexity reduction technique is not studied in the experiment. \n- The empirical results are hard to parse, as they contain too much dataset-specific results that are not clearly explain the paper.  \n\n# Clarity\n- The paper is too dense with unnecessary details. For example, the introduction is too long (2.5 pages). The problem formulation contains too much details that deviate from the actual problem formulation. The details of each dataset (Table 1) and experimental setup can be moved to the appendix. \n- Many figures in each experiment contain too small texts with lots of unexplained dataset-specific legends. \n\n# Significance\n- Although this paper proposes an interesting neural architecture for knowledge completion tasks, it is not clear how much each component contributes to the performance. Also, the empirical results could be presented in a better way to deliver clear conclusions. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The paper presents a graph neural network model inspired by the consciousness prior of Bengio (2017) and implements it by means of two GNN models: the inattentive and the attentive GNN, respectively IGNN and AGNN.\n\nIntroduction outlines clearly what is the goal and why this is in principle an interesting avenue to investigate, however I found it could be more precise. In particular message passing techniques can be implemented very efficiently and are highly scalable. Section on Cognitive intuition of the consciousness prior doesn’t tell much beyond the re-stating of what it is, I am more interested in what it brings to the table and on what is the intuition behind its application to this domain.\n\nOne thing I would argue agains is that the input-dependent local subgraph requires access to the global graph, therefore going back to a conventional GNN. The argument is that the message passing can be constrained to reduce the computational burden. However, this can also be done by means of anchor graphs or other data structures, dynamic pooling and so fort. How do the authors compare to such choices?\n\nOverall I like the “global conditioning“ by means of sampling to have better local representations at each node, an idea that while it can be cast as consciousness prior it is also related to neural processes and architectures alike.\n\nThe implementation section could be made a bit clearer. Currently, for instance, there are references to prediction tasks while I think it would be nicer to have it fully self contained. Also, the aggregation strategy seems to be that of Kipf et al., is it a constraint of the model or other strategies could be used as well?\n\nThe experiment section is clearly explained and results are interesting, showing the potential of the proposed approach. Also the thorough analysis of the model is very well done.\nI wonder what is the performance when using no sampling at all, shouldn’t this be the reference?\n\nThe convergence analysis states that the model converges very fast, does it also translate to better results in case of small amount of training data?"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "title": "Official Blind Review #3",
            "review": "The authors propose to use sampling methods in order to apply graph neural networks to large scale knowledge graphs for semantic reasoning. To this end induced subgraphs are constructed in a data-dependent way using an attention mechanism. This improves the efficiency and leads to interpretable results. The experiments show some improvements over path- and embedding-based methods.\n\nThe paper is partially difficult to read and not well structured (see minor comments below). Overall, I think that the proposed GNN architecture is an original and interesting approach for this specific application. The experimental evaluation presented in the Section 4 shows clear improvements. This, however, is not true for the results presented in the appendix (Table 4). I am missing a discussion of the limitations of the proposed approach. Moreover, a thorough discussion of the hyper-parameter selection and, if possible, theoretical justification would be highly desirable and could strengthen the paper.\n\n\nMinor comments:\n\n- Section 2 start with the paragraph 'Notation', but does not contain any other paragraph.\n\n- The sampling strategy should not be introduced as part of the section 'problem formulation'.\n\n- using standard terms from graph theory for well-known concepts (such as 'induced subgraph') would improve the readability\n \n----------------------------\nUpdate after the rebuttal: The authors have addressed several of my concerns and improved the manuscript. I have raised my score from \"3: Weak Reject\" to \"6: Weak Accept\".",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}