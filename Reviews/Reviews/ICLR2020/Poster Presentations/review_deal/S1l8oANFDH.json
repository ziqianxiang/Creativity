{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The authors consider control tasks that require \"inductive generalization\", ie                                                     \nthe ability to repeat certain primitive behaviors.                                                                                 \nThey propose state-machine machine policies, which switch between low-level                                                      \npolicies based on learned transition criteria.                                                                                     \nThe approach is tested on multiple continuous control environments and compared to                                                   \nRL baselines as well as an ablation.                                                                                                          \n                                                                                                                                   \nThe reviewers appreciated the general idea of the paper.                                                                           \nDuring the rebuttal, the authors addressed most of the issues raised in the                                                        \nreviews and hence reviewers increased their score.                                                                                 \n                                                                                                                                   \nThe paper is marginally above acceptance.                                                                                          \nOn the positive side: Learning structured policies is clearly desirable but                                                        \ndifficult and the paper proposes an interesting set of ideas to tackle this                                                       \nchallenge.                                                                                                                         \nMy main concern about this work is:                                                                                                \nThe approach uses the true environment simulator, as the                                                                           \ntraining relies on gradients of the reward function.                                                                               \nThis makes the tasks into planning and not an RL problems; this needs to be                                                       \nhighlighted, as it severly limits its applicability of the proposed approach.                                                                              \nFurthermore, this also means that the comparison to the model-free PPO baselines                                                   \nis less meaningful.                                                                                                                \nThe authors should clear mention this.                                                                                             \nOverall however, I think there are enough good ideas presented here to warrant                                                     \nacceptance. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "# Summary\n\nThis paper proposes a technique for synthesis of state machine policies for a simple continuous agent, with a goal of\nthem being generalizable to out-of-distribution test conditions. The agent is modeled as a state machine with constant\nor proportional actions applied in each node (regime), and switching triggers between the regimes represented as\nlength-2 boolean conditions on the observations. The technique is evaluated on 7 classic control environments, and found\nto outperform pure-RL baselines under \"test\" conditions in most of them.\n\n# Review\n\nI am not an expert in RL-based control, although I'm familiar with the recent literature that applies formal methods to\nthese domains. I find the studied settings valuable albeit fairly limited, but the paper's method undeniably shows\nnoticeable improvement on these settings. Inductive generalization is an important problem, and the authors' approach of\nlimiting the agent structure to a particular class of state-machine policies is a reasonable solution strategy.\n\nThat said, the complexity of synthesizing a state machine policy clearly caused the authors to limit their supported\naction and condition spaces considerably (Figure 6). That, I'm assuming, limits the set of applicable control\nenvironments where optimization is still feasible. The authors don't provide any analysis of complexity or empirical\nruntime of the optimization process. Breaking it down for each benchmark would allow me to appreciate the optimization\nframework in Section 4 much more. As it stands, Section 4 describes a complex optimization process with many moving\nparts, some of which are approximated (q* and p(τ|π,x₀)) or computed via EM iteration until convergence (π*). It is hard\nto appreciate all this complexity without knowing where the challenges manifest on specific examples.\n\nSection 4.2 needs an example, to link it to the introductory example in Figure 1. The \"loop-free\" policies of the\nteacher are, in programmatic terms, _traces_ of the desired state machine execution (if I understand correctly), but\nthis is not obvious from just the formal definition.\n\nThe EM optimization for the student policy makes significant assumptions on the action/condition grammars. Namely, the\nalgorithm iterates over every possible discrete \"sketch\" of every program, and then numerically optimizes its continuous\nparameters (Appendix A). When the action/condition grammars grow, the number of possible programs there increases\ncombinatorially. Is there a way to adapt the optimization process to handle more complex grammars, possibly with\ndecomposition of the problem following the program structure?\n\nSection 5 needs a bit more details on the Direct-Opt baseline. It's unclear how the whole state machine policy (which\nincludes both discrete and continuous parts) is learned end-to-end via numerical optimization. Granted, the baseline\nperforms terribly, but would be great to understand how it models the learning problem in order to appreciate why it's\nterrible.\n\nWhy were the \"Acrobot\" and \"Mountain car\" benchmarks removed from the main presentation of results?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "This paper can be viewed as being related to two bodies of work:\n\n(A) The first is training programmatic policies (e.g., https://arxiv.org/abs/1907.05431).  The most popular idea is to use program synthesis & imitation learning to distill from a programmatic policy from some oracle policy.  \n\n(B) The second is training compact policies using a complex model-based controller (e.g., Guided Policy Search).   The idea is to use a step-wise model-based controller to design a good trajectory that maximizes reward, while not deviating too far from the current policy.  Then the new policy is learned from this trajectory.\n\nThe authors contrast with (A) via \"our student does not learn based on examples provided by the teacher, but is trained to mimic the internal structure of the teacher\". The authors contrast with (B) in part by claiming that \"the teacher must mirror the structure of the student\", which is supposedly harder.\n\nThus, it seems much of the intellectual merit & novelty lies how the proposed method tackles this \"structure\" problem, from both the teacher and the student side.  However, I'm having a hard time appreciating this aspect of the proposed approach.  I'm also confused by how the \"student does not learn based on examples provided by the teacher\" if it's doing imitation learning on trajectory-level demonstrations. Can the authors elaborate on this point further?\n\nThe experiments seem ok.  The idea of training programmatic polices that \"inductively generalize\" has been done before on arguably more difficult tasks (see Table 2 in https://arxiv.org/abs/1907.05431).   To contrast with this prior result, it seems the main point is that prior work relied on domain-specific program synthesizers.  Can the authors elaborate on this point further?\n\nMinor comments:\n\n-- Adaptive teaching is a pretty ambiguous term, and I think misleading within ML community (cf. https://arxiv.org/abs/1802.05190). I recommend a different algorithm name.\n\n-- Deriving the variational objective is a lot of work to reduce it to just trajectory design.  Seems overkill.\n\n\n\n------------------------------------------------------------\nUpdates after Author Response\n------------------------------------------------------------\n\nI increased my score to accept.  I think this is a worthy contribution, and the authors did an excellent job addressing my concerns.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "This work proposes a framework for structuring policies using finite state machines, training a teacher-student setup using variational inference. The method is tested on a suite of standard control problems against PPO with / without memory and against simple numerical optimisation, analysing both performance and some degree of generalisation.\n\n\nOverall, I like both the problem setting (constraining / structuring policies using SMs), and the proposed modeling and optimisation. In particular, the teacher-student setup makes sense to me in the way it has been casted under VI, and I would like to see it explored further.\n\nI have however a few major issues that prevent me from recommending acceptance of the work:\n\n1. The experiment section is generally lacking in terms of implementation details. The authors don't specify anything about models structure or details about the employed state machines, and do not seem to have included details about their direct optimisation baseline, environment featurisation, hyperparameters used across their experiments, and so on. I very much doubt I would be able to reproduce the results based only on the manuscript.\n\n2. The quality of the policies depend heavily on how close the proposed state machine matches the underlying dynamics of each task. Since - very often - complex tasks are hard to optimally describe without producing very large state machines, I would have liked to see the method tested against poor and/or adversarial specifications, to see whether empirically how well the student-teacher optimisation system can recover under such constraints.\n\n3. Casting the problem as a POMDP, while technically fine (and in most cases reasonable), doesn't seem to provide any significant advantage to the method, and seems to only be adding noise in the formalisms described across the paper. Since the method introduces notation that a lot of RL researchers are not necessarily familiar with, I would suggest the author to try to simplify it where possible. [Also please note that I haven't re-derived section 4.1 under this assumption, so correct me if I'm wrong on this.]\n\n\nAt this point, I am recommending a weak rejection, however I will be happy to significantly reconsider my overall assessment provided that at least point (1) is decently addressed (and ideally I get some response wrt. points 2 and 3).\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}