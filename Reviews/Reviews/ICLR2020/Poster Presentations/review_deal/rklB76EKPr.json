{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper studies the effect of clipping on mitigating label noise. The authors demonstrate that standard gradient clipping does not suffice for achieving robustness to label noise. The authors suggest a noise-robust alternative. In the discussion the reviewers raised some interesting questions and technical detailed but mostly agreed that the paper is well-written with nice contributions. I concur with the reviewers that this is a nicely written paper with good contributions. I recommend acceptance but recommend the authors continue to improve their paper based on the reviewers' suggestions.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\n[Summary]\nThis paper studies the relationship between gradient clipping in stochastic gradient descent and robustness to label noise. Theoretical results show that gradient clipping in general is not robust to symmetric label noise. The paper then proposes a variant of gradient clipping (cl-clipping) that induces label noise robustness. Experiments support these claims on synthetic datasets and typical classification benchmarks.\n\n[Decision]\nThe first contribution, that gradient clipping does not induce robustness to label noise, is an important negative result given the prominence of gradient clipping and datasets with noisy labels. The second contribution, cl-clipping, amounts to minimizing a non-convex loss with saturating regions but, as far as I know, these properties are necessary for robustness to label noise. Theoretical results are limited to SGD with mini-batch size 1 but the insights carry over to larger mini-batches in the experiments. Overall, I recommend acceptance.\n\n[Comments]\nThe parameter tau controls robustness, and a higher noise level requires a higher tau. There is little discussion on how this parameter is chosen in the experiments. On the synthetic dataset, the Huberized loss uses tau=1 and the partially Huberized loss uses tau=2. How are these values chosen? Did the authors observe a U-shaped curve when sweeping over tau? On the real-world datasets, tau is fixed for each method across different noise levels. Does this mean that a single value of tau worked best regardless of the noise level, or was it tuned for a particular noise level?\n\nProposition 4 shows that symmetric noise breaks down the clipping method in Eq (7) which can be seen as a special case of gradient clipping. I might be missing something here, but it is not obvious to me that, when the norm of x is constant across the samples, Eq (7) is equal to gradient clipping.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "Summary:\n\nGradient clipping has been studied as an optimization technique and also as a tool for privacy preserving, but in this paper, it studies the robustness properties of gradient clipping.  More specifically, the main question of the paper is: Can gradient clipping mitigate label noise?  The paper reveals that the answer is no, but further proposes a simple variant of gradient clipping is robust and has nice property of classification calibration.  Experiments show that the proposed variant works under label noise.\n\n\nStrength of the paper:\n\nThe motivation and goal of the paper is stated in the title and is very clear, making it easier to follow the story of the paper.  There are sufficient background on the loss functions and gradient clipping in the beginning that helps guide the reader.  The proposed method is robust to label noise and has theoretical guarantees.  The relationship between similar work is summarized.  Experiments have both synthetic and benchmark datasets to demonstrate the behavior of the proposed method.\n\n\nWeakness of the paper:\n\nCurrently, the experiments only include methods studied in the paper.  It would be better to include baseline methods stated in the end of Section 5 or in Section 4.3.\n\nAfter response:\nThank you for the clarification!  I have read the other reviews and author comments.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "This paper studied a fundamental problem in robust machine learning, that is, can gradient clipping mitigate label noise? The paper is well written, clearly motivated, highly novel and significant not only in a theoretical sense but also in a practical sense.\n\nAs argued in the abstract, gradient clipping is a widely-used technique which is generally motivated from the OPTIMIZATION point of view. In this paper, the authors proposed an entirely new motivation of gradient clipping from the ROBUSTNESS point of view, since intuitively it should be able to mitigate label noise. Surprisingly, the authors proved that for some simple binary classification with label noise, standard gradient clipping does not provide robustness; on the other hand, a simple variant of gradient clipping is robust, and is equivalent to suitably modifying the underlying loss function.\n\nThe major contributions were well summarized in the bottom of page 1, the related work was discussed in sec 4.3, and the caveats of this new methodology were also given in sec 4.4. Note that the proposed composite loss-based gradient clipping is applicable even on top of existing noise-robust losses, for example, the generalized cross-entropy loss, and this serves as a convincing demonstration of the great significance of the paper. Actually, the paper is full of insights, and I really enjoyed reviewing it.\n\nI have a few questions on Tables 1 and 2. Table 1 said the standard gradient clipping is not robust according to Proposition 4. However, Proposition 4 is for the loss-based gradient clipping in (7) rather than the standard gradient clipping in (6). Perhaps I have missed something, but why the non-robustness of (7) can imply the non-robustness of (6)?\n\nIn Table 2, Linear loss on MNIST, the test accuracy was 9.6 when rho=0.6. Is this a typo? The accuracy was still 78.8 when rho=0.4. Moreover, the authors explained why the theoretically grounded linear loss performed so badly, that is, the optimization was so difficult. How about CE+clipping loss on CIFAR-100? The standard gradient clipping was also harmful here, even when there was no label noise at all. Was this due to bad optimization or robustness (or both)? \n\nSince distributionally robust supervised learning is a future direction to go, I think the authors may be interested in a thought-provoking paper:\nW. Hu, G. Niu, I. Sato, and M. Sugiyama. Does distributionally robust supervised learning give robust classifiers? ICML 2018."
        }
    ]
}