{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This is an interesting contribution that sheds some light on a well-studied but still poorly understood problem. I think it might be of interest to the community.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "The authors present a study of GAN dynamics in training with the goal of understanding whether rotational behavior occurs when training GANs on real world datasets, and whether training methods find local Nash equilibria. \n\n\nThe authors start by motivating the study of a game vector field with a toy example in which we can see all relevant behavior in the relevant directions. The work investigates the game vector field with a visualization technique called “path-angle” that attempts to alleviate the problem of high dimensionality by only looking at the cosine similarity (between the linear interpolation between the two points versus the true gradient at the point) along a path between two (concatenated) weight vectors at a time. The work also investigates by looking at the gradient norms of weights in these optimization trajectories.\n\n\nThe work uses these techniques to visualize the dynamics of GANs trained on standard datasets. The authors find that GANs do not converge to local Nash equilibria, that each player ends at a saddle point, and state evidence for “rotational behavior” in GAN dynamics.\n\n\nThe finding that GAN training methods do not find local Nash equilibria is interesting. However, it is unclear to me what the experiments presented show about GAN training dynamics (in particular, it is unclear to me what rotational dynamics are in the context of GANs and what consequences they have for training). I have listed more detailed feedback below.\n\n\nDetailed feedback:\n* Section 3.1: What is the formulation of Mescheder et al 2017? This should be explicitly stated in the paper.\n* The authors never explicitly, formally define what it means for there to be rotational behavior. However, this terminology is used frequently throughout the paper, particularly in the empirical section (5.1). What does rotational component mean in this Section?\n* What is the motivation for completing the path based landscape visualization methods between a random initialization and the final weight vector? Is there a reason why the actual iterates were not investigated with this method?\n* Why does the bump in Figure 3 imply that there is a non zero rotational component?\n* It would be good to complete a more thorough understanding of the spectra of hessians at convergence; the extent of the experiments in Figure 5 and 6 appears to just be 3 training runs; a larger sample size would be good to establish trends.\n* What is the motivation for visualizing via the path based methods? Why does interpolating between initialization and the final learned weight vector tell us about the rotational dynamics in high dimensions? This line may not be representative of the optimization trajectory followed by the actual iterates.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "Summary: \n\nThis paper proposes visualization techniques for the optimization landscape in GANs. The primary tool presented in this paper is a quantity called path-angle, which looks at the angle between the game vector field and the linear path between a point away from a stationary point and a point near a stationary point. The paper present examples of the visualization for dynamics with pure attraction, pure rotation, and a mix of attraction and rotation. Along with this, the authors propose to look at the eigenvalues of the game Jacobian and the individual player Hessian’s to evaluate convergence in GANs. The paper presents application of the tools on GANs trained with NSGAN and WGAN-GP objectives on a mixture of Gaussians, MNIST, and CIFAR10. The primary observation is that the generator performance is good, but the algorithms converge to non-Nash stable attractors. Moreover, it is shown using the path-angle plots that GANs exhibit rotational behavior around stable points.\n\nReview: \n\nThere has been a lot of work in the past few years (and ongoing) on principled training approaches for GANs. The objective of the algorithms is typically to converge a differential Nash equilibrium and/or to reach a stable point of the dynamics quickly. In my view, this work fills some of the gap on the empirical side of things with respect to each goal. \n\nNotably, a main idea to speed up convergence in GANs is to change the gradient play dynamics so rotational components are neutralized. The path angle visualization provides a novel tool to evaluate the empirical ability of any dynamics proposed for GANs to cancel out rotational components. Since it is generally known that gradient play dynamics are susceptible to cycling, I would have been interesting in seeing the path angle plots for some recently proposed algorithms such as consensus, symplectic gradient adjustment, stable opponent shaping, local symplectic surgery, etc to see how they compare. This would have made the experiments using the path angle visualization stronger in my view. Nonetheless, the path angle tool is useful and I can foresee it being commonly used in the future. \n\nAside from neutralizing rotational components, dynamics have been proposed with the goal of avoiding non-Nash stable attractors and converging only to differential Nash equilibria. However, to my knowledge, there has not been much, if any, evaluation in GANs to see if the methods are in fact converging to Nash equilibria as theory may predict. While simple, I found it interesting to evaluate the eigenvalues of the relevant quantities at convergence. I am curious why the authors evaluate the top-k eigenvalues in terms of magnitude? The scipy package referenced in the appendix can compute the largest and smallest real eigenvalues, which is what it seems like you would want to evaluate the definiteness of the game Jacobian and the individual player Hessians. The most interesting empirical result in the paper to me was that it is common to converge to non-Nash stable attractors using standard training techniques and at such stable points the generator performance is strong. This is an important observation and  may cause some consideration of what points should be sought in GANs. I am not fully convinced this is always what the dynamics would always converge to depending on the network, learning rates, optimization methods, etc, but showing that it can be the case is useful. \n\nOverall, I think this paper introduces some useful tools to interpret the performance in GANs and to help understand the behavior of training dynamics. The main tool introduced was the path angle visualization and the primary empirical result was that standard GAN methods may reach non-Nash stable attractors and perform well. The paper probably be condensed in the first 4 pages, so that more experimental results could be presented and this would make the paper stronger.  \n\nPost Response: Thanks for the response. I believe this paper should be accepted.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper tries to provide a deeper understanding of the training dynamics of GANs in practice via characterizing and visualizing the rotation and attraction phenomena nearby a locally stable stationary point (LSSP) and questions the necessity to access a differential/local Nash equilibrium (LNE). In particular, this paper first discusses the difference between LSSP and LNE and formalize the notions of rotation and attraction around LSSP in games. Then, this paper proposes the path angle to visualize the rotation and attraction nearby an LSSP. The path angle is a function that maps linearly distributed points in the line, which is determined by an initial parameter set and a well-trained parameter set, to the angles between the line and the gradient of a given point in that line. The rotation and attraction phenomena can be observed in the plot of the path angle as  \"a quick sign switch\" and \"a bump\" nearby 1, respectively. The experiments empirically demonstrate that: 1. rotation exists in the training dynamics of practical GANs; 2. GANs often converge to an LSSP than an LNE, but still, achieve good results.\n\nGenerally, this paper is interesting and well-written. The contribution is clearly presented and the literature is well discussed. However, I have some questions to be clarified by the authors as follows.\n\n1. In Sec 3.2., this paper tries to motivate the readers to notice the difference between the LSSP and DNE by introducing Example 1. However, I notice that there is a gap that hasn't been presented clearly: Example 1 is a general game but does not correspond to a GAN, which is of the most interest in the paper. Besides, the generator loss at the optimum should be (theta_2 - 1)^2 - 1/2(theta_1 - 1)^2 instead of theta_2^2 - 1/2theta_1^2. \n\n2. For the rotation around LSSP, existing work, including Mescheder et al. (2018), Gidel et al. (2019b),  has a prior discussion. Besides, it is intuitive that an LSSP is not an LNE in practical GANs with high probability because finding a descent direction is easy given such a high-dimensional space. It is also possible to find a sharp descent direction nearby an LSSP because the norm of the gradient is averaged across all dimensions. It is good to formulate these observations in a precise way but it would be better to see further implications of the two observations. If so, the paper quality will be significantly improved. \n\n3. A minor thing is why (c) and (f) in Figure 3 and Figure 4 use different metrics, i.e. FID and IS, respectively? \n\nI also note that this paper has 10 pages and should be expected at a higher level than other accepted papers. Given all these conditions, I think I make it clear why I give a rating 6 currently. \n\nBy the way, I'm not absolutely confident about the comments because I didn't work on analyzing the dynamics of GANs. I'll appreciate it if my issues can be addressed or a potential misunderstanding can be corrected. "
        }
    ]
}