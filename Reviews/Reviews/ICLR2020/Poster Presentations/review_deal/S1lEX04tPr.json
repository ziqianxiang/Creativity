{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper was generally well received by reviewers and was rated as a weak accept by all.\nThe AC recommends acceptance.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper presents a method for adding credit assignment\nto multi-agent RL and proposes a way of adding a curriculum\nto the training process.\nThe best heuristics and structures to incorporate in the\nmodeling, learning, and exploration parts of multi-agent\nRL are still largely unknown and this paper explores some\nreasonable new ones.\nIn most of the tasks this is evaluated on in Figure 5\nthis approach adds a slight improvement to the SOTA\nand I think an exciting direction of future work is\nto continue pushing on multi-agent RL in even more\ncomplex envirnoments where the SOTA break down in\neven worse ways.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper presents a new method CM3 for multi-goal multi-agent settings where agent must care about the rewards of others as well as theirs. The method restructures the problem into two-stage curriculum where individual agents solve the problem in the individual setting of the environment in the first stage and then more agents are introduced in the multi-agent setting in second stage. The method also uses function augmentation to only learn parameters which are necessary for single agent in first stage and then more parameters are introduced in the second stage. The results are shown on three environments which show that CM3 outperforms the baselines.\n\nThe paper is well-written, motivated and clear to read. A lot of important stuff has been pushed into appendix and experiments/setup require more details but overall I believe paper has significant contributions and results. Therefore, I assign a rating of weak accept which I am happy to raise if clarity of paper can be improved.\n\nThe paper doesn’t comment on parameter counts of CM3 compared with baselines which is also an important factor in choosing one method over the other. I would like to see more quantitative analysis as presented in IAC to understand what is happening behind the scenes. It is very surprising to see that IAC is outperforming COMA in most of the tasks which is opposite of what COMA paper suggested.\n\nOn the note of curriculum [1] and [2] uses curriculum in traffic junction settings to improve overall performance as agent increases. This can be compared to moving from stage 1 to stage 2 but instead we move from less number of agents to more. [2] also suggests that using individualized rewards help in better credit assignment. There is also an assumption in the setup that private observations from all other agents are readily available. It would be interesting to see what would happen if agents have to communicate their private state. So, the paper is also missing discussion on communication protocols (discrete, continuous, through critic) [1][2]. \n\nIt is hard to directly compare through the charts. So, tables in Appendix E and F should be moved to the main text. The experiment section needs to be extended and the main model section needs to be decreased in the content amount.\n\nIt seems like you missed 1/n in advantage function’s equation in Section 3, Multi-agent credit assignment section. Figure 6 has been mentioned in 6 second paragraph but doesn’t appear until the last page.\n\n[1] Sukhbaatar, Sainbayar, and Rob Fergus. \"Learning multiagent communication with backpropagation.\" In Advances in Neural Information Processing Systems, pp. 2244-2252. 2016.\n[2] Singh, Amanpreet, Tushar Jain, and Sainbayar Sukhbaatar. \"Learning when to communicate at scale in multiagent cooperative and competitive tasks.\" arXiv preprint arXiv:1812.09755 (2018)."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Contribution:\n\nThis paper derives a loss for cooperative MARL for the case where agents have individual goals assigned to them.\nThe assumption of individual goals allows for a better credit assignment, as well as effective pre-training of one agent to solve its own goal in isolation. Based on this observation, the paper subsequently proposes a curriculum learning scheme to fully take advantage of this property.\nExperiments in varied domains are shown.\n\nReview:\n\nThe paper is well written and easy to follow, and makes a good case by having a thorough experimental analysis, as well as a theoretical analysis of the credit function.\n\nThe applicability of the method seems rather high, even though there exists multiple MARL environments where the multi-goal assumption will be broken. In a predator-prey domain, for example, the predators can't learn anything because their tasks is not solvable with only one agent. In that context, it seems that stage 1 would be useless, but would stage 2 still work, and if yes how would that compare to other baseline methods?\n\nSome details of the multi-stage training are a bit unclear to me. Section 4.5 states \"we train an actor \\pi^1 and critic Q^1 to convergence [...]\". However, I don't fully grasp how this agent will be able to learn to solve all the goals? It seems to me that with N=1, equation (5) reduces to normal actor critic with one agent and one goal, but I'd expect that the policy must be trained on all the goals, as this is hinted at in section 5 (\" in Checkers, we alternate between training one agent as A and B\"). Could you clarify that part?\n\nAbout the function augmentation, could you clarify how the new network \\pi^2 is initialized? It seems that the initial values of W^1 and W^{1:2} in particular are quite important, because if the resulting policy is too far off from the initial policy \\pi^1, then the benefit of the pre-training could be lost on the way. Did you find that any special care like initializing W1 = I and W^{1:2} = 0 is required here?\n\n\nOne minor complaint is that the proposed method never seems to achieve statistically better performance than the best baseline on any of the tasks (for cooperative navigation it is tied with IAC and for SUMO and Checkers it is tied with QMIX). But since the best baseline is different across tasks, it suggests that the proposed method is more versatile.\n\n\nA potentially relevant missed reference: [1].\n\n\n[1] A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning, Carion et al, https://arxiv.org/abs/1910.08809\n"
        }
    ]
}