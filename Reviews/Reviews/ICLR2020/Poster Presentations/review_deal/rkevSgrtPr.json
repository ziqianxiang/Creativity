{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This is a nice paper on the classical problem of universal approximation, but giving a direct proof with good approximation rates, and providing many refinements and ties to the literature.\n\nIf possible, I urge the authors to revise the paper further for camera ready; there are various technical oversights (e.g., 1/lambda should appear in the approximation rates in theorem 3.1), and the proof of theorem 3.1 is an uninterrupted 2.5 page block (splitting it into lemmas would make it cleaner, and also those lemmas could be useful to other authors).",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "UPDATE TO MY EARLIER REVIEW\n============================\n\nSince this paper presets new findings that will be of significant interest to much of ICLR's audience, and the paper is is well-written, I am changing my rating to \"Accept\". Since Reviewer #1 did not submit a review and Reviewer #2 indicated that (s)he does not feel well-qualified to review this paper (it is very much on the theoretical side after all), it would be great to get one further review from an area chair or otherwise qualified person. \n\nMY EARLIER REVIEW\n=================\n\nThis this exciting submission presents a new proof of Leshno's version of the universal approximation property (UAP) for neural networks  -- one of the foundational pillars of our understanding of neural networks. The new proof provides new insights into the universal approximation property. I consider these the main contribution of the paper. Specifically, the authors\n- provide an upper bound on the required width for the neural network\n- show that the approximation property still holds even if strong further requirements are imposed on the weights of the first or last layer. \n\nI rate this submission a weak accept. It’s a very good paper. The work makes useful contributions that should and will be of interest to many in the field. The paper is generally well-written. \n\n\nSome remarks:\n\n- Being somewhat long, the “Proof of Theorem 3.1” would be a much better read if the authors prefixed it  with an outline of the strategy that the proof takes. \n\n- The authors point out that the lack of dependence of Theorem 3.1 on epsilon is surprising, and cite Lin’s work from 2017 who previously found such an independence. Lin’s derivation of the epsilon-independent UAP is much more intuitive than that of this submission, in which the epsilon independence really pops out somewhat magically and for me only made sense when I read the paper again. I would encourage the authors to add to Lin’s paper’s citation sentence that this paper motivates the epsilon independence well. Alternatively, the authors could add a few sentences to their paper to provide intuition on how the epsilon-independence comes about in their line of argument. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review": "This paper studies the representation power of single layer neural networks with continuous non-polynomial activation, and specifically, provided a refinement for the universal approximation theorem:\n1.  Established an exact upper bound on the width needed to uniformly approximate polynomials of a finite degree (to any accuracy of which the upper bound is independent), and\n2.  using this error-free bound to deduce a rate (of width) for approximating continuous functions.\n\nThe writing of the paper is concrete and solid.  The techniques used in establishing the results are interesting, in that:\n1.  The proof for polynomial approximation (Thm 3.1) is direct, via a close examination of the Wronskian of the target polynomial function, and\n2.  the analysis provided that the abilty to universally approximate is also preserved after placing certain restriction on the magnitude of the weights in the approximating neural network.  Consequently, this property is inherited by continuous function approximation to which the result is extended (Thm 3.2).\n3.  This analysis and some of the results derived in the proof may be used for other analyses, e.g. representation power of multilayer networks.\n\nSome further discussion of the results may be of interest to the readers.  \n-  (Optimality of Thm 3.2).  When the result in Thm 3.1 is extended to general continous functions via Jackson's theorem, to what extend does the rate deteriorate?  What does the rate look like when using certain common activations (such as ReLU, sigmoid).\n-  (Reference to random features).  Thm 3.3 appears to be related to random feature representation, whose approximating ability has been studied in prior works.  Some comment on those results may be beneficial (e.g. https://arxiv.org/abs/1810.04374).\n-  Although already a straightforward proof, it seems natural, and as a result may promote the presentation and clarity, to organize the proof to Thm 3.1 using smaller parts, which currently spans over 2 pages."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This this exciting submission presents a new proof of Leshno's version of the universal approximation property (UAP) for neural networks  -- one of the foundational pillars of our understanding of neural networks. The new proof provides new insights into the universal approximation property. I consider these the main contribution of the paper. Specifically, the authors\n- provide an upper bound on the required width for the neural network\n- show that the approximation property still holds even if strong further requirements are imposed on the weights of the first or last layer. \n\nI rate this submission a weak accept. It’s a very good paper. The work makes useful contributions that should and will be of interest to many in the field. The paper is generally well-written. \n\n\nSome remarks:\n\n- Being somewhat long, the “Proof of Theorem 3.1” would be a much better read if the authors prefixed it  with an outline of the strategy that the proof takes. \n\n- The authors point out that the lack of dependence of Theorem 3.1 on epsilon is surprising, and cite Lin’s work from 2017 who previously found such an independence. Lin’s derivation of the epsilon-independent UAP is much more intuitive than that of this submission, in which the epsilon independence really pops out somewhat magically and for me only made sense when I read the paper again. I would encourage the authors to add to Lin’s paper’s citation sentence that this paper motivates the epsilon independence well. Alternatively, the authors could add a few sentences to their paper to provide intuition on how the epsilon-independence comes about in their line of argument. \n"
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors derive the universal approximation property proofs algebraically. They note that this holds even with very strong constraints on the non-bias weights. \n\nThey assert that their results are general to other kinds of neural networks and similar learners. They leave the paper with a question regarding limitations on bias weights. \n\nI do not feel qualified to review this paper. I have opted for a weak accept since it seems thorough and the conclusions offer promise for other applications. However, I will defer to other, more qualified reviewers who have more carefully reviewed the paper than I have. "
        }
    ]
}