{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper presents results of looking at the inside of pre-trained language models to capture and extract syntactic constituency. Reviewers initially had neutral to positive comments, and after the author rebuttal which addressed some of the major questions and concerns, their scores were raised to reflect their satisfaction with the response and the revised paper. Reviewer discussions followed in which they again expressed that they became more positive that the paper makes novel and interesting contributions.\n\nI thank the authors for submitting this paper to ICLR and look forward to seeing it at the conference..",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #1",
            "review": "I am satisfied with the author's response and I am increasing the score to 6 after the rebuttal.\n\n===\n\nThis paper introduces a new and simple method of probing whether syntax information under the form of constituency trees is present in recent pre-trained language models (e.g. BERT, RoBERTa, XLNet and GPT2) without any additional task-specific training. They use the previously proposed concept of \"syntactic distance\" [1] as a sufficient statistics for the constituency tree of a given sentence. They compute the distance between neighboring words in sentence using a distance function f(g(w_i), g(w_i+1)) where f is a divergence between distributions and g(w_i) is the self-attention distribution of w_i at a given layer: the intuition is that words that have similar self-attention distributions belong to the same constituent and thus their syntactic distance is small. In order to conform with right-skewness of English syntax, they propose an affine transformation of the distances that encourage right-skewed trees, and tune the hyper-parameter directly on the target F1 score wrt ground-truth trees. Results suggest that large, pretrained models capture constituency trees to some extent.\n\nThis is an empirical and analysis paper which can be considered as the “twin” of Hewitt et. al (2019, analyzes whether dependency structure can be probed from large pre-trained models). The paper is well-written and easy to understand. The experiments are in general well-organized even if they lack clarity at some point. The related work is comprehensive and covers most of the work in the domain of grammar induction and unsupervised parsing. In its current form however, I feel like the paper misses a \"second half\" therefore it isn’t quite above the acceptance bar. I'd be happy if the authors could kindly elaborate on the following major weaknesses: (i) unclear scientific motivation of using the right-skewness bias; (ii) lack of clarity in the experimental results; (iii) lack of depth and perspective.\n \n1) About right-skewness bias:\nI appreciate that the authors clearly guard against using explicit right-skewness biases. However, I am not sure why the authors use of Eq. (2) apart from boosting performance. The authors claim that \"the main purpose of explicitly injecting such a bias is examining what changes are made to the resulting tree structures\" but the purposed changes are not discussed at length in the text. Therefore:\n1.1) What's the scientific motivation (apart from boosting performance) of skewing the trees using Eq. (2) ?\n1.2) What can you infer from trees without bias and tree with bias ?\n\n2) About clarity in the experimental results:\n2.1) (Shen, 2018) use an \"implicit\" right-skewness bias. From the text, it seems that your results w/o bias use the same bias as (Shen, 2018) ? Therefore they actually have a bias ? This is a bit confusing. What about using the unbiased parsing algorithm as presented in \"Straight to the tree...\" ?\n2.2) This sentence is unclear: \"we select one derived from the best choice of f and g in terms of sentence-level F1 (S-F1)\nw.r.t. gold-standard trees as a representative for the LM\". Are you using S-F1 on *training* for tuning f and g ?\n2.3) This sentence is unclear: \"As LMs are not fine-tuned with training sets in our framework, we only use the test\nset of the PTB\", what do you mean by \"using the test set\" ? Do you mean evaluate or use it for tuning (cf 2.2) ?\n\n3) About perspective and depth:\n3.1) The fact that ADVP , ADJP performance is higher for the tested models is interesting. Do you have any hypothesis for why this is happening?\n3.2) Right-skewness seems to mainly help with VP. Can you formulate an hypothesis for why current models underperform on this label or do not show right bias?\n3.3) The results on Table 2 are interesting, but imho, they are basically showing that these models cannot do constituency parsing with just a linear probe on top of the representations and using the syntactic distance algorithm?\n3.4) Did you consider using other parsing algorithms e.g. chart parser, instead of just the syntactic distance ? That would make a stronger paper.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "\n*Summary \n\nThis paper describes an effective method that induces constituency trees from pre-trained language models, which are attracting great attention recently. The authors demonstrated that the pre-trained language models have some properties that are similar to constituency grammar by showing some interesting features of the extracted trees. \nThis study is based on the motivation to unveil the reason why such pre-trained language models work and the extent to which pre-trained language models capture the syntactic notion of the constituency.\nThey also show that the method can become a reasonable baseline method for English grammar induction.\n\n*Decision and supporting arguments\n\nI'm leaning toward accepting the paper as a conference paper.\nAs the author describes, pre-training LMs are attracting attention, and many people want to understand their inner workings. The paper provides a systematic analysis of the pre-trained LMs from the viewpoint of grammar induction. Also, this paper is well-structured and offers a good literature review.  I think the paper has enough value to be published from the scientific viewpoint. \n\n\n*Additional feedback\n\nFigure 1 & 5-10 look rasterized. It's better to use vector images.\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "[EDIT: Thank you very much for the thoughtful and extensive response. Based on the latest iteration of the paper, I am changing my score to 8]\n\nThis paper studies the representations learned by large pre-trained models trained on language modeling objective (or language modeling-like objective, in the case of masked models). In particular, the authors investigate whether constituency information is present in the hidden layers. In contrast to much existing work that probe for this information with (for example) linear models on top of hidden representations, the authors propose to directly extract binary trees from the model using \"syntactic-distance\" measure that is calculated in various ways (e.g. dot product of hidden representations, distance in attention distributions). Across various models, the authors find that it is indeed possible to induce linguistically meaningful trees, in particular outperforming a right-branching baseline that is strong for English\n\nI found this to be a creative alternative to the existing \"BERTology\" type papers that rely mostly on linear probes, and the experiments are done across a wide number of setups (e.g. across models/datasets).  However, I have several questions/issues with the paper if addressed, would make the paper much stronger:\n\n- The degrees of freedom afforded by the choice in number of layers, heads, similarity measure, etc. is quite high. For example, with 24 layers and 16 attention heads I count (24*(16+1)*2 + 24*3) = 888 different measures of distance that could be used to induce trees. I think the authors should make sure that they are not \"overfitting\" to the test set with the following set of tests:\n\n1. Only use the training set to select the layer/metric combination.\n2. See if the selected combination generalizes to other languages (from comparing Table 1 against Table 2, it seems like this doesn't even generalized within the same language across different domains? This is worrying).\n3. See what the performance is with random initialization. In particular, I would like to see two setups: (a) the network is randomly initialized, (b) the *representations* (i.e. attention/hidden states) are randomly initialized. The random initialization should have variance such that it's not just uniform distributions.\n\n- I found the right branching bias baseline experiment not very informative. As the authors allude to, performance on unsupervised parsing itself is not so interesting, and the point of the paper (in my view) is not to get the best unsupervised parsing performance.\n\n- Related to the above, it is known that the decoding algorithm of Shen et al. 2018 is itself heavily biased towards right branching trees. See in particular: https://arxiv.org/pdf/1909.09428.pdf. Given this, I am not sure if the supposedly good performance in parsing is due to the model actually learning constituency or just the bias.\n\n- It would have been interesting to see how the performance changes as:\n1. The model is fine-tuned on the PTB-training set.\n2. The model is trained from scratch on PTB (obviously with a much smaller model since PTB is smaller).\n\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}