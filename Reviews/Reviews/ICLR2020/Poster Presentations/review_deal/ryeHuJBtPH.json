{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This work introduces a new neural network model that can represent hyperedges of variable size, which is experimentally shown to improve or match the state of the art on several problems. \n\nBoth reviewers were in favor of acceptance given the method's strong performance, and had their concerns resolved by the rebuttals and the discussion. I am therefore recommending acceptance. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper proposes a new graph neural network model capable of performing tasks over hyperedges of variable type and size (i.e. different number and types of graph nodes connected by different hyperedges).  They experimentally verify its effectiveness over the previous state of the art on several datasets and tasks.\n\nI lean toward accepting this paper.  It is a relevant topic area, the design appears novel relative to recent work, and the presentation is mostly clear.  The design decisions are well-motivated and discussed (e.g. the choices to use the static embedding, the exclusion of the diagonal self-attention weights).  Performance is tested on previously used datasets and tasks, for a thorough comparison against recent best methods (DHNE).  The model is clearly described.\n\nA possible weakness of this paper is that the evaluation data sets are all k-uniform hypergraphs with k=3.  This is perhaps the minimal case which their method can address.  For all the discussion of generality to heterogeneous hyperedges, it would have been better to include some dataset (even if synthetic) to establish baseline performance over while ablating k>3 and multiple hyperedge types.  This remains completely un-investigated (even the genome dataset appears to be k=3?).  Although, their edge/hyperedge ablation on these datasets partially addresses the point and the result (Fig 5) seems promising.\n\nSome other notes:\nEq 5.  Given that it is the default, explicitly write that the summation is over j!=i\nEq 8.  Probably leave the ellipses out between v_{i-1} and v_{i+1}?\nThe tasks and measures like AUC, AUPR, and network reconstruction could be described somewhere, even if in an appendix.\nSome/many training hyperparameters not listed?\n\n\nEDIT: Post rebuttal, authors have provided evidence of good performance on data sets with k=4 and k=5, addressing my main concern, and have addressed the smaller notes.  Raising score but still am not expert in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "Summary:\n\nThis work is tackling the problem of predicting variable-sized heterogeneous hyperedges in hypergraphs. The authors do so by introducing a multi-head attention mechanism that takes in a set of node features and predicts the probability of this set of nodes forming a hyperedge.\n\n- Quality and clarity: \n\nThe quality of the paper is good, the writing is clear and the reasoning is relatively easy to follow. The problem the authors are trying to tackle is also clearly defined. The motivation of the dynamic and static embedding choice could be explained better. The intuition that the pseudo-distance tells us how well the static embedding\nof a node can be approximated by the features of the neighbour within the tuple somewhat makes sense, but I also have the impression that this is just one possible interpretation and what happens in practice could actually be quite different. A more formal treatment and analysis of this intuition would be very helpful.\n\n- Originality: \nThe main contribution here is to propose a more flexible formulation to improve over the fixed MLP used by DHNE. This in itself is a good idea, but overall is an incremental improvement.\n\n- Significance: \nThe results are convincing, DHNE is clearly outperformed on the benchmarks. Depending on the ease of training the model compared to DHNE, this work might be a useful contribution for the ICLR community.\n\nOther points\n\nThe related work section should be expanded by discussing relationships to:\n\nBai, Song, Feihu Zhang, and Philip HS Torr. \"Hypergraph Convolution and Hypergraph Attention.\" arXiv preprint arXiv:1901.08150 (2019).\n\nFeng, Yifan, et al. \"Hypergraph neural networks.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.\n\n------------------\nPost-rebuttal\n------------------\n\nI would like to thank the authors for their in-depth response. I found the additional analysis on the dynamic node embeddings insightful and it is reassuring that it confirms the claims. Overall I think the rebuttal addresses my major concerns and I will raise my score.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}