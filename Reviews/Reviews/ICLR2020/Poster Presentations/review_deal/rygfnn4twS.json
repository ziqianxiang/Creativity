{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a network quantization method which is based on kernel-level quantization. The extension from layer-level to kernel-level is straightforward, and so the novelty is somewhat limited given its similarity with HAQ. Nevertheless, experimental results demonstrate its efficiency in real applications. The paper can be improved by clarifying some experimental details, and have further discussions on its relationship with HAQ.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #4",
            "review": "Summary\nThis paper proposes a network quantization method. Different from previous methods focusing on network-level or layer-lever quantization, this work pays attention to kernel-level quantization. Specifically, they use a hierarchical reinforcement learning framework to search in the search space related with quantization. The experiment result validates the significance of the work.\n\nStrength\nThe paper provides us with a new insight into network quantization. Even though the extension from layer-level to kernel-level is straightforward, the improvement is significant and meaningful. The experiment result demonstrates its efficiency in real applications.\n\nWeakness\n1. The algorithm of the paper is similar with previous work HAQ, which is based on DRL to guide the search procedure. Thus, the novelty of algorithm is somewhat weak.\n \n2. For kernel-level quantization, this paper proposes a hierarchical DRL method. However, I didn't see the importance of the hierarchy. The author may discusses more about this and compare it with flatten algorithms for ablation study. \n\n3. The paper didn't compare their methods with other baselines on the same level. I think some algorithms can be applied into kernel-level quantization directly. Based on the same level comparison, the efficiency of your method can be seen more directly. \n\n4. The description was not written well. For example, the detail of the experiment and the hardware settings are unclear.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper proposed a method for network quantization. Similar with the work of \"HAQ: Hardware-Aware Automated Quantization with Mixed Precision\"(CVPR 2019), the proposed method is based on reinforcement learning. The contribution of the work is on the kernel-wise quantization, i.e., assigning different bitwidth to different kernels in one layer. And in the experiments, the proposed method clearly outperformed the state-of-arts of network-wise and layer-wise quantization methods.\nAlthough the high level idea is presented very well, some essential parts of the paper is a little bit hard to follow.  The motivation of using the hierarchical DRL is unclear.\n\nQuestions:\n\nWhy a hierarchical DRL agent is desired for kernel-wise quantization? Is it possible to modify the definitions of state and action of HAQ for kernel-wise quantization, which seems to be a much simpler solution for the task?\n\n\nWhat is the definition of iRd [L_i,K_j] in the Intrinsic Reward?\n\n\nIt seems the original HAQ used simple quantization way instead of LQ-Nets, which is different with the experiment setting in this paper. If I'm correct, does the change affect the HAQ's performance for fair comparison?\n\n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "\nSummary of the work\n\nThis paper proposes to automatically search quantization scheme for each kernel in the neural network. Importantly, due to a large amount of search space, hierarchical RL was used to guide the search.\n\nStrength\n\nFirst of all, I really liked the idea about more detailed search for kernel-wise configurations. It is interesting to see that different kernel settings has different best bitwidth. More importantly, it is great to provide cost model based estimations for cost/latency.\n\nThe authors also provided detailed discussion about weight bit distributions and average quantization bits among the layers. \n\nThe author provided implementation on real world FPGAs using bit-serial spatial architecture, which plays particularly nice with the algorithm. This something that should be highlighted in the discussion.\n\nWeakness\n\nFirst of all, I would love to see more detailed discussion about the corresponding hardware support implications. In particular, given the need of re-using computing resources, kernel-wise quantization has to use the bit-serial version of architecture(otherwise the MAC cannot be reused and we have to layout the entire network on FPGA), which may limit the applicability of the methods to higher number of bits.\n\nThe second potential weakness is the close relation between the method and HAQ. The method feels like a straight-forward extension(with DRL added). What would happen if you directly apply HAQ’s method to the kernel-wise search space?\n\nFinally it would be great if the authors can clarify more about the FPGA setups(number of accumulators being used, whether there is reuse of compute unit in FPGA, or did you just layout everything spatially).\n\nQuestion:\n\nHow do you handle layers like BatchNorm(which normally need floating pt)?\n\nOverall, I find this paper provides interesting insights and solid evaluation and should be accepted to ICLR.\n\n\n---\ni have read the authors' response. \n\nPlease do note that because the different kernel-wise precision, the accelerator has to resort to bit-serial computation, which somewhat limits the structure of the accelerator. e.g. we cannot simply build 8bit MAC along with 4 bit ones, but have to use 4bit ones to bit-serially accumulate the 8 bit ones. This will somewhat limit the applicability of the model, and i think the author should add a discussion section to the paper about this limitation.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "title": "Official Blind Review #1",
            "review": "This paper proposes a new method for quantizing neural network weights and activations that uses deep reinforcement learning to select the appropriate bitwidth for individual kernels in each layer. The algorithm uses a reward function that weights accuracy of the quantized model with latency, energy, and FPGA area, and leverages a high level and low-level controller to create quantized models that can take into account these factors. Compared to prior approaches taht only perform layer-wise instead of kernel-wise quantization, the quantized models can achieve better performance, or latency.\n\nWhile the problem of more effectively quantizing neural network weights and activations is interesting, I found this paper hard to follow and evaluate. The proposed hierarchical deep RL method mentions a number of tricks and design decisions that are not ablated, and the notation and text around the method were difficult to follow. There were also no baselines comparing to other approaches for performing kernel-wise quantization (e.g. random search, quantize based off variance, etc.). Without improved baselines and text I would recommend rejecting this paper.\n\nMajor comments:\n* Based on the analysis of kernel-wise results, it seems that a very simple strategy that chooses QBNs based on weight variance could be sufficient to achieve good performance. However, there’s no comparisons to these kinds of heuristics or even to random search over QBN per-kernel. There are also no ablations that study whether the hierarchical-RL based approach beats a baseline that just directly predicts the kernel-wise QBNs independently.  Without these baselines, it’s hard to know how well AutoQ works.\n* The text that details the hierarchical RL approach with multiple controllers that is core to the new AutoQ approach is extremely hard to follow, with many undefined symbols.\n\nMinor comments:\n* FIg 1: where in the neightwork do these weight kernels come from? What network/dataset?\n* You repeatedly state that ML experts obtain only sub-optimal results at selecting QBNs, can you cite something for this? What if you give experts access to additiional information like visualizations of weight distributions?\n* What’s the tradeoff between using different QBNs for each kernel and specifying this number? Is there additional memory overhead?\n* Table 1: hard to read the exponent in kernel-wise math, add parentheses\n* Please use \\citep vs. \\citet where appropriate\n* Why not also search for kernel-specific activation quantization? \n* The notation in Fig 3 and surrounding text is really hard to follow, e.g. w_w, h_w, and indices into states (roundup could be ceil, names are confusing iRd, eRd?).\n* Eqn 2 comes out of nowhere… why optimize for log(accuracy) vs accuracy? How do you choose the user-defined constants? Decay factor?\n* Hardware overhead estimator: are these models accurate independent of QBN?\n* Why do you have to perform Gaussian augmentation? I didn’t follow the re-labeling of transitions after Eqn. 6, is this needed?\n* Define \\mu when you first use it (Eqn 4?)\n* Is \\delta_a in implementation details the same as \\sigma around Eqn 4?\n* Table 3: Is there variance in these results based off fine-tuning? Could you include error bars or stderr on these estimates?\n* Figure 6: are the search spaces different for these different approaches? I.e. does AutoQ perform better due to the upper/lower bounds you set on QBN?\n\n=============\n\nThank you to the authors for addressing many of my concerns. The updated paper still does not present any baselines for methods that can search or learn kernel-wise quantization parameters, e.g. run random search and select the best-performing models according to your reward function. Additionally, there were no revisions made to the text to improve the presentation, and thus I continue to recommend rejection of this work.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}