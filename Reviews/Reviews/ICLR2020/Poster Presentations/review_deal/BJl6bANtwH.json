{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper presents an ensembling approach to detect underdetermination for extrapolating to test points. The problem domain is interesting and the approach is simple and useful. While reviewers were positive about the work, they raised several points for improvement. The authors are strongly encouraged to include the discussion here in the final version.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents local ensembles, a method for detecting underdetermination when extrapolating to test points. The authors define an extrapolation score which is used to estimate the standard deviation of predictions at test points. The extrapolation score is chosen to represent the variability in predictions that would be generated by models with similar training loss. By considering the eigenvectors of the Hessian that are associated with minimum eigenvalues the directions of the loss surface with minimal curvature are found, and perturbations of the parameters in the subspace of minimal curvature correspond to models with similar training loss. \n\nThe authors show that their extrapolation score is proportional to the first order approximation to the change in prediction under a perturbation of the parameters with minimal change in loss. In practice the minimum eigenvalue/eigenvector pairs are computationally challenging to compute for large matrices. For this reason the subspaces with minimal change in loss are computed by finding sets of vectors that are mutually orthogonal to the eigenvectors associated with dominant eigenvalues of the Hessian. \n\nThe method is validated experimentally first through out-of-distribution detection on synthetic data. The authors then test performance by constructing a \"blind spot\" by generating features that are a linear combination of existing features. Data can be generated as either within or out of distribution and the AUC metric can be applied to test model performance. In the final experiment the authors demonstrate the use of local ensembles in active learning. By determining which of the training samples are in the model's blind spots at each iteration and training based on these examples rather than randomly selecting training examples rates of convergence can be increased. \n\nI vote to accept this paper as the proposed local ensemble method builds on a growing body of literature regarding loss-surface inference, providing a new way to connect the shape of the loss surface to extrapolation detection. The theoretical result showing the first order relationship between the standard deviation of extrapolation predictions and perturbations in solutions is a useful insight. \n\nThere are some points that should be addressed for clarity however. Firstly the proof of proposition 1 should be made clearer. This is central to the work of the paper and a more full treatment of the proof here could help illuminate some intuition about the connection to perturbations and variance of predictions. \n\nThe other main point that is not addressed is that in principal we aim to find the subspace associated with minimal eigenvalues, but in practice this is computationally prohibitive. Therefore a space that has a basis that is mutually orthogonal to the dominant eigenvectors is sought (the found subspace), and this could have minimal relation to the subspace that is actually sought (the optimal subspace). Some experimentation showing how the found subspace relates to the optimal subspace would be informative, as well as how sensitive the results are to how much the found and optimal subspaces differ.\n\nSome minor points:\n- Many of the plots lack axis labels, although many are explained in the captions the figure labeling needs to be improved\n- Some explanation about the choice of AUC as a metric would be informative and could help connect to the initial motivation of the method\n- Experiment details should be given in the main body of the paper rather than the appendix; i.e. in section 5.2 it is only explained that a \"neural network\" is trained, the architecture should be specifically given alongside the discussion of the experiment\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper focusses on underdetermination as being key to extrapolation. In the case of pretrained models, the model extrapolates on a test input if the prediction at this input is underdetermined or multiple predictions are equally consistent with models characterized by similar architecture and loss functions.\nThe underdetermination in the case of over-parameterized model classes as in deep neural networks is used here as a way of detecting extrapolation.\n\nThe authors define an extrapolation score for trained models on unlabelled test point by measuring variance of predictions across an ensemble selected from local perturbations on trained model parameters that fit the training data well or having similar training loss.\n\nThe score approximates the variance of predictions by estimating the norm of the component of the test point’s gradient that aligns well with the low curvature directions of the Hessian, thus providing a tractable quantity in quantifying uncertainty in predictions. The motivation is that if the models have been trained to a local minimum or saddle point, then parameter perturbations in flat directions (small eigenvalues of the Hessian) do not change the training loss substantially.  These models with small perturbations on the flat regions then form the local ensemble for measuring the extrapolation and predicting on out of distribution samples, spurious correlated samples and for  active learning on uncertain data.\n\nThe authors prove that the extrapolation score is proportional to the standard deviations of predictions across a model ensemble with similar training loss. The math in the derivation checks out.\n\n \n\nOne of the novel contributions of the paper is in using computationally cheap post-hoc local ensembles over fully trained ensembles in the baselines that require complicated training procedure. The other key differentiation over baselines is their method leverages the ill conditioned Hessian where  the baselines struggle in requiring an inverse of that ill conditioned Hessian.\n\nThe limitations of their method is in the determination of sufficiently small eigenvalues from the ensemble subspace. Further, the sensitivity of the small set of eigenvalues towards overestimating the prediction’s sensitivity to loss preserving perturbations and being less sensitive to some other under-constrained directions.\nBelow are the potential places where more clarity will help:\nIt would have been good to see a way of measuring the sensitivity in the set of small eigenvalues determination. I urge the authors to think of a way  of quantifying this sensitivity if possible especially since the model class is low dimensional.\n\nIn the experimental section with label, class prediction task, how correlated are the confounders Eyeglasses and Hat? What happens if the models are allowed to train for a longer; does the inconsistency in the behavior of AUC over more eigenvalues change?\n\nIn section 5.4, I think a comparison with the Resampling Under Uncertainty baselines is imperative. \nThere is a typo in E.4 label Attribute->Attractive.\n\nThere is lack of clarity in how similar the models are during training. Although, the ensemble is used post-hoc, its unclear if the models during training differ in initialization only? \n\nWhat are the implications of the method in the case of finetuning models especially is the training data available for fine-tuning is low. \n\nFurther, is there any notion of how the method scales with increasing depth in the neural network models? A comparison with larger test set and models trained on deeper architecture such as ResNet and the like will be interesting to see.\n\nWith noisier data and the inconsistencies in the expected behavior of the method, is there a way of quantifying the amount of noise and the extrapolation? I do see the empirical experiments demonstrating this but some more insight into this is perhaps important.\n\nOverall, its an extremely well written paper with great clarity. The method described by the authors is well differentiated from the baselines in making the clever use of the projection of the ill conditioned Hessian on the low curvature directions of the test point’s gradient. \n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "# Summary of contribution\n- The paper provides a novel fast and simple approximation of second-order local parameter sensitivity of neural networks, to estimate a form of uncertainty wrt to a test sample, which is further used and tested as a novelty detector. \n- The method analyzes the most significant eigenvector/eigenvalues of the Hessian (of training loss), and use the compliment of their span to get directions of local perturbations to network parameters that affect training loss little (\"ensemble subspace\"). The novelty score is then based on how much the prediction is influenced by these perturbations.\n- The idea of estimating \"ensemble subspace\" is interesting and computationally effective. Compared to other recent methods that also use second-order gradients for uncertainty, this paper is more generally applicable, and can be faster at test time. The paper demonstrates good performance on both simulated data and real data (CelebA faces with CNN, etc.).\n\n# Decision TL;DR\nI am giving a weak reject. The paper is strong in its idea, formulation, and theory, but is too similar to recent related works which this paper is reluctant to compare to (either in theory, efficiency, or performance). Since the contribution of the paper lies in the efficient approximation of local ensemble methods, readers cannot gauge how beneficial the contribution is compared to other approximations of ensembles.\n\n\n# Pros\n- Novel way to estimate a local neighborhood that affects training loss little (Note: not an expert in this line of research, not sure if it is completely novel) by estimating significant eigenvectors and using their compliment space.\n- The paper is well-written, and relatively easy to understand, despite a few hard-to-follow spots\n- Widely applicable post-hoc to any trained neural networks, and potentially faster training than ensembles / Bayesian approximated ensembles\n- (Theoretical) stability compared to full Hessian inversion\n\n# Cons\nMotivation wrt other papers unclear, and a lack of comparison.\n- Two of the cited papers (Gal & Ghahramani, 2016; Blundell et al., 2015) both work on local ensembles. The former uses MC dropout, the latter estimates a diagonal covariance of a Gaussian distribution of network parameters. These methods are not mentioned in the motivation or related work, which makes it hard to say this paper is well-placed in the literature.\n- The reason that these methods are not compared to is insufficient. The paper only argues that they are not \"post-hoc\" methods. It is very unclear why in any circumstance (or use case) a post-hoc estimation of local ensemble must be (or is preferred to be) used, rather than having network parameters and local neighborhood jointly estimated. If it is for efficiency reasons, the paper does not provide any experimental comparison of the efficiency. Also, it is hard to argue that the \"post-hoc\" nature of this paper makes it so different from the two prior work. For the first prior work, the only time it is not post-hoc is when the original network does not have any dropout layer, and that circumstance is not very common. For the second prior work, one can easily make it post-hoc by training the network first, and estimate the diagonal covariance post-hoc using their loss. \n- The advantage of this paper is that it is more efficient and stable than alternatives, but only  the full hessian inversion is discussed. In particular, it may be necessary to discuss this paper's efficiency against MC-dropout (Gal & Ghahramani, 2016). This method can be done in mini-batches, while the proposed method has to run forward and back-propagation separately for each sample to get gθ*(x'), and it is unclear how well that scales.\n\nEfficiency analysis lacking\n- As discussed above, the proposed method seems to need to back-prop for each test sample separately without using a batch. How much this affects test efficiency is unclear.\n\nExperimental comparison with similar methods missing.\n- The paper would benefit from comparing to the two cited papers (among which MC-dropout is so easy to implement) as well as a full hessian estimation (for toy datasets at least).\n- The paper poses itself as an efficient alternative, so it would be essential to gauge experimentally how fast each method is.\n\nOthers. (not crucial issues) \n- The performance of the paper's main method (LE w/ predictions) underperforms in Table 2, and a variant had to be proposed to make up for the performance drop. This suggests instability of the proposed method wrt new datasets.\n- The claim in contribution \"We identify underdetermination as a key factor in the unreliability of predictions\" is not verified.\n- Inability to scale up to large networks with larger m needed, compared to real ensemble methods or Bayesian networks with Gaussian distributions.\n\n\n# Room for improvement (decreasing order of importance)\n- Improve placement in the literature by discussing when this paper is more useful than prior work (Gal & Ghahramani, 2016; Blundell et al., 2015).\n- Detailed theoretical or experimental analysis of efficiency against alternative approximations of ensembles.\n- Performance comparison to ensemble and local Bayesian methods.\n\n# Editorial issues\n- Figure 3(c) x axis meaning unclear\n- Figure 4 not mentioned in text, and unclear which experiment this refers to \n- Table 2 experiment's loss gradient version is not explained.\n\n\n#############################################################\nPOST REBUTTAL\n#############################################################\n\nTL;DR: The rebuttal addresses some but not all of my concerns. The MC-dropout comparison especially shows the difference between some approximate ensemble methods and ensembles that specifically changes the loss little (this paper). In the end, it is a good paper in terms of theory, although the experiments is lacking in crucial places (no comparison with many existing papers that do attempt to invert hessian) and lack of analysis of claimed efficiency, which I can only hope don’t turn out to be a big deal. I am increasing the score, to marginally above borderline, but please consider the following feedback for the camera-ready version.\n\n> MC Dropout and Bayes by Backprop effectively measure different types of uncertainty from what our method targets, and so they should not be considered competing methods.\nI disagree; by the same logic we can never compare SVM with random forest because they are so different.\n\n> However, as we discussed in 4.1, small eigenvalues make turning this representation into a proper posterior distribution difficult, because these eigenvalues need to be inverted to obtain a covariance matrix, but these inverted eigenvalues can be numerically infinite.\nI agree; but the main difference is the full hessian is hard to implement while there are papers that have implemented the diagonal hessian. It would benefit the paper to compare to that and prove the issue of instability on top of arguing theoretically.\n\n> First, we clarify that our method can in fact be performed using mini-batches of test points (both the forward and backward passes).\nAs far as I know, with mini-batches, the gradients averaged over all samples in the mini-batch are computed for each network parameter. But this method needs the gradient wrt each sample separately.\nIf the authors have some implementation trick that allows computing network parameters’ gradients wrt each sample in the mini-batch separately, please include in the implementation details. Otherwise, please mention this drawback in any “fast” claim.\n\nI will change the score upwards due to the informative rebuttal. Please include much of the discussion in the paper or appendix.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}