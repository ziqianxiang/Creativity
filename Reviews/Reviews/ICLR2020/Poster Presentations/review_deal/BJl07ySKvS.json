{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The paper consider the problem of program induction from a small dataset of input-output pairs; the small amount of available data results a large set of valid candidate programs.\nThe authors propose to train an neural oracle by unsupervised learning on the given data, and synthesizing new pairs to augment the given data, therefore reducing the set of admissible programs.\nThis is reminiscent of data augmentation schemes, eg elastic transforms for image data.\n\nThe reviewers appreciate the simplicity and effectiveness of this approach, as demonstrated on an android UI dataset.\nThe authors successfully addressed most negative points raised by the reviewers in the rebuttal, except the lack of experimental validating on other datasets.\n\nI recommend to accept this paper, based on reviews and my own reading.\nI think the manuscript could be further improved by more explicitly discussing  (early in the paper) the intuition why the authors think this approach is sensible:\nThe additional information for more successfully infering the correct program has to come from somewhere; as no new information is eg given by a human oracle, it was injected by the choice of prior over neural oracles.\nIt is essential that the paper discuss this. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper handles the challenge of generating generalizable programs from input-output specifications when the size of the specification can be quite limited and therefore ambiguous. When proposed candidate programs lead to divergent outputs on a new input, the paper proposes to use a learned neural oracle that can evaluate which of the outputs are most likely. The paper applies their technique to the task of synthesizing Android UI layout code from labels of components and their positions.\n\nThe experiments compare the method against the baseline InferUI. To summarize the results, we can see that the proposed method in the paper can perform about as well as existing hand-crafted constraints that guide the search process of the previous work, when training an oracle on the dataset with negative examples created by noising the positive examples.\n\nOne limitation of the method is that it would works best when there is a clear latent structure behind the outputs produced by the correct program, such as in the paper's target domain of generating UIs where there are clear aesthetic rules and design guidelines that make it possible to evaluate which output is most preferred. For other domains, it may be more important to evaluate the candidate program together with its output, which would make it similar to a re-ranking approach.\n\nI believe this paper presents a novel and insightful approach to creating programs from imprecise specifications. Therefore, I vote to accept the paper.\n\nSome questions for the authors:\n- How big was $\\mathcal{I}_i$ in the supervised dataset $\\mathcal{D}_{S+}$? Was it always 3?\n- I wasn't able to find any evaluation of a model trained on $\\mathcal{D}_U$, did I miss it in the paper?"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "[Summary]\nThis paper aims to improve the generalization of program synthesis, ensuring that the synthesized programs not only work on observed input/output (I/O) examples but also generalize well to assessment examples (i.e. model the real intent of the end-user). To this end, the paper proposes a framework that iteratively alternates between producing programs with an existing program synthesizer and augmenting examples to disambiguate possible programs with a neural oracle that learns to select correct outputs. Several architectures and the design of input of the neural oracle have been investigated. The experiments on Andriod layout program synthesis with an InferUI synthesizer show that the proposed framework can improve the generalization of synthesized programs. However, I find it is difficult to evaluate the effectiveness without sufficient qualitative results and the intermediate outputs (e.g. a distinguishing input and candidate outputs) of the proposed framework (see details below).\n\nSignificance: are the results significant? 4/5\nNovelty: are the problems or approaches novel? 4/5\nEvaluation: are claims well-supported by theoretical analysis or experimental results? 3/5\nClarity: is the paper well-organized and clearly written? 4/5\n\n[Strengths]\n\n*motivation*\n- The motivation for improving the generalization of program synthesis by augmenting examples is convincing.\n\n*novelty*\n- The idea of utilizing a neural network to select correct outputs to augment examples for disambiguating the possible programs is intuitive and convincing. This paper presents an effective way to implement this idea.\n\n*technical contribution*\n- The paper investigates a set of network architectures and ways to specify the network input for learning the neural oracle. The RNN+CNN model that leverages both rendered views and features seems effective.\n\n*clarity*\n- The overall writing is clear. The authors utilize figures well to illustrate the ideas. Figure 1 clearly shows the proposed framework.\n\n*experimental results*\n- The presentations of the results are clear. The results demonstrate that the proposed framework can improve generalization accuracy.\n\n*reproducibility*\n- Given the clear description in the main paper and the details provided in the appendix, I believe reproducing the results is possible if the dataset is available. \n\n[Weaknesses]\n\n*related work*\nThe descriptions of the related work are not comprehensive. Some neural program synthesis works explore a variety of mechanisms to encode examples and fuse their features, which are not mentioned in the paper. [Devlin et al. in ICML 2017] investigates different attention mechanisms to sequentially encode a set of I/O examples and performs pooling to merge them. [Sun et al. in ICML 2018] proposes a doubly encoding method to capture more details of examples and merge the features using a relation network. I believe it would be interesting to see if these methods could further improve the performance of the neural oracle.\n\n*experiment setup*\n- The experiments are not sufficient. While the claims look promising, the proposed method is only evaluated in only one dataset, which is not sufficiently convincing. I suggest the authors to also experiment the FlashFillTest dataset where string transformation programs are synthesized. \n- A more comprehensive description of the dataset is lacking. \n\n*experiment results*\n- I find it hard to judge the effectiveness of the proposed framework without seeing sufficient qualitative results. I suggest the authors randomly sample some synthesized programs (both success and failure) and present them in the paper.\n- I believe it is important to present some examples of the given I/O pairs, initially synthesized programs (p_1), found distinguishing input (x*), candidate outputs (y), the prediction of the neural oracle (i.e. selected outputs), the augmented examples (I \\cup {(x*, y*)}), and finally the next synthesized program. Without this, it is very difficult to understand the performance of the proposed framework and what could go wrong. \n\n*ablation study: the neural oracle*\nOnly the final performance (i.e. the program synthesis performance) is shown in the paper. I believe it would be helpful if the performance of the neural oracle was also presented. As the whole framework depends on how accurate the neural oracle can select the correct output, it is important to evaluate this. One way to show this is to simply show the performance of all the neural oracles (with different architectures) trained on D_S (the positive samples and the incorrect samples) or even D_{S+}.\n\nDevlin et al. \"RobustFill: Neural Program Learning under Noisy I/O\" in ICML 2017\nSun et al. \"Neural Program Synthesis from Diverse Demonstration Videos\" in ICML 2018"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "= Summary\nA method for a refinement loop for program synthesizers operating on input/ouput specifications is presented. The core idea is to generate several candidate solutions, execute them on several inputs, and then use a learned component to judge which of the resulting input/output pairs are most likely to be correct. This avoids having to judge the correctness of the generated programs and instead focuses on the easier task of judging the correctness of outputs. An implementation of the idea in a tool for synthesizing programs generating UIs is evaluated, showing impressive improvements over the baseline.\n\n= Strong/Weak Points\n+ The idea is surprisingly simple and applies to an important problem in program synthesis.\n+ The experiments show that the method works very well in UI-generation domain\n- The paper repeatedly claims general applicability to program synthesizers, but is only evaluated in the specific domain of UI-generating programs. I have substantial doubts that the approach would work as well in the domains of, e.g., string manipulation, Karel, or data structure transformations. My doubts are based on the fact that there are easily generalizable rules for UIs (no overlaps, symmetry, ...), whereas other domains are less easily described. This creates a substantial gap between paper claims and empirical results.\n- The writting is somewhat sloppy (see below), which makes it sometimes hard to understand. Names such as \"views\" are used without explanation, and it's not explained how a device is an input to a program (yes, I get what this means, but it makes in unnecessarily hard to follow the paper)\n\n= Recommendation\nI would ask the authors to rewrite their paper to make less general claims, but believe that the general idea of judging the correctness of a program (or policy) by evaluating it on different inputs is a powerful concept that would be of substantial value to the wider ICLR audience. Improving the readability of the paper would make me improve my rating to a full accept.\n\n= Minor Comments\n* page 1, par \"Generalization challenge\": The second sentence here is 4 lines long and very hard to follow. Please rephrase.\n* page 2, par 2: \"no large real-word datasets exists\" -> exist\n* page 2, par 3: \"even when both optimizations of InferUI are disabled\": at this point, the reader doesn't know about any optimizations of InferUI.\n* page 4, par 1: \"i.e., $\\exists p \\in \\mathcal{L}$\" - $\\mathcal{L}$ is undefined here (will be defined later on the page)\n* page 4, par 2: \"Generate a candidate program $p_1 \\models \\mathcal{I}$\" - in step 2, there are suddenly also $p_2 \\ldots p_n$, which are never explicitly generated. Either adapt this step, or explicitly generate them in step 2 based on the distinguishing input\n* page 7 par 2: \"We use one screen dimension as the input specification $\\mathcal{I}$, the second as the distinguishing input\" - this confused me, as the paper discussed discovering the distinguishing input (page 4, paragraph \"Finding a distinguishing input\"), whereas it sounds here like that input is manually selected.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}