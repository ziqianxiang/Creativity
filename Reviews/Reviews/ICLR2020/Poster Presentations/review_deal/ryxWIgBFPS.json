{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper proposes to discover causal mechanisms through meta-learning, and suggests an approach for doing so. The reviewers raised concerns about the key hypothesis (that the right causal model implies higher expected online likelihood) not being sufficiently backed up through theory or through experiments on real data. The authors pointed to a recent paper that builds upon this work and tests on a more realistic problem setting. However, the newer paper measures not the online likelihood of adaptation, but just the training error during adaptation, suggesting that the approach in this paper may be worse. Despite the concerns, the reviewers generally agreed that the paper included novel and interesting ideas, and addressed a number of the reviewers' other concerns about the clarity, references, and experiments. Hence, it makes a worthwhile contribution to ICLR.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #5",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper proposes a method of discovering causal mechanisms through meta-learning, by assuming that models transfer faster if their causal graph is correct.\n\nFor a possible causal graph, for adaptation to one interventional dataset, the samples are iteratively revealed and the log likelihood of the next sample is measured, after which the parameters are updated using one step of gradient descent on that sample. The sum of these log likelihoods is the ‘online likelihood’ and a measure of speed of adaptation. The parameters are initialised using maximum likelihood estimation on the train dataset.\nThe meta learning procedure then at each episode samples an interventional distribution and an interventional dataset from that distribution. It then performs a gradient based update of the belief over graphs based on the difference between the online likelihoods of each graph on that dataset.\n\nThe meta-learning approach appears to be a novel contribution. The authors provide a theoretical argument by counting ‘effective parameters’ to suggest why models using the right causal model obtain a higher online likelihood. Additionally, they prove that the gradient updates to the graph belief are easy to compute and converge. The method is validated with several synthetic experiments which discover the direction of the arrow between two random variables, each either continuous or discrete. Furthermore, they successfully experiment with the combination of learning a representation of a raw data to two random variables with learning the causal direction. The paper is very well written and most claims are carefully proven.\n\nI recommend a weak rejection for this paper, because:\n1) The empirical validation is not strong enough, as no real dataset is used, only toy datasets. The toy experiments themselves could also be more extensive.\n2) I am unconvinced of two of the theoretical claims made: (A) the fact that the expected gradients in Prop 1 are 0, implies that the right causal graph has better online likelihood and (B) that the method is easily extensible to more than two random variables [Appendix E].\n\nSupporting arguments:\n1.1) Fig D.1 suggests that, in the experiments using continuous random variables, the training dataset alone is sufficient to discover the true causal model, under the assumption of independent additive noise, as is done in e.g. [1]. I find it plausible to believe that the training curve on the training dataset alone already makes it possible to disambiguate the causal from anti-causal model. A similar pattern is shown by the authors themselves in Appendix B on discrete variables.\nFor finite training data, the models are distinguishable, while for infinite training data they are not [Fig B.1]. The exact same holds for finite and infinite interventional data [Fig 1].\nAre these experiments then good benchmarks for causal discovery based on intervention when the causal model can already be inferred from the non-interventional training data?\n1.2) The simplicity of the representation learning setup doesn’t convince that the method is applicable to more real-world settings with more complicated encoders. Additionally, some important details on this experiment are missing (see below).\n1.3) All experiments show the effect of intervention on the cause p(A). No experiments are given for intervention on p(B|A). Does the method then still work?\n1.4) No experiments for more than two random variables are performed in this paper.\n2.A) Prop 1 shows that the expected maximum likelihood gradient for one conditional probability distribution is zero if the graph is correct and that CPD is not intervened on. Subsequently a claim is made that this effectively reduces the number of parameters and thus that adaptation is faster. However, the zero-expectation gradient may still be non-zero and even large on the small intervention sample. It is unclear to me why they therefore can be excluded in the number of parameters. Furthermore, whether the online likelihood is large will depend not only on generalisation, but also on the training convergence, since not empirical risk minimization is used, but SGD with a fixed number of steps. Thus, even though the authors prove that the method will converge to the causal graph with lowest online likelihood, it is unclear why this is necessarily the correct causal graph.\n2.B) In appendix E it is mentioned that cycles can occur in causal models. However, it is unclear why factorization (76) is still correct in the cyclic case. Perhaps I am misunderstanding, but it seems to me that p(x) = \\prod_i p(x_i | x_{Pa_i}) only makes sense for a DAG. Hence, how would the online likelihood be computed for cyclic models?\n\nSuggestions for improvement:\n- Could you explain in what realistic settings we would have access to data from a large number of different interventional distributions?\n- Could you show a plot similar to Fig 1, but with online likelihoods? Such a plot may be more indicative of the ideal episode length than Fig 1.\n- Could you provide details on the representation learning experiment? In particular: (1) is \\theta_D different in the train dataset and each interventional dataset? (2) How do the gradients of theta flow through the meta-update steps?\n- A reference to Appendix B appears missing in main text.\n\n[1] Nonlinear causal discovery with additive noise models. Hoyer et al 2009"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "In this work, the authors proposed a general and systematic framework of meta-transfer objective incorporating the causal structure learning under unknown interventions. Under the assumption of small change (out-of-distribution data), the work mainly focuses on the theoretical and empirical analysis of relations on two random variables in causal graphs (causal and anti-causal directions), so that a differentiable regret function using the joint distribution of the small \"intervention\" dataset can be built. \n \nThe motivation is to adapt or transfer quickly by discovering the correct causal direction and learning representation based on it. The idea of disentangling the marginal and conditional factors to reduce the sample complexity and thus achieve fast adaptation is novel and insightful. Proposition 1 and its proof provide the theoretical supports on this point very well. The structure causal model is parametrized and then optimized in a meta-learning procedure. Experiments on simulated data under categorical or continuous distributions can verify the efficiency of inferring causal graphs. \n \nHere are some concerns about the proposed algorithm:\n \n1). When the authors discussed small change, there is no formal (mathematical) definition on it. For instance, an invertible function could be one of the properties given for the out-of-distribution data. The example (rotation) in Fig. 3 works to some extent because the small transformation is invertible. Also, the intervention seems simple in the work, for example, the rotate angle (a value) in Fig. 3 only involves one parameter dimension. In this case, learning an encoder to infer the correct causal relation is not that difficult. Is there possible that the encoder cannot learn a good enough theta to find the correct causal direction? It would be nice if the limitations of using causal graphs are discussed.\n \n2). Given a direction A causes B, the experiments are conducted by performing interventions on the cause A. How about to put an intervention on the effect B? According to the algorithm analysis (Table D.1), for the discrete bivariate model, the parameter dimension of a correct structure becomes N^2, while the one of an incorrect structure becomes N + N^2. Compared to intervention on cause, the reduction of sample complexity here is not that obvious. A general discussion on the effect intervention for bivariate models would be helpful. \n \n3). The work opens a new direction of inferring causal relationships together with representation learning, which has the potential for more out-of-distribution scenarios. While the authors claim that it is the first step, the current empirical studies for structure models use synthetic data with relatively constraint assumptions. It is highly recommended for the authors to provide discussions about real-data tasks with neural causal models in future work.      \n"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "\nSummary: \nThe paper first shows that, in a very simple two-variable task, the model with the correct underlying structure will adapt faster to a causal intervention than the model with the incorrect structure. This idea is used to develop a “meta-transfer” objective function for which gradient ascent on a continuous representation of the model structure allows learning of that structure. The paper shows that optimizing with respect to this objective with a simple model is guaranteed to converge to the correct structure, and also presents experimental results on toy problems to demonstrate.\n\nOverall: Accept.\nI really enjoyed reading this paper. It is clear, well-motivated, well-written, does a good job of connecting to related work, and presents an interesting method for structure learning. While the experiments are quite toy and questions about how well this will work in more complex models with many variables remain largely unaddressed, these do not detract much from the paper for me. Instead, the paper does a good job of motivating its contribution and exploring its effect in simple intelligible tasks, and I feel I got more out of this paper than most SOTA papers.\n\n\nClarity: Very clear.\nSignificance: Potentially quite significant as this is starting to bring causal structure learning into the realm of tensorflow and pytorch.\n\nQuestions and comments:\n- All else being equal, the speed of adaptation between two very similar models will serve as a good proxy, as shown in this paper. However, I can easily imagine scenarios where the two models one wants to differentiate between are quite different, and have very different optimization landscapes. Here, the speed of adaptation will be quite dependent on these landscapes and not just on the underlying model structure. Do you have thoughts about how this can be extended to such a scenario?\n\n- The parameter counting argument is not nearly so strong if what actually changes is the conditional p(A|B). In that case, the sample complexity for the correct model would be N^2 = O(N^2) and for the incorrect model would be N + N^2 = O(N^2). Does the objective still work here? Would be great to add an additional experiment showing the results in this case.\n\n- Doing an intervention and drawing a new D_int for each step of gradient descent seems quite prohibitive in a lot of domains. Are there ways to decrease this burden?\n\n- In Figure 2, can you speak to why the N=100 curve for the MLP parameterization converges more slowly than the N=10 curve? I would still expect more data to be beneficial here.\n"
        }
    ]
}