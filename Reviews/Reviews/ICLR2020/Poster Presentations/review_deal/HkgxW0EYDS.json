{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The paper describes a simple method for neural network compression by applying Shannon-type encoding. This is a fresh and nice idea, as noted by reviewers. A disadvantage is that the architectures on ImageNet are not the most efficient ones. Also, the review misses several important works on low-rank factorization of weights for the compression (Lebedev et. al, Novikov et. al).   But overall, a good paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have published one or two papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #4",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "The paper introduces an end-to-end method for the neural network compression, which is based on compressing reparametrized forms of model parameters (weights and biases). Shannon-style entropy coder is applied to the reparametrizations.\n\nReparameterized forms are tensors stored in the compressed format. During inference, they are uncompressed into integer tensors and transformed via parameter decoders into weights/biases of convolutional and dense layers.\n\nDuring training, model parameters are manually partitioned into groups, and within a group they are considered as samples from the same learned distribution. Similarly, parameter sharing is introduced among the corresponding parameter decoders.\nLoss function, which is minimized during training is a sum of rate loss (self-information of all reparametrizations) and cross-entropy classification loss under reparametrization. A trade-off between compressed model size and model accuracy can be explored by varying a constant before the rate loss.\n\nDuring optimization, several tricks are applied. “Straight-through” gradient estimator is used to optimize the loss function over discrete-valued reparametrizations by means of stochastic gradient descent. Relaxation is used to obtain good estimates for both the rate term and its gradient during training.\n\nThe proposed idea is well-founded and intuitive. The proposed method is extensively evaluated on different classification network architectures and datasets. It provides good compression while retaining a significant portion of accuracy.\n\nIn the experiments, It'd be interesting to see a comparison on more efficient networks like MobileNets, ShuffleNets on ImageNet dataset. Also, I wonder whether under the same compression rate the proposed method outperforms DeepCompression (Han et al., 2015) in terms of accuracy? (for example, for LeNets and VGG-16)\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper presents a compression algorithm for neural networks. It uses a linear projection to map the weights and biases to a latent space where they are quantized using a learnt coding distribution. The method is simple yet it shows strong performance of a variety of compression benchmarks.\n\nOriginality: The core idea is related to (Balle et al. 2018), but the paper puts its own twist to it with the projections and applies it to model compression. It is certainly an interesting direction to explore.\n\nPresentation: The paper is well written. It is easy to read and understand. It properly cites prior works and contains all the technical details. I appreciate that the authors fit the paper into the recommended 8 pages.\n\nImpact: The method shows very strong performance in terms of compression ratio, but its unclear whether the compressed model can be used to speed up inference. Currently the main use case would be saving storage/bandwidth.\n\nQuestions:\n- In section 2.2, the paper talks about the form of the coding distribution. It has d components and l dimensions. How is d determined?\n- Section 2.1, How are the weights grouped for coding? is every filter its own group? If the experiments use different groups for different models, how is it decided which model uses which approach?\n- \\phi was fixed for the CIFAR10/ImageNet experiments. Could you provide further insight into why this choice was made?\n\nAssessment:\nWhile the idea is not groundbreaking, it is very well presented and evaluated and shows strong performance."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The authors propose a novel, information theoretic approach to learning compressed neural networks, whereby model parameters $\\theta$ are expressed in an (approximately discrete) latent space as $\\theta = f_{\\psi}(\\phi)$. During training, \"reparameterizations\" $\\Phi$ are treated as draws from a generative distribution $q(\\phi)$. As per the source coding theorem, penalizing the entropy of $q$ (more accurately, of $\\hat{q}$) directly promotes compressible choices of $\\Phi$ and, therefore, of $\\Theta$. After training, arithmetic coding of $q$ is used to compress $\\Phi$. Empirical results on a suite of common benchmarks demonstrates the proposed algorithm gracefully trades off between model performance and size.\n\n\nFrequency Domain Interpretation:\n  Here, I am operating off of the assumptions that, in Sect 3.2 paragraph 3, $\\phi_{w}$ should be $\\psi_{w}$. If so, can \"Our Method (DFT)\" also be interpreted as frequency domain SQ (of convolutional filters)? When trained on natural images, CNN filters are typically \"smooth\" (see any visualization thereof) and this smoothness translates as a prior of sorts on $\\Phi_{w}$. This insight was previously explored in [1, 2] for purposes of compressing CNNs. Does evidence of this manifest in your experiments? For example, do the empirical distributions of high- and low-frequency components of $\\phi_{w}$ differ?\n\n\nFeedback:\n  The authors propose a general framework where in (seemingly) arbitrary decoders $f$ can be used for purposes of \"reparameterization\". It is therefore somewhat disappointing that, in practice, $f$ is restricted to affine transformations and, in some cases, even fixed. Memory issues notwithstanding, it is unclear how well complex decoders $f$ could be jointly learned with (surrogate) encodings $\\tilde{\\Phi}$ and density models $\\tilde{q}$.\n\n  The lack of comparison between the proposed method's test-time throughput and that of its baselines leaves open questions regarding potential real-world use cases. Much of the time, methods for compressing neural networks divide their attention between both memory footprint and prediction speed. Outright ignoring this aspect of the problem seems odd. As a reviewer, I would much rather see the authors take a proactive approach in addressing this issue.\n\n\nQuestions:\n  - What do results look like if convolutional decoders $f_{\\text{conv}}$ are jointly learned during training?\n  - How sensitive is the algorithm to different groups assignments? Can assignments be reliably made by simply looking at the architecture?\n\n\nMinor Comments:\n  - The general notation of the paper is cumbersome at times, can $\\phi$ to be changed to, e.g., $\\tilde{\\theta}$? Similarly, can notation such as $\\theta_{k, W}$ be simplified as $W^{k}$?\n\n  - Section 2 jumps around a bit. Consider trying to order things \"chronologically\" such that training comes before compression? By the same token, details regarding \"Model Partitioning\" could be move to the end of the section such that the preceding material just refers to $\\theta$ more abstractly?\n\n  - Some additional details regarding training (esp. discretization and model distributions $\\tilde{q}$) would be appreciated.\n\n\n[1] \"Compressing convolutional neural networks in the frequency domain\", Chen et al. 2016\n[2] \"CNNpack: Packing convolutional neural networks in the frequency domain\", Wang et al. 2016"
        }
    ]
}