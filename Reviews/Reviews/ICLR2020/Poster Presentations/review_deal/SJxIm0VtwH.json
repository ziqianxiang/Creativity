{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This work proposes a new adaptive method for solving certain min-max problems.\n\nThe reviewers all appreciated the work and most of their concerns were addressed in the rebuttal. Given the current interest in both adaptive methods and min-max problems, this work is suited for publication at ICLR.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This paper proposes two methods named OSG and OAdagrad for solving stochastic non-convex non-concave min-max problems. In theoretical analyses, a convergence rate of $O(\\epsilon^{-4})$ and a much better rate are provided for OSG and OAdagrad, respectively, to find the $\\epsilon$-accurate first-order stationary point. Finally, the superior performance of the proposed method is empirically verified on training generative adversarial networks (GANs).\n\nClarity:\nThe paper is well organized and easy to read.\n\nQuality:\nThe work is of good quality and is technically sound. However, I did not verify the proof in detail.\n\nSignificance:\nA stochastic non-convex non-concave minimax problem studied in this paper is recently considered as an important class of the optimization problems because important machine learning problems such as GANs fall into this class and most past papers studied convex-concave min-max problems instead. A few studies [Iusem+(2017), Lin+(2018)] proposed optimization algorithms for this problem and derived convergence rates $O(\\epsilon^{-4})$ and $O(\\epsilon^{-6})$, respectively. On the other hand, proposed methods have several preferable properties compared to these methods. For instance, OSG exhibits a comparable convergence rate to [Iusem+(2017)] with a fewer per-iteration complexity and OAdagrad exhibits a much faster convergence rate $O(\\epsilon^{-2/(1-\\alpha)})$ depending on the parameter $\\alpha$ that is an order of the growth of the cumulative stochastic gradients norm. In addition, the order of $\\alpha$ is shown to be slow in general and certainly faster convergence rates are also confirmed in experiments on training GANs. Thus, experimental results seem consistent with the theory. \nSince a derived convergence rate of OAdagrad is potentially much faster than those of existing methods, I think the OAdagrad is one of the promising methods for training GANs."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "This paper proposed a new algorithm (Optimistic Stochastic Gradient) for solving a class of non-convex non-concave min-max problem. The convergence theory is established for finding first order stationary point. The authors also proposed an adaptive variant of the proposed algorithm, called OAdagrad and showed an improved adaptive complexity.\n\n- It is not immediately clear to me why the update of Algorithm 1 becomes the algorithm in (Daskalaskis et al. 2017) when there is no constraint. Can the authors further explain this clearly? The two variables in (Daskalaskis et al. 2017) are updated with different signs, but here  z=(u,v)  (I assume, the authors should clearly define z too), it seems that u and v are updated with the same sign?\n\n- In terms of the theoretical contributions, aside from the presenting theorems, can the authors also comment on what is the key point to achieve the derived results? For example, in Algorithm 1, the only difference from stochastic extragradient method is T(z_{k-1}) instead of T(x_{k-1}), and the theorem achieves the same iteration complexity with weaker assumption.  Is this because the replacement term, or it is the proving technique improvement that can also be applied to stochastic extragradient method?\n\n- For Algorithm 2, why there is no projection operator like in Algorithm 1? Also, the algorithm design is a bit different from traditional AdaGrad as their H matrix is the historical average of all past gradient square (element-wise). Here the s_i is L_2 norm of historical gradient, there is no averaging (divided by k) operator. Can the authors elaborate on this algorithm design?\n\n- For experiments part, it is better for the authors to compare with more baseline methods such as Optimistic Adam. For Figure 1, can the authors put different batches in different plot so that we can directly compare the performances of different algorithms? It does not make sense that alternating Adam failed in training SA-GAN here but success in the original paper. Can the authors figure it out and provide the comparison on this?\n\nDetailed comments: \n\n- it is better to define m_t in or before Algorithm 1 to make it clear for the readers.\n- In Page 7, comparison with â€¦ paragraph, should be \\beta_1 = 0, \\beta_2 -> 1? If so, it is still a bit different with OAdagrad?\n\n\n======================\nafter the rebuttal\n\nI thank the authors for their response and it addressed most of my concerns.  ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "--------------------------\nAfter the revisions made by the authors, my main concerns about the paper have been removed. Therefore, I am raising my score to 6 (weak accept).\n--------------------------\n\nSummary\n\nThe present work is concerned with the development of algorithms for the solution of variational inequalities in the stochastic setting, that is when the gradient computations are corrupted by noise. In this setting, the authors propose a variation of the extragradient method which they call Optimistic Stochastic Gradient (OSG), and show that by using a suitable method of variance reduction, the convergence rate of the algorithm matches the state of the art rate of convergence while relaxing the assumptions on the VI from pseudomonotone to assuming that the associated minty variational inequality has a solution. The authors furthermore introduce an adagrad-version of the same algorithm and show that improved convergence rates can be obtained depending on the growth rate of the cumulative stocastic gradients. An extensive suite of experiments studies the empirical performance of the proposed algorithms and compares them to the commonly used Adam optimizer.\n\nDecision\n\nIn its present form, while some of the contributions of the paper seem to be relevant contributions (theoretical analysis of adagrad version, state of the art performance of optimistic mirror descent), other contributions (some of which are advertised strongly in the abstract) are incremental or implicit in earlier work (optimistic stochastic gradient descent, relaxation from pseudomonotone to minty VI). Furthermore, while the experimental part is detailed, the connection to the theory could arguably be strengthened more. I believe that by more concisely focusing the paper on its innovative aspects while relating it more directly to existing prior work, it could make for a much more valuable contribution to the literature, which is why I vote for rejecting the paper in its present form.\n\nAdditional Detail on decision\n\nNovelty of OSG\nThe authors themselves note that the difference between OSG and OMD of Deskalakis et al is the inclusion of a projection step and the variance reduction by averaging multiple gradient evaluation at each iteration (which corresponds to choosing a different batch size for different iterations). I don't think these modifications are major enough to warrant \"introduction of a new algorithm\". The fact that all experiments seem to be conducted without constraints and constant batch size further strengthens this impression.\n\nRelaxation to Minty VI\nAnother claimed improvement of the paper is relaxing the assumption of pseudomonotonicity to the mere assumption of existence of a variational inequality. However, if I'm not mistaken (please correct me if this assesment is incorrect), the only part where the pseudomonoonicity assumption enters the proof in Iusem et al is on page 36, to prove the last inequality of Equation (105). Here, however, the assumption of a Minty VI could equally be used. Thus, the weakened monotonicity assumption is not related to the use of SGO as opposed to extragradient, which is not apparent from the paper.\n\nSuggestions for revision\n\nI do think that the material can make for a solid paper, but I think it would strengthen rather than weaken the contribution to point out in more detail the close precursors of some of the results in the paper. Notably, rather than inventing a new algorithm (OSG), the paper proposes a way to combine variance reduction with Optimistic mirror descent of Deskalakis et al such as to achieve state of the art convergence rates which so far were only known for the variance reduced extragradient method. Furthermore, it points out that these convergence results (just as in the case of variance reduced extragradient) hold as soon as the associated Minty variational inequality holds true. This latter point would be much stronger, if examples were given that illustrate why this is a meaningful extension. Finally, the paper derives improved rates of an OAdagrad. In my opinion this point should be made more prominent by deemphasizing the part on OSG.\n\nThere is a typo in the definition of pseudomonotonicity.\n\nQuestions for authors\n\n(1) Does figure 1 show iterations or epochs on the x axis? In order to support the theoretical claims, wouldn't it need to show the epochs on the x-axis? Otherwise, a larger batch-size simply corresponds to having access to more calls of the stochastic gradient oracle.\n\n(2) Is there a new technical difficulty to overcome when replacing the extragradient method with the OSG compared to the proof of Iusem et al? If yes, can you give a concise description of it?\n\n(3)It is not \"evident from the definition\" to me, why pseudo-monotonicity implies the existence of a solution of the Minty variational inequality, even though I believe that this is true. Could the authors explain this to me?\n\n(4) The main improvement of OSG over extragradient lies in it needing to compute half the number of gradients per iteration. Is it possible to  explicitly compare the constants in the bounds on the convergence rates of the two algorithms? It seems that this would be required to truly make the case of a reduced complexity of OSG.\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}