{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The paper provides a careful, reproducible empirical comparison of 5 graph neural network models on 9 datasets for graph classification. The paper shows that baseline methods that use only node features (either counting node types, or summing node features) can be competitive. The authors also provide some guidelines for ways to improve reproducibility in empirical comparisons of graph classification.\n\nThe authors responded well to the issued raised during review, and updated the paper during the discussion period. The reviewers improved their score, and while there were reservations about the comprehensiveness of the set of experiments, they all agreed that the paper provides a solid empirical contribution to the literature.\n\nAs machine learning becomes increasingly popular, papers that perform a careful empirical survey of baselines provide an important sanity check that future work can be built upon. Therefore, this paper, while not covering all possible graph neural network questions, provides an excellent starting point for future work to extend.\n\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This type of benchmarking paper is long overdue for graph classification with deep neural networks.  The paper would've been strongly if it had the following:\n\n1. Considered more structural features than simple node degree and clustering coefficient.  Prior work [1] has looked at such features and answered questions like: How do structural features improve classification performance?And, which structural features are the most useful? \n\n2. Investigated which graph neural network performs better for which graph structures (preferential attachment, small world, regular, etc) and for how much homophily.  \n\n3. Investigated the robustness of graph neural networks on classification as the structure of graphs become more random (e.g., by rewiring edges while maintaining degree distribution).\n\n[1] B. Gallagher, T. Eliassi-Rad. Leveraging Label-Independent Features for Classification in Sparsely Labeled Networks: An Empirical Study. Lecture Notes in Computer Science: Advances in Social Network Mining and Analysis, Springer, 2009."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "This paper provides an empirical comparison between several existing graph classification algorithms, aimed at providing a fair comparison among them, as well as proposing a simple baseline that does not take into account graph structural information.\n\nOverall, I found the experimental section very thorough and sound, with proper ways of performing parameter tuning and reporting test accuracies. On the other hand, I found this paper to miss some deeper insights into why some models perform better than others, and what are the challenges provided by these graph datasets. Also, there are other interesting dimensions of comparison that are not considered (see details below), as well as insights that could benefit future work in the area.\n\nAs a disclaimer, I would like to mention that I am more familiar with graph node classification methods, as opposed to whole-graph classification (which is the focus of this paper), so I cannot assess very well the authors’ choice of which models to compare, and which datasets they were tested on.\n\nPositive aspects of the paper:\na)  Extensive experiments, which try to find the best parameter configuration for each of these models.\nb) I appreciate the addition of the structure-agnostic baselines, although I was missing major details about the particular choice of baselines (see below).\nc) The writing is very clear and easy to follow.\n\nPoints where I found the paper lacking:\n1.  As mentioned above, I believe there are insufficient details about the baselines. For instance what is the global sum pooling over? If it is over neighbors, then this is not exactly “structure-agnostic”. If it is over features, then why was this architecture chosen as opposed to just a multilayer perceptron, without the Deep Sets component? I see the value in an order-invariant component when aggregating features over a node and its neighbors, but why is this necessary at node level? \n\n2.  What is the range of values considered for each parameter in the hyperparameter validation phase? For instance, I found it a bit surprising in table A.3. that all models need 500 patience. \n\n3. I found the discussion over the results to be quite limited. For instance:\n a) The authors point out that the performance on the NCI1 dataset is different than all the others (it is the only dataset where the baseline is not the best). How is this dataset different? \nb) How do these different models compare depending on certain dataset features? Perhaps a model is better than the rest if a dataset has some particular properties.\nc) Are the differences in performance shown in Tables 3-4 indeed significant, given the standard deviations, or these models practically perform the same?\nd) What makes these datasets challenging, such that the best performing models get up to 70-80%? This could provide interesting insights for future work.  \n\nIf the authors clarify some of the issues above, especially about the results discussion part, I believe this paper may indeed be of value to the graph classification community. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "\n\n********** Post-Rebuttal Update **********\n\nI appreciate that authors provided comments on all the raised concerns and updated the paper accordingly. I think the revised paper is of a higher quality. \n\nAlthough the paper's experimental setup could be done in a more proper way -- nested CV; lack of which degrades the conclusiveness of the comparisons, I still think the paper is better to be accepted than not, given that 1) as pointed out by the authors the computational cost of the current experiments was quite high already, 2) the issues of replicability and reproducibility are important and GNNs are quite popular for various applications and 3) the results are interesting and important to be considered for future GNN works. So, I am happy to change my rating to weak accept. However, for the record, I would like to mention the following point:\n\nThe authors removed the word nested from the paper which is good since I still believe it does not correctly reflect what’s been done in the work. However, in the reply, they imply that several works including Varma & Simon, 2006 [1] would consider their method a nested CV. They attribute this different viewpoint to the reviewer’s misunderstanding of CV for *k-fold* CV. That is not true. CV requires “cross” testing which is absent from the inner loop of this work. I refer the interested readers to actually the mentioned paper by the authors: Varma & Simon, 2006 [1], sectio:background. Page 2 clearly defines cross validation requiring a for-loop for several evaluation across different training/test sets and page 3 defines nested CV as requiring an inner loop and  outer loop of this kind. So, as far as I understand, based on [1], the proposed method is not nested CV since the inner (model selection) part uses the classic single train/test split without any loop.\n\n\n********** Summary **********\n \nThe paper conducts an empirical study of 5 recently-proposed graph neural networks (GNN). Three questions are studied:\n1) For some selected set of classification tasks (chemistry and social networks) and datasets (9 different datasets), how do the considered GNN models perform, relative to each other, given the same hyperparameter search strategy and a “nested” cross validation scenario.\n2) How much does the structure (graph edges) bring on top of a multi-layer perceptron (MLP) operating on merely the node features?\n3) Do the considered GNN models exploit structural information of the input graphs beyond that of the node degrees?\n \n\n********** Strengths and Weaknesses **********\n \n+ it is shown that the inclusion of nodes’ degree in its feature representation brings a very large performance boost. This is very interesting.\n+ it is shown that on some datasets, the results of a simple baseline -- only operating on the node features, can achieve similar results to the elaborate GNNs. This result is very informative and suggests that this baseline should always be included in GNN works. \n+ some of the results in table 4 contradicts the corresponding papers which can be informative for the practitioners of the field.\n+ the reproducibility and replicability problems, that is the motivation of this work, are important concerns of the field.\n \n- reproducibility and replicability problems is not specific to research done on graph neural network and is a caveat for the general machine learning research as discussed by Lipton and Steinhardt 2018., and in fact for the current scientific practice at large as pointed in the recent NSA report [Reproducibility and Replicability in Science, 2019]. So, it may be important to raise and investigate this in different subfields of machine learning, such as GNN, individually. However, the current abstract and introduction of the paper (before the last sentence of intro’s second paragraph), associates this problem specifically to the research conducted in GNN. I strongly believe the better formulation is to refer to Lipton and Steinhardt 2018 as the troubling trend in ML as well as the NSA report in general and then mention that in the current work the authors focus on the subfield of GNN *classification*.\n- page 1: “For instance, it is often unclear how hyper-parameters have been selected or which validation splits have been used.”. These being *often* the case in GNN research is not backed up by statistics and is not shown to be specific to GNN. Even if it’s assumed the 5 GNN methods , under this paper’s scrutiny, are representative of GNN classification, GNNs are used well beyond graph classification.\n- it should be clearly discussed what are the additional information that this work brings on top of Shchur et al. 2018. Is it only the shift of focus from node classification to graph classification?\n- the two notions of “fairness in comparison” and “ablation studies” are sometimes conflated. For instance, the argument against “fairness in comparison” of using one-hot encoding in GIN is not a matter of fairness (since it’s a novel proposal for node features of GNNs) but rather a matter of “ablation studies” (to show the effectiveness of individual components of a new GNN method). \n \n- many concerns regarding the nested cross validation:\na) it is important to note that the paper (as described in algorithm 2) does not do “nested cross validation” despite the claim in various parts of the work. Algorithm 2 conducts model selection based on a fixed train-val split. As such, there is no “cross” validation even with the minimum of 2 folds.\nb) the standard deviations reported in table 3 and 4 are unreasonably high. This important observation is not properly discussed. This high std is in contrast to the standard deviation reported in the baseline papers and makes most methods fall into one-std interval of each other. I strongly suspect that this is due to the non-nested cross validation that is done in this work. That is, the hyperparameters are found using a single val set consisting of 9% of all data. A proper nested cross validation would test each set of hyper parameters against k_in validation folds to make a robust inference of best hyperparameters.\nc) there are 3 runs used “to smooth the effect of unfavourable random weight initialization on test performances”, however the 10-fold outer CV should take care of this to some extent. I would argue it would have been much more helpful to use this budget to do a proper CV for model selection using 3 folds.\nd) The datasets are quite small (600-5000 samples). That might make the 10-fold cross validation problematic and overly optimistic in its performance report (and high variance). It might have been better to do say 5-fold cross validation but repeat the whole process twice. Of course, this is an old dilemma in the pratice regarding the bias and variance of the estimated CV error but as far as I am aware there is a general agreement that getting “closer” to leave-one-out setup is not a good idea, Furthermore, in my anecdotal experience 10-fold CV for a dataset with only 600 samples can really be problematic. \n \n- would the ranking of GNN methods change with different types of the node features being used?  Some GNN setups might be better at representation learning and thus should work better on raw features (as opposed to hand engineered ones).\n \n- how is the number of parameters (or capacity) across different GNN models and the baseline? Has this been taken into account for a fair comparison? For instance, could it be that higher/lower number of parameters explain the differences across GNNs and baseline due to over/under fitting?\n \n********** Disclaimer. I did not thoroughly check the paper’s report of the 5 GNN methods (summarized in table 1). \n \n\n********** Final Decision **********\n\nI believe the paper has merits as well as interesting findings. It is also true that this is an important concern in machine learning research. However, there are many issues that degrades the quality of the paper. Of all those critiques, I suggest “Weak Reject” mainly due to the ones raised regarding a proper nested cross validation; which is the main proposal of the paper.\n \n\n\n********** Minor Points **********\n- Page 1: “Our results put on a fair and unique reference scale many published results which, as we document, were obtained under unclear experimental settings.“: please rephrase; at least one preposition is missing.\n- Page 3: Nested Cross Validation: the brief textual explanation makes the simple idea more complicated than it is. Instead, I suggest to use citation as well as a small figure or short algorithm describing it since it’s the focus of the paper. It will also serve pedagogical purposes.\n- Table 1: precisely define the two mark “A” (ambiguous) and “-” (lack of information).\n- page 7: “higher or equal than” → “higher than or equal to”\n- in page 6, it’s mentioned that some experiments took more than 72 hours. This sounds excessive for such small datasets. My guess is that this is due to the fact that large number of epochs are allowed (e.g. 5000 in table A.3) in conjunction with extremely small learning rate (e.g. 1e-6). The question is if those extreme hyperparameter settings are actually important for this study?\n- the first paragraph of 6.1 argues that “GNNs are still unable to exploit the structure on such datasets [D&D, PROTEINS, ENZYMES]”. While this might be true, the conclusion is only based on 5 methods and limited by the design choices such as the architecture. It should be toned down.\n- page 5, “Features” paragraph is not very clearly written. For instance, from the sentence: “More in detail, in the former nodes ...”, it’s hard to understand what “former” refers to.\n- mean and standard deviation should have the same number of decimals (table 3,4)\n- page 4: “Moreover, the authors applied early stopping, which entails the use of a validation set“. Early stopping, in a less common setup, can be used by only looking at the training set and stopping with the same n-patience strategy as used in this work.\n- page 4: “we conform to the available code and do not use sampled neighborhood\nAggregation.“: needs more explanation.\n \n\n********** Points of improvements **********\n\nThe main point of improvement would be a proper nested cross validation setup. \nThe authors can also consider the ReScience journal: https://rescience.github.io/\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        }
    ]
}