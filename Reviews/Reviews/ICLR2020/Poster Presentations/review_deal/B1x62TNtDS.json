{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper presents a critical appraisal of variational mutual information estimators, and suggests a slight variance-reducing improvement based on clipping density ratio estimates, and prove that this reduces variance (at the cost of bias). They also propose a set of criteria they term \"self-consistency\" for evaluation of MI estimators and, and show convincingly that variational MI estimators fall short with respect to these.\n\nReviewers were generally positive about the contribution, and were happy with improvements made. While somewhat limited in scope, I believe this is nonetheless a valuable contribution to the conversation surrounding mutual information objectives that have become popular recently. I therefore recommend acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "Summary:\nAs the title suggests the paper focusses mostly on a negative result:\nMutual information (MI) estimators obtained by variational methods have severe\nlimitations that make them potentially not useful for down stream tasks.\nBesides highlighting the problems with variational MI estimators the authors\nsuggest a modification to slightly improve the performance of MI estimators\nbased on partition functions by reducing their variance when MI is high.\n\nThe authors give a good overview / introduction of various approaches to\nvariational MI estimation by discriminative and generative methods.\nGenerally, MI estimation involves the estimation of the KL divergence between\nthe joint distribution and the product of the marginals.\nThe authors present a unifying view on the different approaches that optimizes\nthe log density ratio required for the KL divergence over the space of\nlog density ratios.\nDiscriminative approaches model the density ratio directly (through e.g. neural\nnetwork models) and generative approaches model the separate densities\n(as generative models where it is possible to evaluate the (conditional)\nprobabilities / likelihoods of the data generating process).\n\nThe authors prove that discriminative approaches that are based on the partition\nfunction approach suffer from high variance where mutual information is high\n(Theorem 2).\nThe estimator based on a finite sample has high variance even if the density\nratio approximation is correct.\n(The partition function approach is a way of staying constrained to the log\ndensity ratio function space.)\nThis high variance problem is something that has previously been observed\nempirically and is the main theoretical point that is being made about\nlimitations of MI estimators.\n\nIn order to slightly alleviate the problem of high variance the authors suggest\na way of biasing MI estimators by clipping the density ratio estimates\nthrough a constant chosen as a hyper-parameter.\nThey prove that their clipping approach reduces variance and therefore\nintroduces a bias variance tradeoff.\n\nIn their later experiments the clipped version of the discriminative approach\nperforms much better in terms of variance than without clipping and also better\nthan a generative approach.\n\nIn order to empirically evaluate the quality of MI estimators the authors\nsuggest three criteria that they call self-consistency:\n(i) independence, (ii) data processing, (iii) additivity\n\nSelf-consistency is evaluated experimentally on images where mutual information\nis computed between original image and image with part covered.\n\nThe authors claim and experimentally show that discriminative approaches fail\nin (iii) and generative approaches fail in (i), (ii).\nOverall, variational MI approaches do not satisfy self-consistency.\n\nEvaluation:\nI suggest to accept the paper.\nThe theoretical contribution of showing the variance limitation of\ndiscriminative approaches seems significant.\nThat insight leads to the idea that clipping can be a useful bias that\nsignificantly reduces variance without making the already biased anyways\nresults much worst in the experiments.\nHowever, I also feel like the paper is not yet as focused as it could be.\nIt contains many concepts that could need a little bit more space.\n\nSuggestions:\n- Page 2: Nitpick, but in the definition of $L^p(Q)$ using $\\colon$ twice is\n\tnot super readable on the first read\n\n- Page 2: In the definition of $I_{BA}$ clearify whether $p(x)$ is a marginal\n\tor a joint density (as $P$ is the cumulative joint)\n\n- Page 3: In Theorem 1 what is the definition of $P \\ll Q$?\n\tThis suggests an order on the space of measures $\\mathcal P$?\n\n- Page 3: \"Obtain an density ratio estimate\" -> Obtain a density ratio estimate\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "This work summarizes the existing methods of mutual information estimation in a variational inference framework and describes the limitations in terms of bias-variance tradeoffs. Further, the authors care about the self-consistency, namely, independence, data processing, and additivity, which are properties of both entropy and differential entropy. Further, density ratio clipping is proposed to lessen a high-variance problem in estimating a partition function.\n\nIn regard to the clipped density rations, it is not clear why both variance and bias become small when S is close to 1 and using small $\\tau$ in Theorem 3 and 4. In Fig. 2 on the benchmark experiment, $\\tau=5.0$ showed lower variance than $\\tau=1.0$.\n\nThe comparison with MINE and BA is also expected on the benchmark experiment.\n\nAs for the self-consistency tests on images, especially, for the ‘data-processing’ property, it would better to apply affine transformation such as rotation, translation, and scaling, rather than rows deletion.\n\nIn Section 3.1, $\\Gamma$ is not defined.\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper relates most existing variational mutual information estimators to density ratio estimation and uses this to show high variance of certain categories of estimators; this motivates a new SMILE estimator that uses clipping to explicitly control the variance of the estimator (while potentially increasing bias). The paper also conducts some experiments on how well mutual information estimators satisfy some of the core properties of mutual information in practice.\n\nThe core result of Theorem 2, although is proof is very simple once the problem has been phrased appropriately, is enlightening.\n\nThe self-consistency tests are also sensible and important, although it would be nice to understand better which of these properties are truly important for various uses of mutual information in the literature.\n\nThe proposed SMILE estimator is natural. But:\n\n- As $\\tau \\to \\infty$, the variance bound of Theorem 4 naturally becomes infinite (as we'd expect). But we'd hope that the bias would become 0, when in fact Theorem 3 only shows in this case that the bias is at most S. Of course it should be true that $\\mathbb E_Q[r] = \\mathbb E_Q[r_\\infty]$; can we fix Theorem 3 so that it recovers this fact?\n\nThen the biggest question not really addressed by this paper is: how do you set $\\tau$?\n\n- If we know the (approximate) value of $S$, then we can presumably trade off between the bias and variance terms of Theorem 3 and 4 to find the most effective value for $\\tau$. (Indeed, it might be nice to write a corollary showing that if you set $\\tau$ to something depending on $n$ and some upper bound on $S$, you achieve some rate in L2 risk.)\n\n- Is it possible to relate the value of $S$ to, say, the suboptimality in cross-entropy loss of the classifier used to obtain $r$ (when getting $r$ that way)?\n\n- Is it possible to get a probabilistic upper bound on $S$, either with some other technique or perhaps a simple Hoeffding/Bernstein-type bound on the SMILE estimator with some initial value of $\\tau$, and then use that to choose the rate for $\\tau$?\n\n- Is there some other practical scheme (even if it doesn't admit bounds) to set $\\tau$ to control the bias, given that it depends on $S$?\n\n\n\nSmaller points and minor suggestions:\n\n- In the paragraph following Theorem 2, it is first of all strange to say that \"alternative procedures to obtaining the density ratio estimates will not alleviate the high variance issue\" when indeed your proposal does just that. The last sentence of this paragraph clarifies the issue, but still makes $r \\ne r^*$ sound \"bad,\" when in fact it's a trade-off; this could be rephrased more clearly.\n\n- In Theorem 3: I think writing $e^{-2 \\tau} \\lvert S - e^\\tau \\rvert$ would be clearer than $\\lvert e^{-\\tau} - S e^{-2 \\tau} \\rvert$ as you have it.\n\n- In the multivariate Gaussian experiments, Section 6.1: why do you effectively warm-start the estimator for higher mutual informations? Is it just so you can put things in one plot per method? I think it would be more natural to train each different problem from scratch; you can still use the same plot format if you like (with additional vertical lines between problems or something), but it's not clear whether the behavior of the estimator is affected by this choice.\n\n- Why are the flow-based results not shown in Figure 2 / Table 2?\n\n- In Section 6.2 when setting up the three evaluation settings, it would be good to have a forward reference to Figure 3 (since these settings are perhaps easier to understand visually)."
        }
    ]
}