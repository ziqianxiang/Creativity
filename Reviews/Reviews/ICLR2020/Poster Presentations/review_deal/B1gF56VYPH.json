{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "Two reviewers recommend acceptance while one is negative. The authors propose t-shaped kernels for view synthesis, focusing on stereo images. AC finds the problem and method interesting and the results to be sufficiently convincing to warrant acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\nThis paper proposes a deep learning method to produce \"pans\" of an input image. That is, simulated images of the scene from translated viewpoints. Unlike some previous work that considers only a fixed baseline (such as the 2nd view of a stereo camera), this approach allows generation of a range of views. A specially crafted convolutional architecture is shown to be well-suited to this problem. Results demonstrate visually pleasing image generation and low metric errors on several datasets.\n\nStrengths:\n- The justifications for the design choices in this paper, in particular the convolution structure and connection to image geometry, was quite strong compared to recent papers (although, many of the presented ideas are known in more classical, non-learning, techniques). \n- All presented empirical results are impressive, and show the method is likely to \"really work\" and be reproducible, judging from the number of experiments where the method has consistently outperformed. \n- The method is clear and straightforward to implement either on its own, or as a module/architecture within a larger pipeline.\n\nAreas for Improvement and Detailed Suggestions:\n- The problem of panned view generation is a bit more narrow than some authors are lately considering (generate any viewpoint including off-axis rotations). \n- The t-shaped network architecture here is largely presented as only appropriate to handing image panning. Could a more general network be created, perhaps parameterized by the type of rigid motion occurring? Better, could more flexible networks be proposed with sparsity constraints that allow sub-patterns like the t-network to be learned in a data-driven fashion? \n- Please try to cite referenced work more consistently. For several methods, such as Deep 3D Zoom Net, you have begun to discuss the method using name only for large stretches. It would be helpful to keep using the citation at least once per paragraph to remind the reader of the source of these ideas. \n\nDecision and Justification: \nWeak reject due to the lack of generality in the approach. I suspect the impact of this work will be a bit less than the very competitive bar for ICLR this year. However, I must note that I am the least expert in this area, out of any paper in my stack. \n\n"
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The submission proposes a method to perform stereoscopic view synthesis. The method consists of a neural network model that estimates a novel viewpoint either to the left or to the right of a given image. The two key insights of the proposed method is 1) to learn the weights of a t-shaped kernel when performing novel view synthesis, and 2) to estimate and use adaptive dilations on those kernels.\n\nThe proposed approach is sound and the evaluation methodology used is adequate. The technique proposed is somewhat related to the recent interest in the community to apply CNNs to non-regular grids [1,2,3].\n\nSec. 4.2 states that the proposed method outperforms other existing methods on monocular depth estimation, while the table seems to indicate that other methods (e.g., Wang et al. 2019) obtain a better a1 measure.\n\nI would refrain from calling a 0.004 increase in a1 “a remarkable improvement” (sec. A.9).\n\nMinor details\n- I would refrain from using the word “prove” (abstract, sec. 4), since no proof is provided. “demonstrate”\n- p. 2 “is open known to be a much more complex problem”: I think the authors meant either “is known to be a much more complex problem” or “is still an open problem”?\n- p. 9 “planed” should be written “planned”.\n- p. 9 “the receptive field [...] is of the 153x153 size”: should be “has a size of 153x153”.\n\n\n[1] Su, Yu-Chuan, and Kristen Grauman. \"Learning spherical convolution for fast features from 360 imagery.\" Advances in Neural Information Processing Systems. 2017.\n[2] Coors, Benjamin, Alexandru Paul Condurache, and Andreas Geiger. \"Spherenet: Learning spherical representations for detection and classification in omnidirectional images.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n[3] Zhao, Qiang, et al. \"Distortion-aware CNNs for Spherical Images.\" IJCAI. 2018.\n"
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "N/A",
            "review": "The paper considers the problem of performing stereoscopic view synthesis (i.e., generating a new view seen from a different camera position) at an arbitrary position along the X-axis from a single input image only. This is an important problem as it enables 3D visualization of a 2D input scene. The paper focuses on the particular problem of generating a stereoscopic view from a single image (i.e., a right and left view from a center image). \nFor this purpose, the paper proposes a t-net architecture which is an autoencoder or U-net like architecture that estimates the values for the t-convolutions proposed in the paper. The network (called monster-net) takes a center image and a pan amount as input, and from those synthesizes the image with the respective view.\n\nThe paper demonstrates that their idea of t-convolutions outperforms recent competing approaches such as deep 3D on available datasets as well as on an in-house collected dataset. The figures provided demonstrate that the views generated by the proposed Monster-net visibly look slightly better than those generated by the competing approaches DeepD and SepConv. In addition, the paper is well written and easy to follows. I therefore recommend acceptance of this paper. I would like to emphasize that while I work in deep learning, I don't work on view synthesis and therefore it is difficult for me to evaluate the novelty of the proposed approach as well as the difficulty of the problem."
        }
    ]
}