{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The paper reports interesting NAS patterns, supported by empirical and theoretical evidence that the pattern arises due to smooth loss landscape. Reviewers generally agree the this paper would be of interest for the NAS researchers. Some questions raised by reviewers are answered by authors with a few more extra experiments. We highly recommend authors to carefully reflect on reviewers both pros and cons of the paper before camera ready.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "The paper makes an interesting observation and tries to explain what causes it: architecture search methods tend to favor models that are easier to optimize, but not necessarily better at generalization.  I lean towards accepting the paper but there is some clear room for improvement.\n\nThe paper shows that NAS methods comes up with shallower but wider cells. \n- These cells are easier to optimize because they have a smoother loss surface and lower gradient variance. \n- Being easy/fast to optimize is favored by a NAS method because the models are typically not trained to convergence. Instead they are evaluated after a brief period of training. \n- This leaves the question: why are they smoother.\n\nBefore going into the comments I want to state that I am very happy to see that a paper providing an analysis of existing methods to enhance our understanding. This is very much needed in the architecture search community. \n\n==== Comments ====\n- Could you describe what exactly is plotted in Fig. 5, 6 and 7. Specifically what are the aces. . It would make the manuscript more self contained. This point is also one of the main reasons why I did not give the manuscript a higher score. I am unsure of what is plotted but I am giving this manuscript the benefit of the doubt because it is consistent with my own experience.\n\n- So far in the main text, most results focus on DARTS. It would be interesting to see the same consistent behavior is also present when comparing cells originating from different search spaces. (This is slightly different than the setting where the experiment is repeated on isolated search spaces). If this comment is unclear, please ask for a clarification.\n\n- Would it be possible to alter the conclusions by modifying the initialization of the weight matrix. It appears to be the case that the smoothness and the variance both depend on the eigenvalues of the weight matrices. If we could make them more well behaved we could potentially make the narrower architectures train faster?\n\n- Can you use a better term than common connection pattern in the abstract and conclusion. In general the abstract and conclusion could be written in a crisper and more to the point way.\n\n- Please update Table 1 to actually include parameter sizes. This would make the result more reliable. Also the adapted cells need to be explicitly provided. \n\n- Could you provide me with a better understanding of the difference between standard lipschitz and block lipschitz.\n\n==== More minor issues ====\n#) Correction required\nIn section 2, Zoph et al. is cited where it is argued that weight sharing could detrimental for the performance of nas methods. However this paper does not use weight sharing\n\n#) Corrections recommended\nXie et al. 2019 is said to be state of the art. In the actual publication the authors only claim to be competitive. I believe that this is a more balanced statement since those results are not always best on all dimensions (sometimes better on FLOPS but not parameters, unclear whether they would be faster on an actual device, on the large scale results they do lag behind in quality too.)\n\n#) My apologies for the slight digression but I do not think that using the Sciuto et al. paper is a good reference to discount weight sharing approaches or claim that random search is equally good. Some of their experiments are performed on a tiny search space with only 32 models. This gives random search a high probability of getting the right model, while a simple bias in the algorithm might cause it to be sub-optimal. That experiment does not show that random search is as good. Also on the PTB task, their reported results are worse than the open sourced implementation for DARTS. This means to me that this paper cannot be used as a reliable reference. \n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "--- Update after author's response ---\n\nThank authors for providing the detailed response to all my concerns. I am revising my rating to weak accept.\n\nSpecifically, generalising with 52 different variants of depth-width settings are enough, and the updated version is more solid than the earlier one. I think this paper should be accepted.\n\n\nSummary:\nThe paper observed one common pattern of searched cell by 5 NAS algorithms, which is the cell found usually has large width but small depth structure, and claims the reason of such pattern is because architectures with shallow but wide structure converge fast during training, and thus are sampled by the NAS policy. To justify this fast convergence claim, the paper proposed to 1) define a width-depth level (width based on feature maps dimension, and depth based on its DAG connection) for each cell in the search space, and randomly sampled one architecture at each level on top of some best-cell discovered by NAS algorithms, 2) training them on original task from scratch independently, and provide visualization of training curve under various learning rate settings, loss landscape as well as gradient variance plot. For theoretical analysis, the paper formulates the narrowest and widest cells, and showing the difference of gradient is bounded by its Lipschitz smoothness of parameter matrices, and usually such variance indicates the widest architecture could converge faster than the narrowest one. \n\nWhilst this observation is interesting, I found the empirical and theoretical justifications seem to be insufficient to support the claims for the following reasons, 1) The experiments are overwhelmingly built on top of **1** architecture (even obtained from random sampling) of each width-depth level, and it may not well represent the common behavior in the search space, thus the generalization of these claims remains questionable; 2) Theoretical analysis showing the gradient variance difference of narrowest architecture is bounded comparing to the widest cell, however, in practice, these architectures are not properly evaluated. Without proper extension, it is hard to conclude that such difference bound between a wider and narrower architecture pair; 3) experiments in supplementary negatively affects the generalizability of this paper, since the observed trend on DARTS search space does not agree with the one on AmoebaNet and SNAS cases; 4) paper claims the NAS algorithm tends to pick the fast converging architectures more than those late converged one, while intuitive, without showing some detailed process about how NAS algorithms converge, and which architectures they actually sampled during the search phase. \n\nNevertheless, I do agree that this paper has a clear motivation, and the observation is interesting and important. If the author could show the conclusion still hold after scaling the experiments, I am learning to accept this paper in the end.\n\n\nStrength\n+ Observation of these NAS algorithms tends to pick wide but shallow cell type is interesting, motivation of this work is well justified.\n+ Experiments are throughout, instructive under the paper's current setting.\n+ Theoretical justification for the gradient variance between the narrowest and widest cells are sound. \n+ Paper is well written and easy to follow, the figures are presented clearly.\n\nWeakness\n\n- Insufficient experimental setting to support the claim.\nMy major concern is that under the current experiment design, it is not clear if the observation is well justified, and impedes the main contribution. As mentioned in the review's summary, it is not that convincing that, for each width-depth level, one architecture is enough. I understand exploiting all the variants is resource consuming, however, without such experiments, the current experiments can be impacted by many factors, such as, 1) as in Appendix A2, for each level, the paper \"fixed the partial order of their intermediate nodes\" and \"replace the source node of their associated operations by uniformly randomly sampling a node from their proceeding nodes in the same cell\", if I understand correctly, this means the operations will remain the same. However, since these best architectures are searched over both operation and topology connection, new architectures generated from this way may be sub-optimal, hence the larger gradients or slow convergence is not only because they are \"narrower\" and \"deeper\". Without proper isolation, it is not possible to conclude as in paper. \n\nTo this end, I suggest author provide the following experiments, 1) sampling all the variants at each level, within a search space like NASBench-101[1], where all the architecture performances are known, 2) at least sampling a sufficient number in the current space (probably > 30 to be statistically significant); 3) Random sampling a small topological variances, then run NAS search algorithms to search the best operation set, then redo the experiments in the paper. \n\n- Trending of DARTS evaluation results does not agree with SNAS and AmoebaNet.\nIn Figure  5 (loss landscape plot) and Figure 6 (gradient variance heatmap) for DARTS, the wider-shallower architectures are better comparing to narrower-deeper ones, however, this trend is not significant in Figure 14,16 for AmoebaNet space and Figure 15, 17 for SNAS space. After a closer look, I noticed in Figure 2, the Darts C3 and C4, input node x0 is not connected to the graph, while C, C1, C2, and all the topologies in Figure 10, 11, x0 is connected. Could the author(s) comment on this? Will this be the reason why the C3, C4 DARTS are worse than other architectures?\n\nMinor comments\nPage 1 - Introduction paragraph 3 line 1 - 'typologies': is this referring to 'topologies'?\nTable 1 - Adapted architectures on CIFAR-10 are mostly worse, even on CIFAR-100, they are better in a small margin. This does not support the claim well.\nFigure 9 - Could the author provide the width-depth information for each index? \n\nReference\n[1] Ying et al. NAS-Bench-101: Towards Reproducible Neural Architecture Search, ICML'19.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "Summary:\n\nThis paper tries to understand the characteristics of the architectures found by common NAS methods in the cell-search space. Specifically it characterizes the cell-search space used by DARTS, SNAS, AmeobaNet and finds that a most of these search methods find cells which are wide and shallow in depth (they give a specific definition of width and depth for characterizing cells). In fact these cells are usually the widest and shallowest architectures in their search space. The author empirically find that because these kinds of topologies converge faster during training and inevitably every NAS algorithm during search don't train upto convergence but only up to a bit and make decisions based on partially converged statistics there is a bias in selection towards these topologies. They also provide theoretical intuition to back-up these empirical findings. \n\nSecondly they analyze the generalization performance of such wide and shallow cell structures accidentally emphasized by search procedures. They take the common cell structures found by common NAS algorithms (NASNet AmoebaNet, ENAS, DARTS, SNAS) and make them the widest and shallowest possible in the search space (following the SNAS cell connection pattern) while keeping number of parameters as constant as possible. They find that on cifar10 the test error of the adapted architectures usually increase a bit while on cifar100 the adapted architectures decrease a bit. \n\nComments:\n\n- Overall the paper is interesting and well-written. Definitely liked the fact that wide and shallow networks are being accidentally biased towards during search. Liked the empirical analysis and theoretical insights backing it up.\n\n- The generalization experiments suggest to me that on bigger datasets wider and shallower networks might be better for generalization actually. Can we take the cell architectures found by various algorithms and 'scale-up' to ImageNet by doing the usual trick of replicating more of the cells together and training? At least going by Table 1 I find myself not agreeing with the statement \"The results above have shown that architectures with the common connection pattern may not generalize better despite of a faster convergence.\" On cifar100 wider and shallower is better. Perhaps on ImageNet they will be even better? So NAS algorithms' strategy of training partially may be exactly the right thing to do? Any thoughts?\n\n- Any idea about if this pattern extends to RNN space as well or only limited to CNNs?\n\n- Overall my main gripe is that while it is interesting findings but I am not sure I understood the main takeaway or significance of these results especially the generalization ones and how it informs search algorithm design."
        }
    ]
}