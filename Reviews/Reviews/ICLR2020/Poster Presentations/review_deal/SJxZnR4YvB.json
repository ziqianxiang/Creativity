{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper tackles the problem of regret minimization in a multi-agent bandit problem, where distributed learning bandit algorithms collaborate in order to minimize their total regret. More specifically, the work focuses on efficient communication protocols and the regret corresponds to the communication cost. The goal is therefore to design protocols with little communication cost. The authors first establish lower bounds on the communication cost, and then introduce an algorithm with provable near-optimal regret.\n\nThe only concern with the paper is that ICLR may not be the appropriate venue given that this work lacks representation learning contributions. However, all reviewers being otherwise positive about the quality and contributions of this work, I would recommend acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "\nThis paper considers the problem of obtaining an optimal regret algorithm in a distributed setting without incurring a large communication cost. In the standard and linear MAB settings, the authors propose algorithms and show that they achieve optimal regret up to logarithmic factors with communication costs that are almost independent of the horizon T. In addition the authors establish interesting lower bounds on the communication cost to obtain sublunar regret.\n\nOverall I found the paper very well motivated and clearly written. I did not go through the proofs in the appendix carefully, but I did check a few sections and found them to be correct. I did have a few concerns. \n\nI found the discussion around load balancing very confusing. Perhaps the authors could provide a picture to explain the issue? In addition, there is a lack of experiments- it is always nice to see comparisons to baseline even though the theory implies you would do better. Finally, the algorithm doesn’t seem extremely practical from an applied point of view - for a linear amount of time all the bandits are pulling the same arms and there is no communication at all. I understand this repeated work doesn’t affect the regret - but it is an artifact of using elimination. In general optimism based approaches (such as UCB) tend to work significantly better than elimination-style methods. I’d be curious to hear the authors comment on whether they think a UCB style algorithm is possible in this setting.\n\nOverall I recommend the paper for acceptance. \n\n\n"
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors study a bandit problem where there are multiple agents (say, M) and each of the agents is playing a multi-armed bandit problem for T rounds. The agents can communicate with each other in order to achieve small regret. The problem is to design a strategy for arm-playing and communication so that the agents all combined can achieve a small regret w/o communicating a lot. The authors study this bandit problem in 2 settings: 1. multi-armed bandit setting and 2.  bandit linear optimization. For both these settings, the authors establish elimination style algorithms with communication. upon communication the sub-optimal arms are eliminated and the game continues with the remaining arms. \nThe authors establish regret guarantees as well as communication guarantees. The interesting result is that with constant communication the regret scales as if full communication was available.  \n\nThe results are interesting and I do not have any objections with the paper, except that ICLR might not be the right avenue for such work given that it lacks any ideas regarding representation learning."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "N/A",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "The paper considers the problem of distributed multi-arm bandit, where M players are playing in the same stochastic environment. The goal of the paper is to have small over-all regret for all the players without a significant amount of communication between the players. \n\n\n\nThe main contribution of this paper is obtaining regret ~root(M KT) with ~M bits of communication in MAB, and regret ~d*root(MT) with ~Md bits of communication in linear bandit setting. \n\nThe main intuition of the algorithms in this paper is to do \"best arm identification\" with epoching: At every epoch t, the central server sends the set of possible best arms to each player and each player pulls it for 2^t /M times, followed by a communication round. Thus, the cumulative regret is comparable to having one player doing this epoch strategy for MT iterations, where the regret follows.\n\nThe problem considered in this paper is interesting and the result is new, the technique looks simple on paper but it requires a masterful combination of known tricks in (linear) MAB to obtain the best bound.  \n\n\nIt seems that in the MAB setting, the lower bound could be further strengthened with a log(K) factor, since removing this factor would ultimately require \"dynamic epoching\" which is not possible with limited communication. This would mostly complete the picture in the distributed MAB regime.\n\n\nMissing citation:\nThe authors are missing citations relevant to distributed MAB with collisions, see for example \n\"Non-Stochastic Multi-Player Multi-Armed Bandits: Optimal Rate With Collision Information, Sublinear Without\"\n\n\nAfter Rebuttal: I have read the authors' responses and acknowledge the sensibility of the statement.\n\n\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        }
    ]
}