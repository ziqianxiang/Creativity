{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "The paper considers the problem of knowledge-grounded dialogue generation with low resources. The authors propose to disentangle the model into three components that can be trained on separate data, and achieve SOTA on three datasets.\n\nThe reviewers agree that this is a well-written paper with a good idea, and strong empirical results, and I happily recommend acceptance.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "This paper studies knowledge-grounded dialogue response generation in the low-resource setting. More precisely, it proposes a disentangled decoder consisting of three components: language model, context processor, and document reader. Disentangled decoder architecture provides a flexibility to train (or pre-train) different components on different data, making it convenient for low-resource setting. Overall, it is a sound idea for low-resource setting with generally positive experimental results, but limited in novelty in terms of the proposed architecture (similar to [1*, 2*]) and disentangling language and knowledge idea (similar to [3*]), lacks comparison with baselines for low-resource setting, and lacks discussion/reference of a few closely related works.\n\nHere are some of my questions and concerns for the paper:\n\nGreat to have some ablations in terms of pre-training to see its effect on different components. However, it would be quite useful to also have ablations over components by completely dropping a component (like LM) while training decoding manager. It would also be useful to see some statistics/discussion on the effect of different components in the actual generation of responses. It might also be useful to include qualitative examples of the generated responses annotated with predictions of different components for each generated word.\n\nIn Figure-2 (c) and (d), some ablation results are reported for when pre-training is removed for each of the three components independently. It is interesting, though, to see that removing pre-training does not hurt (might even improve) the performance (esp. for Test Unseen) much for FULL training data case. Is there a particular reason for this observation? Also, it makes me curious how the proposed model would perform without pre-training any of the components. Would it already outperform the baselines discussed in the paper? If so, are these baselines strong enough (SOTA or close to SOTA) to help draw a meaningful conclusion from comparison with them? For example, how would fine-tuning a pre-trained MASS  [4*] perform and compare as a baseline? Can authors comment on this?\n\nThe proposed approach is very similar in architecture to [1*, 2*, 3*], which are not discussed/referenced in the paper. Except for pre-training, the only difference from [2] is that copying and generation distributions are softly combined into separate distribution each, independently. Inducing a single output distribution is done instead by deciding which source to use by an MLP layer on decoder state as in Eq. 11. So, I think the authors need to better isolate what the core contribution of this paper is: disentangled decoder or pre-training strategy? If it is the proposed disentangled decoder architecture, then authors should compare with similar architectures [2, 3] in the low-resource setting by initializing encoder and decoder from pre-trained weights on the same Reddit corpus. If it is the pre-training strategy, then it should be compared with various pre-training strategies for sequence generation (e.g., [4*]) proposed recently. For example, it would be useful to include a comparison with fine-tuning a pre-trained MASS  [4*] with the same amount of WoW training data (changing from 1/16 to 1/1).\n\nPresentation of the paper can be improved by 1) changing the name of “document reader” (maybe to “knowledge processor” similar to context) as it essentially attends on the document rather than reading, 2) using abstraction in the technical section to help simplify notation and make it more interpretable.\n\n\nREFERENCES:\n[1*] Get To The Point: Summarization with Pointer-Generator Networks, See et al.\n[2*] DeepCopy: Grounded Response Generation with Hierarchical Pointer Networks, Yavuz et al.\n[3*] Disentangling Language and Knowledge in Task-Oriented Dialogs, Raghu et al.\n[4*] MASS: Masked Sequence to Sequence Pre-training for Language Generation, Song et al.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.",
            "review": "The authors propose a novel framework for training a knowledge-grounded dialogue model. They decouple the three main elements of such a model - a language model, a context processor, and a document reader - and as such can pretrain each component separately. They achieve state-of-the-art results on two benchmark datasets, and can additionally obtain near-state-of-the-art results while training on a fraction of the task data.\n\nOverall I think it’s a strong paper with a good set of experiments, baselines, and considers multiple datasets.\n\nDisentangling the model elements is a clever way to allow for more robust pre-training, and indeed yields favorable results. The authors show that just the pre-training aspect is not the root cause of their boost in performance, as a pre-trained baseline model fails to replicate their best results. The contribution is broadly applicable to other areas in which data collation is more difficult; the authors additionally do a good job of pointing out that their knowledge encoder is not limited to text but can also use other knowledge grounding including images, videos, or a knowledge-base. Finally, the authors detail thorough ablation studies for their models.\n\nOne major thing missing from their ablation in Figure 2 is a setting when *no* pretraining is used. That would be much more comparable to the setting used for TMN, since that had *no* pretraining available to it. Alternative, adding pretraining to the baselines would be another good way to do this, which would help disentangle how much the architecture is helpful over the pretraining.\n\nAlthough the authors point out that a major advantage of their architecture is that we can separate the pretraining for each of the components, I would also be interested to see how they find the model doing if a single source of pretraining is used. I.e., only reddit pretrained weights for all 3 components, or only wikipedia etc. I definitely don’t think that holds back this paper, just think it would provide some evidence of the value gained utilizing the disparate sources.\n"
        },
        {
            "rating": "8: Accept",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper presents an encoder-decoder model architecture for knowledge-grounded dialogue generation in a low-resource setting. The model includes two GRU-based encoders which represents knowledge and dialogue context independently from each other. The decoder also has the following three independent components: language model, context processor, and document reader, each of which works as an individual response decoder. While the language model considers the decoder's hidden state only, the context processor and the document reader apply the copy mechanism from dialogue context and knowledge, respectively. Then, the decoding manager generates the final distributions by aggregating the component outputs with Gumbel softmax.\n\nThe main contribution of this work is the disentangled architecture where the following three types of internal components are trained separately on different data from each other. Firstly, context encoder, context processor and language model are trained on un-grounded dialogues with no knowledge. Secondly, knowledge encoder can be built purely on knowledge sources in parallel with the dialogue-based components. Finally, document reader and decoding manager require knowledge-grounded dialogues as training data.\n\nThe experimental results show that the proposed architecture help to resolve the low resource problems of knowledge-grounded dialogue generation. The model achieved state-of-the-art performances on both Wizard of Wikipedia and CMU DoG even with only 1/8 of the training data.\n\nOverall, this is a well written paper with reasonable ideas supported by strong empirical evidences.\n\nPlease find below some questions and minor comments:\n- Would it be possible to evaluate the performance of the model with no grounded dialogues? It would be interesting to see how well the model works only with the un-grounded Reddit and Wikipedia data.\n- Have you thought about plugging in other language models trained on larger amount of texts such as Common Crawl? It seems possible, since the language model itself has no dependency to the context or the knowledge.\n- What do you think of generalizing the architecture for general dialogue generation problems with no knowledge grounding? This disentangled decoder may be applied to the language model trained on the larger amount of texts and the context model on the smaller amount of dialogues.\n- I suggest to provide some actual outputs from the models to show the advantage of the proposed method qualitatively.\n- Could you possibly make the subfigures of Figure 2 and 3 larger? They are now too tiny to see the details.\n"
        }
    ]
}