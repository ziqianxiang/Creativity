{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper proposes a model-based policy optimization approach that uses both a policy and model to plan online at test time. The paper includes significant contributions and strong results in comparison to a number of prior works, and is quite relevant to the ICLR community. There are a couple of related works that are missing [1,2] that combine learned policies and learned models, but generally the discussion of prior work is thorough. Overall, the paper is clearly above the bar for acceptance.\n\n[1] https://arxiv.org/pdf/1703.04070.pdf\n[2] https://arxiv.org/pdf/1904.05538.pdf",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "This paper presents POPLIN, a novel model-based reinforcement learning algorithm, which trains a policy network to improve model-prediction control. The paper studies extensively how to utilize the policy, by planning in action space or planning in parameter space and how to train the policy, by behavioral cloning, by GAN or by averaging the results of CEM. The experiments show that the proposed algorithm can perform very well in MuJoCo tasks. \n\nOverall, the paper is well-written and the method is novel. The extensive experiments make the paper more convincing. \n\nQuestions:\n1. In the example of arm in the first paragraph of Section 4.2, although the mean is 0, the randomness in sampling will be the tie breaker. So \"failing\" is probably not the best word here. \n2. It seems that the policy network is deterministic. Why?\n3. Reparametrizable policy often requires fewer (noise) parameters to be optimized over. For example, suppose the policy outputs a multi-variate Gaussian distribution with diagonal covariance in R^10, then we only need to optimize over 10 parameters (the Gaussian noise).  Why optimize all parameters in the policy network, which makes optimization more difficult? \n4. Sec 5.1 Para 2: To my knowledge, the state-of-the-art model-based RL algorithm in MuJoCo environments is MBPO (https://arxiv.org/abs/1906.08253, NeurIPS 2019). \n5. What's the architecture of policy network? More importantly, how many parameters does the policy network have? It's really interesting to see that CEM works for such a high dimensional space. In an environment where a larger network is required, the optimization seems to be more difficult. \n6. In Ablation Study, what does \"imaginary data\" mean? \n7. I'm also curious to see how the objective of CEM improves. \n\nMinor comments:\n\n1. Sec 5.1 Para 2 L11: efficient -> efficiently. \n2. Are you talking about Figure 3 at Sec 5.2? If so, could you please add a link to the figure? \n3. A lot of default values need to be specified: What's the policy distillation method used in POPLIN-A/P in Table 1? Does POPLIN-A mean POPLIN-A-Replan or POPLIN-A-init? Does POPLIN-P mean POPLIN-P-Sep or POPLIN-P-Uni? \n4. Sec 4.1 Eq (2): \\delta_0...\\delta_\\xi are \\xi+1 sequences. \n5. Sec 5.3 Para 3: multi-model -> multi-modal. "
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #2",
            "review": "Summary\n\nThis work provides a novel model-based reinforcement learning algorithm for continuous domains (Mujoco) dubbed POPLIN. The presented algorithm is similar in vein to the state-of-the-art PETS algorithm, a planning algorithm that uses state-unconditioned action proposal distributions to identify good action sequences with CEM in the planning routine. The important difference compared to PETS is the incorporation of a parametric state-conditioned policy (trained on real data) in the planning routine to obtain better action-sequences (CEM is used to learn the \"offset\" from the parametric policy). The paper presents two different algorithmic ablations where CEM either operates in action space or parameter space (POPLIN-A and POPLIN-P respectively), in combination with different objectives to learn the parametric policy. The method is evaluated on 12 continuous benchmarks and compared against state-of-the-art model-based and model-free algorithms, indicating dominance of the newly proposed method.\n\nQuality\n\nThe quality of the paper is high. This is an experimental study and the number of benchmarks and baselines is far above average compared to other papers in that field. One minor point is that averaging experiments over 4 seeds only is usually not optimal in these environments, but in light of the sheer amount of baselines and benchmarks excusable. While the experimental results are impressive, the authors mention that asymptotic performance in Walker2d and Humanoid might not match the asymptotic performance of model-free baselines. This could be stated more clearly. Also, there are no Humanoid experiments in the paper despite mentioned in the text (2nd paragraph in Section 5.1)?\n\nClarity\n\nThe clarity of the paper can be in parts improved upon. For example, how does the \"policy control\" ablation (mentioned in Section 4.3) work precisely, i.e. the interplay between executing the parametric policy in the real world and harnessing the environment model? I assume the policy distillation techniques in Section 4.4 are different alternatives for the second-to-last lines in the pseudocodes from the appendix? Which one is the default used in Section 5.1? On a minor note, above Equation (7), a target network is mentioned---where does the target network occur in Equation (7)? There are some plots that do not mention the name of the environment, e.g. in Figure (4), but also some in the appendix. Furthermore, it could be stated more clearly that the reward function is assumed to be known. If the authors improve the clarity of their paper significantly, I am willing to increase my score further (presupposing that no severe issues arise in the discussion phase).\n\nOriginality\n\nAdding a parametric policy to PETS is not the most original idea, but clearly a gap in the current literature.\n\nSignificance\n\nThe experiments and the empirical results make the paper quite significant.\n\nUpdate\n\nThe authors improved the clarity of the paper. I therefore increase to 8. Section 4.4 paragraph \"Setting parameter average (AVG)\" can still be improved---does this go together with POPLIN-P-Uni from Section 4.2?",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I do not know much about this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "Contributions\n\nThis paper proposes a MBRL algorithm for continuous action space domains that relies on a policy network to propose an initial trajectory, which is then refined, either in the action space or in the policy space, using standard CEM.\n3 options are evaluated to update the policy network: a GAN approach, a regression approach (behaviour cloning) and a direct modifications of the policy based on the perturbation found by CEM.\n\n\nReview\n\nThe presentation of the method is thorough, as well as the motivations and the experiments. However, the presentation of the experimental result could gain in clarity. For example, plots in Figure 7 are totally unreadable when printed on paper, and even after zooming in the pdf it's difficult to tell what's going on. I think the text doesn't even mention on which environment these curves are obtained.\nI'd suggest some normalized performance ratio over the final performance of the base algorithm, to show the contribution of all the ingredients of the method.\n\nFor the results presented in section 5.1, it's unclear to me which exact variation of POPLIN-A and POPLIN-P have been selected.\n\n\nFinally the abstract reads \"We show that POPLIN obtains state-of-the-art performance in the MuJoCo benchmarking environments\" (similar claims are made elsewhere in the paper). I'd tone this down a little, since the said performance ist state-of-the-art only for the 200k frames regime, and as it's hinted at the end of 5.1, I assume the proposed approach suffers from the same shortcoming as other competing MBRL methods which is that their performance tends to plateau after a few hundred thousand samples. Overall, the SOTA performance in MuJoCo is held by MFRL methods, but typically require more training (up to 1M)\n"
        }
    ]
}