{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper investigates improving robustness to adversarial examples by using mode connectivity in the loss function. The paper received three reviews by experts working in related areas. In a strongly positive review, R1 recommends Accept, but gives some specific technical questions. The authors submitted a response to these questions; in post-review comments, R1 was satisfied and maintained the highly positive review. R2 recommended Weak Reject and also asked specific technical questions, including some additional details on experiments, statistical significance, etc. The author response also convincingly responded to these concerns. R3 recommended Weak Accept but suggested improving the writing, which authors have done in their revision. Given that R1 and R3 are highly positive and R2's concerns were addressed in the response and revision, we now recommend (weak) Accept.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper repurposes results on mode connectivity (that is, minimal loss paths between local optima) to improve adversarial robustness. The idea is new (to me) and appears both elegant and effective. The approach provides a fast way to repair models that have been attacked. \n\nMore broadly, mode connectivity provides a powerful window into adversarial robustness and model choice. It constructs approximately optimal paths between models, where the path depends on a choice of loss. It thus allows to investigate how various models relate to each other from the perspective of various losses. The paper barely scratches the surface of this approach. \n\nComments:\nThe claim that all models on paths have similar test losses (bottom p8) seems a bit of a stretch given the bottom panels (middle and right) of Figure 4.\nThe paper is difficult to read; it somehow feels quite disjointed.\nProposition 1 is difficult to parse.\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "This paper studies leveraging mode connectivity to defend against different types of attacks, including backdoor attacks, adversarial examples, and error-injection attacks. They perform a comprehensive evaluation to show the benign test accuracy and attack success rate over the models in the connected path between pairs of models with the same or different properties, e.g., both are attacked, both are benign, one attacked and one benign, etc., where the connected path is learned using existing algorithms to find the high-accuracy path over two different models. Their evaluation suggests that in certain attack scenarios, exploring the mode connectivity could help find a model that has a high benign accuracy, while with a significantly lower attack success rate than the models at the end points.\n\nIn general I like this paper. Although mode connectivity has been studied in the literature, to my knowledge, this is the first work to extensively study this topic in the context of attacks. An interesting part of the paper is their evaluation on backdoor attacks, where they show that with a very small number of training samples for fine-tuning, they are able to find a model with decent test accuracy, while the watermarks are removed. Meanwhile, the attacks studied in this work include various settings, i.e., poisoning attacks, error-injection attacks, evasion attacks, and also adaptive attacks where the adversary knows that the defender will use path connection to improve the robustness. These make this paper a good reference as a systematic study of their proposed topic.\n\nHowever, my main question is that while the algorithm is pretty effective to defend against backdoor attacks and error-injection attacks, the results of evasion attacks are somehow negative. While it is helpful to show negative results if this is indeed the case, do you have some possible explanation why the models on the path are less robust than the two end models?\n\nAnother concern is that while the benign test accuracy of the model found by their algorithms looks good, both CIFAR-10 and SVHN evaluated in this paper have a small label set and may not be challenging enough. It would be great if the authors can provide some results on a more complicated image recognition benchmark, e.g., some intermediate-level datasets studied in previous work on attacks such as CIFAR-100.\n\n-------------\nPost-rebuttal comments\n\nI thank the authors for clarifying my questions, and I keep my original score.\n------------",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This paper proposes an adversarial defense method based on mode connectivity. The goal of the method is to repair tampered networks using a limited number of clean data examples. The authors consider two types of adversarial attacks: backdoor attacks and error-injection attacks. The proposed method takes two potentially tampered networks, then constructs a low-loss path connecting the weight vectors of the given models in the space of network parameters (the path is constructed using the small set of clean data examples), finally an intermediate point on the path is used as a weight vector corresponding to the “repaired” model. The authors analyze the properties of the paths and show that intermediate points on the mode-connecting paths deliver both high clean-data accuracy and low attack success rate. In the experiments the proposed method shows better results compared to baseline defense techniques including fine-tuning, training from scratch, and pruning followed by fine-tuning. The paper also analyzes evasion adversarial attacks from the perspective of mode-connectivity and observes the existence of barriers in the landscape of robustness loss on the paths connecting regular and adversarially-trained models.\n\nI would like to note that the text of the paper is well-structured and clear. Another strength of paper is that the authors consider multiple supporting experimental settings and extensions of the proposed method. The considered cases include adaptive attacks (the settings in which the attacker is aware about the employed defense method).\n\nAlthough there are many strong sides of this paper, I have identified several serious flaws in the justification of the approach and the results. Thus, I consider the paper to be marginally below the acceptance threshold. I am willing to increase the score if the authors address my concerns expressed below.\n\n1) What is the conceptual difference between using mode-connectivity and fine-tuning?  Both methods try to find a point in the weight space which (1) is close to the initial point and (2) delivers low loss on the bonafide data. What feature of the proposed method can explain its success? The “technical explanations” for the effectiveness of the proposed path connection method only show that the random sampling procedure fails to find models with high-clean-accuracy and low-attack-accuracy. However, this doesn’t necessarily mean that fine-tuning (which performs directed search) cannot find models which perform similarly to the models found by the proposed method (potentially the hyperparameters of the fine-tuning procedure can be tuned better). Given the aforementioned similarity of the proposed approach and fine-tuning, the explanation (either theoretical or empirical) for the effectiveness of the proposed method has to be provided in order to motivate the methodological proposal of the paper.\n\n2) Please report the error bars of the accuracy computed over multiple runs of the experiments (according to Appendix E multiple runs were performed, but only the mean values of the accuracy are reported). Including the error bars would help to quantify the statistical significance of the results.\n\n3) What strategy was used to choose the parameter t? The parameter has different values depending on the model/dataset. How should this value be chosen in general?\n\n4) In the formulation of Proposition 1 it is first stated (assumption (c)) that the directions of the gradient w.r.t. x and the vector v are aligned (the normalized dot product of the vectors is >= c), however then the proposition requires that the vectors must be similar. Why assumption (c) has to be included in this formulation, if the proposition also requires a stronger condition? Either the assumptions or the claim of the proposition has to be formulated clearly. For example, it can be explicitly stated that the proportionality holds approximately (up to terms which go to 0 as c goes to 1). I also recommend to include a term which accounts for the tail of the Taylor expansion in equation A4 instead of using the “approximately equals” sign. Moreover, I recommend to include the discussion of the effect of this tail term on the proof and the statement of the preposition.  \n\nAdditional minor comments, which do not affect the assessment:\n\nIn Section 3.1, paragraph 3 the description of Figure 1 reads as follows: “our path connection is trained using different portion of test data”. Another sentence below in the paragraph: “For example, path connection using merely 1000/2500 CIFAR-10 samples from the test set only reduces the test accuracy of VGG16 models by at most 10%/5%”. Which part of the dataset was used for finding the path connections? How the “test data”, which is claimed to be used for finding paths, is different from the test set used to evaluate the accuracy of the models? \n"
        }
    ]
}