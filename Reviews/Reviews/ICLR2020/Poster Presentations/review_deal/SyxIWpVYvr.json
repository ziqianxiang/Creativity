{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "I have read the paper and the reviews carefully. Despite the numerical scores, I think this paper is above the bar for ICLR, and recommend acceptance.\n\nThis paper addresses the now-well-known problem that generative models often assign higher likelihoods to out-of-distribution examples, rendering likelihoods useless for OOD detection. They diagnose this as resulting from differences in compressibility of the input, and propose to compensate for this by comparing the log-likelihood to the description length from a strong image compressor. They show this performs well against a variety of OOD detection methods.\n\nThe idea is a natural one, and certainly should have been one of the first things tried in addressing this phenomenon. I'm a little surprised it hasn't been done before, but none of the reviewers or I are aware of a prior reference, so AFAIK it's novel. One reviewer believes the contribution is small; while it's simple, I think the field will benefit from a careful implementation and testing of this approach.\n\nMultiple reviewers raise the concern of whether generative models' bias towards low-complexity inputs is just a matter of needing better generative models. I don't think so: even arbitrarily good generative models will still be limited by the inherent compressibility of an input (e.g. as measured by Kolmogorov complexity).\n\nI'm also not concerned about the lack of an explicit threshold; if one has proposed a good score function, there are many ways one could choose a threshold, depending on the task.\n",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #3",
            "review": "This paper analyzes the peculiar case that deep generative models often assign a higher likelihood to other datasets than they were trained on. The running hypothesis here is, that input complexity plays a central role. Measuring a proxy for input complexity shows that it is tightly anticorrelated with likelihood and therefore seems to describe the trend well. A new OOD detection score is introduced based on these insights.\n\nThe failure of Cifar-10 generative models to detect SVHN as OOD via thresholded likelihood is an interesting question. I also think that the conclusion of this work, that this behavior may be natural and a consequence of likelihood itself, seems very sensible.\n\nThe overall message of the paper is very important and might help to avoid researchers wasting time on improving likelihood models for OOD detection. Instead, it seems a more sensible way to deal with this problem is to consider likelihood ratios.\n\nOne concern is that a formal discussion if the likelihood is supposed to behave the way the authors describe, or if a better model might solve this eventually. I think it is already mentioned that empirically there is no reason to believe a better model would solve this, it would just be nice to have a theoretical statement here as well.\n\nAnother concern is that even though the empirical results look quite promising, it would be good to stress-test the proposed score on more common OOD detection benchmarks against state-of-the-art methods to see if likelihood generative models with the proposed criterion are competitive there.\n\nAll in all, I think this is a great paper, additional theoretical analysis and stronger empirical results would be helpful to increase the significance of the work.\n\n--------------\nPost Rebuttal\n--------------\n\nI would like to thank the authors for their interesting response. \nHowever, I will not change my score as the response did not change my opinion on the need for a more theoretical treatment of the statement, which is a serious (and potentially hard to resolve) shortcoming of the paper in my opinion.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "# ==== Summary of the paper ====\n\nThis paper proposes a new criterion for out-of-distribution (OOD) detection for\ngenerative modelling of images (absence of labels). The OOD detection task is defined as follows: given a generative model p(x) trained on some data $X \\sim p^*$ (where $p^*$ is the true unknown data generating distribution), and a test point y, how can we make use of p to detect that y is not drawn from $p^*$ (i.e., out of distribution)? The first natural idea is to use the likelihood (density) given by p, which, by now, is known to be problematic since the likelihood can be high when evaluated on data points from a completely different domain. \n\nThis paper contributes the following results:\n\n1. Show that \"simple\" images (e.g., constant color) tend to give high likelihood, whereas complex images (e.g., noise) tend to give low likelihood. See Figure 2. The paper further quantifies this complexity with the normalized size of the compressed input images, as given by a compression algorithm. The paper shows that there is a negative correlation between the compressed size and the likelihood (models trained on CIFAR 10. See Figure 4).\n\n2. To address this issue with using only the likelihood for OOD detection, the paper proposes a new measure $S(x)$ given by the negative log likelihood minus the normalized compressed size. See Eq 1. The paper connects this measure to Bayesian model selection.\n\nEmpirical results on more than 10 image datasets show that the proposed measure works better than the negative log likelihood in most cases.\n\n# ==== Review ====\n\nThe paper is easy to follow. The finding that simple images tend to give high likelihood is interesting. My concerns are:\n\n1. How much does the conclusion that \"simple images tend to give high likelihood\" depend on the complexity of the model? As far as I can see, only a few models are studied here: Glow and PixelCNN++. What is the quality of the learned models? Do they generate realistic images? What happens if you consider simpler models (say, reduce the number of layers in Glow.)?\n\n2. Given a model p and a test input image y, how exactly do you tell if y is out of distribution? Is there a threshold? If so, what is the threshold? I understand that by AUROC used in Table 1, you vary the threshold. For each value of the threshold, you compute the true positive and false positive rates, and plot the ROC curve. The reported numbers in Table 1 are areas under the curve. Is this correct? But this does not explain how to perform OOD detection given one input image.\n\n3. What is the reason for the poor AUROC in the case of TinyImageNet in Table 1? This is an interesting case since it may suggest that there are other hidden factors (besides the complexity of images) that can affect the OOD detection with the likelihood.\n\n4. Have you tried to run the experiment in Table 1 with a model trained on Constant (simplest images) or Noise (most complex images)?  What happens? Also, why not also include CIFAR10 (used to train the model) in the list of datasets in Table 1?\n\nOverall, my main concerns are with the thoroughness of the experiments, and that the two main contributions (summarized above) may not be enough. I will consider my evaluation again after seeing responses from the authors.\n\n\n# ==== Minor. Did not affect the score ====\n\n* The bottom margin seems off. Please check whether the Latex template is used correctly.\n\n* The paragraph before Section 3: \"... could replace the computed likelihood values for the negative of our complexity estimate ...\" I think it is too soon to make this conclusion. \n\n* The term \"likelihood ratio test\" used in Section 3.2 is very misleading. There is no hypothesis testing there.\n\n* Might be better to briefly describe the meaning of AUROC at the beginning of section 5.\n\n\n\n\n"
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "The paper discusses the inductive biases in generative models that tend to assign higher likelihoods to \"less complex\" images. In particular, a likelihood based generative model (like Glow or PixelCNN++) trained on a particular dataset has significantly higher likelihood for data that have lower compression ratios (e.g  with PNG). The authors propose a simple approach based on the likelihood ratio between a trained model and a \"prior\" model (based on existing compression methods) and demonstrate that this improves unsupervised OOD detection on certain dataset pairs. The idea is quite simple (and surprising it seems to work!), but it seems that better understanding of the proposed method could be achieved.\n\nQuestions:\n\nIf our goal is to perform OOD detection, then higher likelihood for test samples might not be an issue, as we can simply declare samples with higher and lower likelihoods as OOD?\n\nIt would seem that using L(x) might already distinguish some OOD samples from Figure 4? What are the AUC of OOD detection if we had simply used these? \n\nFrom Figure 4, the x axis is p(x|M) and is between zero and one, while L(x) should be -log p(x|M_0). Why would we expect to observe a highly linear correlation between the two (as opposed to linear with p(x|M) and exp(L(x)))? Visually it seems that a lot of the datasets have similar distributions in both cases -- wonder why the proposed S statistic improves OOD empirically? Does this still hold for continuous likelihood models like Glow?\n\nThe performance seems to depend on the \"prior\" chosen (e.g. FLIF seems to have much better performance than PNG); any insights why this is the case?\n\nTable 1: I suppose a reasonable comparison for AUROC is max(current, 1 - current)? If AUROC is very small, we can still obtain good classifiers with flipped predictions.\n\nIt seems strange that the black-and-white images and colored images are compared together (say I train on MNIST and test CIFAR10); it should be quite easy to detect even with existing approaches. I suppose it would be most interesting to see cases where existing approaches fail miserably (like CIFAR10 vs. CIFAR100/TinyImageNet); unfortunately the proposed methods does not seem to improve much for PixelCNN++.\n"
        }
    ]
}