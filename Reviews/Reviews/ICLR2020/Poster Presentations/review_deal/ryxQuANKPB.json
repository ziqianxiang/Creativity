{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This work proposes use of two pre-trained FST models to explicitly incorporate semantic and strategic/tactic information from dialog history into non-collaborative (negotiation) dialog systems. Experiments on two datasets from prior work show the advantage of this model in automated and human evaluation. While all reviewers found the work interesting, they made many suggestions regarding the presentation. Author'(s) rebuttal included explanations and changes to the presentation. Hence, I suggest acceptance as a poster presentation.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #2",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This is an interesting paper about use of Finite State Transducers to model semantic and tactic history in dialogues. This can be fruitful in settings where negotiation settings.\n\nI have some basic questions.\n\nWhy not a direct comparison with the models proposed by Wang et al., 2019, He et al., 2019?\n\nWhy is human evaluation performed only w.r.t. HED and its variant but not Sequicity?\n\n How do you come up with number 37.6 in negotiation dialogue?\n\nI think, there is a long way to go to solve the considered dialogue modeling problem. I like the application of FSTs for the problem. Why not compare w.r.t. HMMs as you mention it as a choice in the intro?\n\n\n"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "3: Weak Reject",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review": "This work presents two FST models to explicitly incorporate semantic and strategic/tactic information into dialog systems. Experiments on two datasets from prior work show the advantage of this model. \n\nGiven the writing of this paper, I am not sure I understand the novelty of this work on the modeling side. Particularly, section 2.3 is too brief to provide enough technical details so readers can understand how exactly the two FST models work and why they are effective in encoding semantic and strategic information. With the important details being left out, it is difficult to evaluate the novelty of this work. \n\nIn addition, although the evaluation in section 3 shows the proposed model outperforms some existing competitive systems. The experiment setup is questionable:\n\n- Both competitive systems (He et al., 2018) and (Lei et al., 2018) are not exactly the same as proposed in these two works. To make a fair comparison, I strongly suggest running the systems released from their works\n- The evaluation measurements used in the human evaluation is not directly comparable to prior work, which makes it further unclear about the performance of the proposed model. \n\n\nSome additional comments on details\n\n- In the paragraph before section 2.4, I think \"probability density function\" should be \"probability distribution\", since the random variables are discrete.\n- In the joint loss function (the equation before section 3), why use an indicator function instead of the cross-entropy loss for the last term? \n- In experimental setup, my understanding is that the model was jointly trained with all components. Why there are two separate learning rates?\n- In evaluation, what are the exact definitions of Uni.acc and Bi.acc? I read this paragraph multiple times, and I am still not sure whether I understand it correctly. "
        },
        {
            "experience_assessment": "I do not know much about this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A",
            "review": "This paper proposed a framework for non-collaborative dialog systems. To track the history better, the authors propose to apply two pre-trained finite state transducers (FSTs) to take the sequence of dialog acts or strategies as inputs and output sequences of state embeddings for the hierarchical encoder-decoder (HED) part as inputs. The authors test their model on two tasks, a bargain task and a persuasion task.\n\nThe model that the authors prepose makes sense to me. It seems that the FSTs can successfully help the model to track the history better and propose useful state embeddings for the rest part of the model, on both dialog acts and strategies. Also, the authors have done a comprehensive empirical study to show the gain of the proposed techniques over the pure HED model and some baseline method. So I think this is a good work. But potentially it would be better if the authors can show more insights and details for the FSTs. \n\nSome detailed comments:\n\n1. The example dialogs in Table 3 show us that the FSTs can output dialog act distributions that can help the agent behave better compared to the case with an RNN for tracking the history, and the case without FST/RNN. This shows the success of the FST framework.\n\n2. The authors have done a comprehensive comparison for the cases with / without the two FSTs, as well as the case without the strategy predictor. Also, the authors compared with another existing method (Sequicity). The proposed method performs well compared to the baseline methods, on both general performances and the two types of human-evaluations. This shows the good empirical performance of the proposed method.\n\n3. This paper can be better if the authors can provide more insights and details for the FSTs. For example, why FST is better than RNN? The authors mentioned something about that, but it is not clear enough and it will be great if the authors can provide more discussions on this. In addition, the output alphabets and the sets of states are not clear for the FSTs, it will be better if the authors show the full details for the FSTs, as in my opinion this is the major contribution of this work.\n\n4. A minor comment: There are some inaccurate expressions in this paper. For example, the authors mentioned that the FSTs about put probabilistic density functions. However, to my understanding, the set of dialog acts is finite, hence FSTs output probabilistic mass functions, not probabilistic density functions. Please proofread about that.\n\nQuestion:\n\n1. Can you explain more about the choice for the parameters? For example, why the possible strategy output is a 15-dimensional binary-value vector?\n\n2. For the loss L_{joint}, shouldn't it be (st_{t+1,j}\\not\\in u_{t+1}) in the indicator function?\n\n"
        }
    ]
}