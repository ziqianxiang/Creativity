{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "This paper presents a certified defense method for adversarial patch attacks. The proposed approach provides certifiable guarantees to the attacks, and the reviewers particularly find its experiments results interesting and promising. The added new experiments during the rebuttal phase strengthened the paper. There still is a remaining concern that is novelty is limited as this paper could be viewed as the application of the original IBP to patch attacks, but the reviewers believe in that its empirical results are important.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "This paper attempts to extend the Interval Bound Propagation algorithm from (Gowal et al. 2018) to defend against adversarial patch-based attacks. In order to defend against patches which could appear at any location, all the patches need to be considered. This is too computationally expensive, hence they proposed to use a random subset of patches, or a U-net to predict the locations of the patches and then use those patches to train. The algorithm is tested on the MNIST and CIFAR-10 datasets and it was shown that sometimes the IBP approach is useful for defense, although often with a significant loss on accuracy on clean data (e.g. on CIFAR the loss on clean accuracy is an astounding 300% -- from 66.5% - 35.7%).\n\nI think the technical contribution of this paper is a bit weak in that they mostly followed the original IBP and the only novelties are the random patch training and guided patch training. I partially like how the experiments are conducted, especially the one that generalizes to other shapes. On the other hand, the networks that are tested seem pretty poor by any standard. An experiment that is definitely missing is a CIFAR network that performs a little better than the current one. Clean accuracy of only 66.5% and 47.2% are very lousy for CIFAR.\n\nAnother missing experiment is one that would test on different epsilon values. I couldn't find what are the current epsilon values used?\n\nBesides, since this work is testing on adversarial patches, I would like to at least have it applied to some real-life images with patches that are of real-life size. I could care a bit less on how good it is, but one can still make an empirical test (e.g. certified defense accuracy on 5x5 patches, but empirical test using real-life sized patches 40x40 or 80x80) and see how the results would be. All the experiments mentioned above would significantly strengthen the experiments section of the paper.\n\nI don't think I read anywhere a confirmation that the testing is performed on all patches of the prescribed size. Could the authors please confirm whether this is true?\n\nMinor: \nThere is a typo in Eq. (5) and Eq. (6), where the second term multiplied by |W^(k)| should be \\underline{z}^(k-1) - \\bar{z}^(k-1) instead of \\underline{z}^(k-1) + \\bar{z}^(k-1)\n\nYou should mention that |W^(k)| stand for element-wise absolute value when it first appears.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "The paper proposes a certified defense for adversarial patch attacks.\nTechnically, the authors use the well developed IBP based methods (Gowal et\nal., 2018, Mirman et al., 2018, Zhang et al. 2019).  The technique is simple\nbut effective. Since the number of possible patches are quadratic w.r.t image\ndimension, to reduce the number of bounds to propagate, the authors propose a\nU-Net based NN to predict the worst case scenario, and only propagate \"worst\ncase\" bounds predicted by the U-net.\n\nEmpirically, the proposed method gets good results, with certified accuracy\nsometimes even higher than empirical accuracy by previous methods.  The authors\nalso provide results for transferring robustness properties to shapes that are\nnot included during training.\n\nOverall, the contribution of this paper is novel, and results are promising,\nbut it still has some missing components, especially the idea of combining\nmultiple IBP bounds into one, which can be very effective for adversarial\npatches, as I will elaborate below.\n\nSuggestions and Questions:\n\nThe core idea behind IBP is that for whatever input perturbation is given (any\nLp norm or semi-norm, or non-norm based perturbations like patches at arbitrary\nlocations), it converts them to per-neuron lower and upper bounds after the\nfirst linear/conv layer.  For example, if the input perturbation is *two* patches\nB_1 and B_2, after propagating them through the first layer of the network, we\ngot two lower bounds l_{i,1}, l_{i,2} and two upper bounds u_{i,1}, u_{i,2} for\nthe i-th neuron. We then take the worst case bound, l_i = min(l_{i,1},\nl_{i,2}), u_i = max(u_{i,1}, u_{i,2}) and propagate only one set of bounds l_i\nand u_i to the next layer. The authors should explore on this direction, as\ndetailed below:\n\n1. For the exhaustive patch enumeration in (11), we can actually greatly reduce\nthe computation cost by combining the bounds of different patches after the\nfirst layer of the network, as I mentioned above. At the input layer, the\nnumber of bounds (each for one possible location of patch) are large; but after\nthe first linear/conv layer, we can compress them to one or a small group of\nbounds by taking the worst cast of them, like l_i = min(l_{i,1}, ...,\nl_{i,|L|}), u_i = max(u_{i,1}, ..., u_{i,2}). The patches close with each other\nshould also produces similar lower and upper bounds, so taking min or max over\nthem will not make the combined bounds much worse. This is better than U-net\nprediction since we are guaranteed to include the worst case scenario.\n\n2. Considering multiple patches (at different locations) on a single input. In\nthe simplest case, consider multiple 1x1 patches (in fact, this is equivalent\nto bounded L0 norm threat model); since each patch is 1x1 (only changing one\npixel), the bounds should be relatively tight after the first layer, and after\nthe first linear/conv layer we got 28*28 or 32*32 bounds which will be combined\ninto one or very few sets of lower and upper bounds that will be propagate into\nlater layers.  Multiple larger patches (2x2, 5x5) can be difficult since bounds\nare looser; multiple 1x1 in my opinion is both technically feasible and\npractically important, and should definitely be included in this paper.\n\n\nMinor issues:\n\n1. Eq. (5) and (6) are incorrect; the second term should be \\overline{z} -\n\\underline{z}. Also it is missing the bias term.\n\n2. In related works (section 4.1, page 3), (Weng et al., 2018) is not a defense\nmethod (it is a certification method, and no training is involved).\n\n3. In Table 3, it is better if the authors can provide empirical adversarial\naccuracy to IBP defended networks as well.\n\nOverall, I think it is a good paper but the authors should explore more to\nstrengthen their contributions. I gave a weak reject but I will not hesitate\nto recommend an accept as long as the authors can provide additional results\nmentioned above.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "rating": "6: Weak Accept",
            "experience_assessment": "I have read many papers in this area.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review": "This paper proposed a certified defense method for adversarial patches. The paper is motivated by the finding that several existing works on adversarial patch defenses are easily \"breakable\" by in the white-box setting. The idea of the proposed method is derived Interval Bound Propagation (IBP), which is originally proposed for certified defense against adversarial noise. To simplify the certificate training in the patch defense setting (which original scales quadratically with respect to the image size), two randomized training methods are proposed. Lastly, experimental results indeed verify the effectiveness of the proposed method.\n\nThe paper is well-written and very well-organized. It is interesting to see supposedly strong adversarial patch defense methods \"break down\" in a very simple setting. And the contribution of the paper is significant to the field.\n\nI do have a question with regard to the randomized training method:\nAlthough random patches (or selected worse-case patches at random location) can be used for certificate training, in order to create a certificate during testing, does it mean that you do have to conduct many forward pass of the network with respect to all patches at all locations?"
        }
    ]
}