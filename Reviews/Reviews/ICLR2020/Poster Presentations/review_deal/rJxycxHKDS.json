{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "Although some criticism remains for experiments, I suggest to accept this paper.",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "title": "Official Blind Review #1",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.",
            "review": "The paper provides an unsupervised domain adaptation approach\nin the context of deep learning. The motivation is clear, related work\nsufficient and experimental settings and results convincing. \nI have only very minor comments:\n- I would prefer to get the paper additionally linked to a few more\n  transfer learning techniques out of the deep learning domain\n  which is important as well\n- do you really need to call it (multi) flow network .... - a flow network\n  is a well established concept in algorithmics and refers to a graph problem\n  ... to avoid name clashes ...\n- in the references you have provided back links to the pages where the references\n  are used - this is handy but also confusing and a bit unusual - I think it was not part \n  of the standard template\n- please avoid using arxiv references but replace them by reviewed material. In parts\n  I am willing to accept such kind of gray literature provided by well known authors but\n  this should not become a standard habit\n- I am happy to see that the code will be published - I hope this is really done, because\n  from the material it maybe hard to reconstruct the method"
        },
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "After Discussion Period:\n\nI stick to my original score. My issues are largely resolved.\n\n----\nThe submission is using adaptive computation graphs for domain adaptation. Multi-flow network is the main architectural element proposed in the submission. And, it is composed of parallel blocks of computations which aggregated using weighted summation with learnable weights. The domain adaptation is performed by setting different weights for source and target dataset. The adaptive weights and network parameters are all learned jointly by minimizing the combination of classification loss and domain difference loss.\n\nAlthough the idea of adaptive computation is not novel and has been explored, their application to the domain adaptation problem is novel to the best of my knowledge. Moreover, the proposed method is sensible and technically sound.\n\nThe submission talks about different amount of computation needed per domain as an intuition behind the method. This is sensible and intuitive; however, it has not been experimented. The paper uses the same amount of layers for all domains making the amount of computation exactly same. It would be interesting to see the performance when different paths actually lead different computations. For example, parallel blocks can have different number of layers etc.\n \nThe submission only provides result for RPT and DANN. These are clearly not state-of-the-art domain adaptation methods. Proposed method does not necessarily need to have state-of-the-art adaptation results to be accepted, but not reporting what state-of-the-art performance is makes the experimental results incomplete.\n\nFigure 4 suggests that there is no real parameter sharing at the end of the training. And, all domains have different computations. Authors should try to explain this behaviour since it is quite counter-intuitive. \n\nIn summary, proposed method is somewhat novel, interesting and seems to be working well. Improved discussion on the experimental study is definitely needed.\n",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "rating": "3: Weak Reject",
            "experience_assessment": "I have published in this field for several years.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "title": "Official Blind Review #3",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "review": "In this paper, the authors proposed to address the information asymmetry between domains in unsupervised domain adaptation. Innovatively, they resort to a multiflow network where each domain adaptatively selects its own pipeline. I quite appreciate the idea itself, while there are many essential issues to be addressed first. \n\nPros:\n-\tThe way tackling the information asymmetry, or untie weights, between domains is novel and interesting. \n-\tThe proposed network/framework can be easily extended to the multi-task setting, or multi-source/multi-target domain adaptation.\n-\tThe paper is well-written and easy to follow. \n\nCons:\n-\tThe most critical downside of this paper is its insufficient experiments to support the whole idea, where we will detail in the next.\n\nExperimental issues:\n-\tComparison with other state-of-the-art UDA methods (e.g., CDAN) is a must. This paper improves UDA in terms of adaptive parameters sharing, which is completely independent from most of the UDA contributions (including the DANN you compared) which improve the distribution alignment between feature representations. Therefore, it is imperative to compare that line of SOTA methods, otherwise why should we consider adaptative parameters sharing instead of distribution alignment? At best, the proposed multiflow network combined with the SOTA feature alignment method (e.g., CDAN other than DANN) should be considered and expected to beat CDAN itself. \n-\tMany ablation studies or hyperparameter sensitivity analyses are missing. \no\tHow do you determine the number of parallel flows, i.e., K? Is it possible that 3 or 4, more than 2 flows, are better even in the UDA between two domains? \no\tDo you try any other possibilities of grouping a computational unit, and how will different configurations influence the performance? \no\tIs there a possibility that none of the gates in the final layer is activated? Do you need some constraints?\n-\tSince the authors mentioned the potential of the multi-flow network in adaptation between multiple domains, it is necessary to investigate multi-source or multi-target domain adaptation. Only in this case may the significance of different K values be demonstrated. \n-\tThe baseline results in Table 1 are not comparable to some reported papers, and even lower than those reported in other UDA papers. "
        }
    ]
}