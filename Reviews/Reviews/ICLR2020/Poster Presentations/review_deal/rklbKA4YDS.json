{
    "Decision": {
        "decision": "Accept (Poster)",
        "comment": "In this paper, the authors propose a novel approach for learning the structure of a directed acyclic graph from observational data that allows to flexibly model nonlinear relationships between variables using neural networks. While the reviewers initially had concerns with respect to the positioning of the paper and various questions regarding theoretical results and experiments, these concerns have been addressed satisfactorily during the discussion period.  The paper is now acceptable for publication in ICLR-2020. ",
        "title": "Paper Decision"
    },
    "Reviews": [
        {
            "experience_assessment": "I have published in this field for several years.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.",
            "title": "Official Blind Review #3",
            "review": "Summary: \nThe authors propose a prediction model for directed acyclic graphs (DAGs) over a fixed set of vertices based on a neural network. The present work follows the previous work on undirected acyclic graphs, where the key constraint is (3), ensuring the acyclic property. The proposed method performed favorably on artificial/real data compared to previous baselines. \n\nComments: \nI do not understand yet if the proposed formulation really ensures the acyclic condition. More precisely, the condition (3) ensures undirected acyclic property, which also implies the directed ones. However, I am afraid that the condition (3) might also eliminate DAGs containing “undirected” cycles, i.e., cycles when we neglect directions of edges as well. So, I think a formal proof is necessary to show that the proposed formulation can output all possible DAGs, not a subset. \n\nThe present work heavily relies on the previous work of Zheng et al. (2018). The proposed formulation is designed for DAGs, but due to my concern about the correctness raised above, it is difficult for me to evaluate the originality of the present work. \n\nMinor comments:\n-“X_\\pi_j^G denote the random vector containing the variables corresponding to the parents of j in G.” – this definition is not clear. Please clarify the meaning. \n\n\nComments after Rebuttal:\nAfter reading the rebutall comments, I modified my score.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."
        },
        {
            "experience_assessment": "I have published one or two papers in this area.",
            "rating": "6: Weak Accept",
            "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #2",
            "review": "This work addresses the problem of learning the structure of directed acyclic graphs in the presence of nonlinearities. The proposed approach is an extension of the NOTEARS algorithm which uses a neural network for each node in the graph during structure learning. This adaptation allows for non-linear relationships to be easily modeled. In addition to the proposed adaptation, the authors employ a number of heuristics from the causal discovery literature to improve the efficacy of the search. Empirical results are provided which compare the proposed algorithm to prior art. \n\nOverall, I found the paper to be well written and sensible. However, there are a few items that prevent me from recommending acceptance at this time: \n\n1) The proposed approach seems to necessitate a fair number of hyperparameters. How were these chosen? On what basis should practitioners choose their hyperparameters for real-world application?\n\n2) The authors employ causal language in both the introduction and the experiment section. However, there is no notion of (a) whether the proposed algorithm is sound and complete,  and (b) under what assumptions we can expect a fully directed graph to be reasonable (I assume the additive noise model?). \n\n3) The experiments seem to indicate that prior work (CAM) outperforms or performs similarly to the proposed method. While this shouldn't prevent a paper from being published, it would be nice to see an extended discussion as to why the authors think this is occuring. ",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        },
        {
            "experience_assessment": "I have read many papers in this area.",
            "rating": "8: Accept",
            "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.",
            "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.",
            "title": "Official Blind Review #1",
            "review": "################## Response to rebuttal ##################\n\nI would like to thank the authors for their diligent rebuttal. Overall, I consider that the issues raised in my review have been mostly addressed. \n\nIn summary, after reading the other reviews and going through the revised manuscript, I believe the paper would be a meaningful contribution to the field, both in terms of the potential applicability of GraN-DAG to certain problems and its potential to lead to future work. As a result, I have updated my score to support acceptance without reservations.\n\nIn particular, the authors now discuss clearly some of the potential limitations of the approach, such as the scaling of its asymptotic complexity with respect to the number of features or its statistical performance in the small sample size regime. The additional experimental results are, in my opinion, also a good addition to the manuscript, further clarifying the strengths and weaknesses of NOTEARS, CAM and GraN-DAG under different regimes and modelling assumptions. I believe these changes substantially improve the paper, as it will both help practitioners decide when to use GraN-DAG as well as clearly delineate avenues for future work.\n\n# Minor points / additional questions\n\nThe new results regarding convergence in Table 4 suggest that early stopping might indeed be instrumental. I wonder the extent to which NOTEARS could benefit from early stopping, be it in terms of runtime or, less likely due to its use of explicit regularization, the quality of the structures it retrieves. \n\n# Typos\n\nSection 3.6: … GraN-DAG performs fewer iteration(s) than NOTEARS…\nSection 5: GraN-DAGand DAG-GNN -was- were not designed with…\nAppendix A.1: … graphs can be roughly -halfed- halved when executed…\nAppendix A.5: … graphs were generated using the -BarabsiAlbert- Barabási-Albert model …\nAppendix A.9: … to selecting hyperparameters in practice consist(s) in ...\n\n################## Original review ##################\n\n# High-level assessment\n\nThe manuscript presents an alternative to the method in [2] to extend the ideas of [1] to nonlinear DGMs using neural networks. \n\nFrom a methodological perspective, in my opinion the proposed approach is sound and sufficiently novel to be a significant contribution. While some limitations might be present, such as being prone to overfitting without relying on additional (pre/post)-processing heuristics or perhaps being computationally intensive for high-dimensional data ($O(p^{3})$ runtime per SGD step), I believe that addressing those issues (if indeed present) could be left for future work.\n\nThe paper is written clearly, being easy-to-follow while providing enough detail to understand the proposed method in depth and attempt to reproduce the results.\n\nIn my opinion, perhaps the weakest aspect of the paper concerns the experimental analysis, which is somewhat limited in its scope. Most notably, I believe the current experimental setup focuses on cases likely to be favorable to the proposed approach relative to the baselines ($n > p$, nonlinear and non-additive ground-truth models) while presenting no results for other settings commonly encountered in structure learning applications, such as high-dimensional $n << p$ problems or ground-truth models given by linear or generalized linear SCMs.\n\nTaking all those factors into consideration, I currently lean towards a “weak accept” rating, but would gladly modify my score after the author discussion phase as appropriate.\n\n# Summary\n\nIn this paper, the authors propose a novel approach for learning the structure of Directed Graphical Models (DGMs) from observational data that allows to flexibly model nonlinear relationships between variables using neural networks.\n\nThe proposed method builds on the recent breakthrough in [1], which pioneered the use of a smooth characterization of acyclicity for directed graphs. In particular, the authors in [1] use this characterization to learn the structure of **linear** DGMs by solving a (nonconvex) continuous, constrained optimization problem.\n\nThe main contribution of this manuscript is an extension of the approach in [1] to the nonlinear setting by making use of neural networks, and which appears to outperform the only existing method that shares a similar high-level goal [2], presumably by allowing more flexibility in its parametrization.\n\nIn a nutshell, the authors accomplish this by:\n\n1. Flexibly modelling the conditional distribution of each variable $X_{j}$ as a nonlinear function of (possibly) all other variables $\\bf{X_{-j}}$ using an MLP, i.e. $p_{j}(X_{j} \\mid \\bf{X_{-j}}) = f_{j}(X_{j} ; NN_{j}(\\bf{X_{-j}}))$, with $f_{j}$ denoting any probability density differentiable w.r.t. its parameters and $NN_{j}$ being a neural network with parameters $\\theta_{j}$.\n\n2. Deriving a function $C_{i,j}(\\theta_{j})$ such that $C_{i,j}(\\theta_{j}) = 0$ is a sufficient condition for the conditional distribution of $X_{j}$ to be independent of $X_{i}$.\n\n3. Using $C_{i,j}(\\theta_{j})$, which is differentiable w.r.t. $\\theta_{j}$ for most values of $\\theta_{j}$, to define a “connectivity” matrix to be directly plugged into the acyclicity criterion in [1].\n\n4. Following [1], the model is then fit by (constrained) maximum likelihood using an augmented Lagrangian method. However, unlike [1], the unconstrained subproblem in each iteration of the augmented Lagrangian method is solved using stochastic gradient descent, rather than a batch quasi-Newton method, and regularized using early stopping on an external validation set, rather than by using explicit $L_{1}$ regularization to induce sparsity.\n\n5. The authors also incorporate additional “heuristics”, such as borrowing the Preliminary Neighbour Selection (PNS) and feature selection pruning techniques from [3], which are reported to greatly enhance the performance of the proposed approach (Table 4), and adapt the thresholding scheme in [1] to the nonlinear setting.\n\nThe proposed method is evaluated using synthetic and semi-synthetic data, as well as one real-world dataset.  The results suggest that, in low dimensional problems ($n > p$, $p \\le 100$) for which the ground-truth relationships between variables are nonlinear and non-additive, the proposed approach clearly outperforms [2] and also outperforms [3] in many settings, albeit often not by a statistically significant margin in the latter case.\n\n# Major points / suggestions\n\n1. I believe it could be argued that one of the main drawbacks of the smooth acyclicity constraint in [1] is its high computational complexity w.r.t. the number of features. However, [1] uses a batch quasi-Newton method to solve the unconstrained subproblems in their optimization routine, which should in principle lead to relatively few $O(p^3)$ matrix exponential evaluations being necessary. \n\nAs it relies on the same characterization of acyclicity as [1], this drawback might also limit the scalability of the proposed approach for high-dimensional data. Moreover, since the method in this manuscript uses mini-batch gradient descent instead, this problem could in fact be exacerbated by perhaps requiring substantially more $O(p^3)$ matrix exponential evaluations.\n\nTo this end, I would be glad if the authors could clarify whether this could indeed be a limitation or not. In any case, I believe it could also be helpful to include a small discussion regarding computational complexity for the proposed approach and related work in the manuscript or appendix.\n\n2. All experimental results reported in the manuscript use only data generation models with nonlinear and non-additive relationships between variables.\n\nThis setup is well aligned with the contribution, as it emphasises its ability to flexibly model nonlinear relationships between variables learnt using global, gradient-based optimization. In contrast, most baselines are limited to either linear/generalized linear models or use some variant of greedy search.\n\nWhile investigating this setting is certainly the main priority given the problem statement, I believe it would also be important to characterize the extent to which the performance of the proposed approach deteriorates when this extra flexibility is not needed, if at all. To this end, I would encourage the authors to repeat their experiments for synthetic data, but generated under linear and generalized linear SCMs.\n\nIn particular, it is remarkable that CAM [3], which suffers from model misspecification due assuming the data is generated by a generalized linear model, remains so competitive throughout all experiments shown in the current version of the manuscript. It would be interesting to see if the proposed approach can match or still outperform CAM under CAM’s “ideal scenario”.\n\n3. Parallel to the previous point, all experimental results in the manuscript so far concern somewhat low-dimensional data $p \\le 100$ and relatively abundant data $n \\approx 1,000 > p$.\n\nThis setup might also be well aligned with the proposed approach, which could be more prone to overfitting than some of the baselines due to its high capacity and its lack of explicit sparsity-inducing regularization, which was found to boost performance for NOTEARS [1, Fig. 3(b)]. \n\nAlso, in the small sample size regime, the reliance on an external validation set for early stopping might become a liability by (i) reducing the amount of data available for model fitting and (ii) making the fitted DGMs less stable w.r.t. changes in the random seed which, compounded with the nonconvexity of the objective, could be problematic if the DGM is to be interpreted for downstream tasks (e.g. in computational biology).\n\nTo this end, I would likewise encourage the authors to extend their experimental setup for synthetic data by (i) repeating their experiments for one “high-dimensional” setting ($n << p$) and (ii) studying the stability of the DAG w.r.t. different training runs.\n\n4. All experimental results currently focus heavily on accuracy, while the computational aspect of the problem is largely secondary.\n\nTo this end, it would be interesting to study, using synthetic data, the scalability of the proposed approach relative to the best performing baseline approaches with respect to (i) number of features, (ii) sparsity of the ground-truth DGM and (iii) sample size, covering a sufficiently broad range in each case (low-dimensional vs high-dimensional, sparse vs dense, scarce vs abundant sample size).\n\n# Minor points / suggestions\n\n1. Perhaps the decision to permanently set the thresholding masks to 0 when an entry of the connectivity matrix is “small enough” could be justified further. For example, by showing that this heuristic rarely “zeros out” incorrectly any entries, or that such entries would seldom become nonzero again if the model was trained for longer without being masked.\n\n2. I might have missed it, but I could not find the info about how the train/validation split is formed for the experiments following Section A.1.\n\n# References\n\n[1] Zheng, Xun, et al. \"DAGs with no tears: Continuous optimization for structure learning.\" *Advances in Neural Information Processing Systems*. 2018.\n[2] Yu, Yue, et al. \"DAG-GNN: DAG Structure Learning with Graph Neural Networks.\" *International Conference on Machine Learning*. 2019.\n[3] Bühlmann, Peter, Jonas Peters, and Jan Ernest. \"CAM: Causal additive models, high-dimensional order search and penalized regression.\" *The Annals of Statistics* 42.6 (2014): 2526-2556.",
            "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."
        }
    ]
}